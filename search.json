[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mihai Ion",
    "section": "",
    "text": "Assistant Professor of Finance (2014-present)\nUniversity of Arizona\nVisiting Instructor (2013-2014)\nPurdue University"
  },
  {
    "objectID": "index.html#academic-appointments",
    "href": "index.html#academic-appointments",
    "title": "Mihai Ion",
    "section": "",
    "text": "Assistant Professor of Finance (2014-present)\nUniversity of Arizona\nVisiting Instructor (2013-2014)\nPurdue University"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Mihai Ion",
    "section": "Education",
    "text": "Education\n\nPhD Finance (2013)\nPurdue University\nMBA (2008)\nPurdue University\nBS Mathematics (2007)\nJacobs University Bremen"
  },
  {
    "objectID": "index.html#research-interests",
    "href": "index.html#research-interests",
    "title": "Mihai Ion",
    "section": "Research Interests",
    "text": "Research Interests\n\nEmpirical Corporate Finance\nPolicy uncertainty, corporate investments, international trade\nEmpirical Asset Pricing\nCross-sectional return predictability, factor models, behavioral asset pricing\nMachine Learning\nLarge language models, random forests, causal inference"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture10_merging_reshaping.html",
    "href": "teaching/FIN 525/lectures/lecture10_merging_reshaping.html",
    "title": "L10: Merging, reshaping datasets",
    "section": "",
    "text": "The data we need for our projects is rarely all in one place (in a single dataframe) or organized the way we need it. This means that we very often have to combine two or more datasets into a single dataset and/or change the organization of the dataset (what appears in the rows and what appears in the columns) to better suit our needs. Here we cover some of the main tools we can use to perform these operations using the Pandas package.\n\nimport pandas as pd\nfrom IPython.display import display #allows us to ``pretty print`` multiple objects in the same cell\n\nLet’s create some example datasets:\n\ndf1 = pd.DataFrame({'year': [2001, 2002, 2003], \n                    'tic': ['MSFT','TSLA','AAPL'], \n                    'fy':[2002,2003,2004]})\ndf1\n\n\n\n\n\n\n\n\nyear\ntic\nfy\n\n\n\n\n0\n2001\nMSFT\n2002\n\n\n1\n2002\nTSLA\n2003\n\n\n2\n2003\nAAPL\n2004\n\n\n\n\n\n\n\n\ndf2 = pd.DataFrame({'year': [2001, 2002, 2004], \n                    'ticker': ['MSFT','NFLX','AAPL'], \n                    'fy':[12,12,12]})\ndf2\n\n\n\n\n\n\n\n\nyear\nticker\nfy\n\n\n\n\n0\n2001\nMSFT\n12\n\n\n1\n2002\nNFLX\n12\n\n\n2\n2004\nAAPL\n12\n\n\n\n\n\n\n\n\n#example of display\ndisplay(df1)\ndf2\n\n\n\n\n\n\n\n\nyear\ntic\nfy\n\n\n\n\n0\n2001\nMSFT\n2002\n\n\n1\n2002\nTSLA\n2003\n\n\n2\n2003\nAAPL\n2004\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\nticker\nfy\n\n\n\n\n0\n2001\nMSFT\n12\n\n\n1\n2002\nNFLX\n12\n\n\n2\n2004\nAAPL\n12"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture10_merging_reshaping.html#merging-by-columns-with-.merge",
    "href": "teaching/FIN 525/lectures/lecture10_merging_reshaping.html#merging-by-columns-with-.merge",
    "title": "L10: Merging, reshaping datasets",
    "section": "Merging by columns with .merge()",
    "text": "Merging by columns with .merge()\nIf the keys we want to merge on are columns in our dataframes (as opposed to indexes) we have to use the .merge() (.join() will not work for this purpose):\nAbbreviated syntax:\nDataFrame.merge(right, how='inner', on=None, \n                left_on=None, right_on=None, \n                left_index=False, right_index=False, \n                sort=False, suffixes=('_x', '_y'))\nWe’ll replace DataFrame and right with the names of the two dataframes we want to merge (respectively). In the subsections below, we explain what the different choices for how mean. When the key(s) on which we want to merge have the same name(s) in the two datasets, we use the on parameter to specify the names of the keys (see the example immediately below), otherwise we need to specify them using the left_on and right_on parameters.\nThe documentation at https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html has some additional examples that you may find useful.\n\nInner join\nThe inner join combines the datasets based on the INTERSECTION of the values in the “key” columns.\nFor example, if we want to inner merge the two dataframes using the year column as a key:\n\ninner1 = df1.merge(df2, how='inner', on='year')\ninner1\n\n\n\n\n\n\n\n\nyear\ntic\nfy_x\nticker\nfy_y\n\n\n\n\n0\n2001\nMSFT\n2002\nMSFT\n12\n\n\n1\n2002\nTSLA\n2003\nNFLX\n12\n\n\n\n\n\n\n\nBut if we want to merge on both year and ticker information, we can not use on because the ticker information has different names in the different dataframes:\n\ninner2 = df1.merge(df2, how='inner', \n                   left_on = ['year','tic'], right_on = ['year','ticker'])\ninner2\n\n\n\n\n\n\n\n\nyear\ntic\nfy_x\nticker\nfy_y\n\n\n\n\n0\n2001\nMSFT\n2002\nMSFT\n12\n\n\n\n\n\n\n\nNote how the name of the fy column in each of the two datasets has been changed. To control that process ourselves, we can use the suffixes parameter to specify suffixes that should be appended at the end of common column names:\n\ninner2 = df1.merge(df2, how='inner', \n                   left_on = ['year','tic'], right_on = ['year','ticker'],\n                  suffixes = ('_df1', '_df2'))\ninner2\n\n\n\n\n\n\n\n\nyear\ntic\nfy_df1\nticker\nfy_df2\n\n\n\n\n0\n2001\nMSFT\n2002\nMSFT\n12\n\n\n\n\n\n\n\n\n\nOuter join\nThe outer join combines the datasets based on the UNION of the values in the “key” columns. For example:\n\nouter = df1.merge(df2, how='outer', \n                 left_on = ['year','tic'], right_on = ['year','ticker'])\nouter\n\n\n\n\n\n\n\n\nyear\ntic\nfy_x\nticker\nfy_y\n\n\n\n\n0\n2001\nMSFT\n2002.0\nMSFT\n12.0\n\n\n1\n2002\nTSLA\n2003.0\nNaN\nNaN\n\n\n2\n2003\nAAPL\n2004.0\nNaN\nNaN\n\n\n3\n2002\nNaN\nNaN\nNFLX\n12.0\n\n\n4\n2004\nNaN\nNaN\nAAPL\n12.0\n\n\n\n\n\n\n\n\n\nLeft join\nIn a left join, the unmatched keys from the left dataset are kept, but the unmatched keys from the right dataset are discarded.\n\nleft = df1.merge(df2, how='left', \n                 left_on = ['year','tic'], right_on = ['year','ticker'])\nleft\n\n\n\n\n\n\n\n\nyear\ntic\nfy_x\nticker\nfy_y\n\n\n\n\n0\n2001\nMSFT\n2002\nMSFT\n12.0\n\n\n1\n2002\nTSLA\n2003\nNaN\nNaN\n\n\n2\n2003\nAAPL\n2004\nNaN\nNaN\n\n\n\n\n\n\n\n\n\nRight join\nIn a right join, the unmatched keys from the right dataset are kept, but the unmatched keys from the left dataset are discarded.\n\nright = df1.merge(df2, how='right', \n                 left_on = ['year','tic'], right_on = ['year','ticker'])\nright\n\n\n\n\n\n\n\n\nyear\ntic\nfy_x\nticker\nfy_y\n\n\n\n\n0\n2001\nMSFT\n2002.0\nMSFT\n12\n\n\n1\n2002\nNaN\nNaN\nNFLX\n12\n\n\n2\n2004\nNaN\nNaN\nAAPL\n12"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture10_merging_reshaping.html#merging-on-index-using-.join-and-.merge",
    "href": "teaching/FIN 525/lectures/lecture10_merging_reshaping.html#merging-on-index-using-.join-and-.merge",
    "title": "L10: Merging, reshaping datasets",
    "section": "Merging on index using .join() (and .merge())",
    "text": "Merging on index using .join() (and .merge())\nAs mentioned above, this covers the situation when the keys on which we want to perform the merge are indexes in the dataframes we want to merge. In this case, we can either use the .merge() function and specify left_index=True, right_index=True, or we can use the .join() funciton without specifying any keys at all (because .join() assumes that you are using the index):\nFirst, let’s add an index (MultiIndex) to the two dataframes:\n\ndf3 = df1.set_index(['year','tic'])\ndf3\n\n\n\n\n\n\n\n\n\nfy\n\n\nyear\ntic\n\n\n\n\n\n2001\nMSFT\n2002\n\n\n2002\nTSLA\n2003\n\n\n2003\nAAPL\n2004\n\n\n\n\n\n\n\n\ndf4 = df2.set_index(['year','ticker'])\ndf4\n\n\n\n\n\n\n\n\n\nfy\n\n\nyear\nticker\n\n\n\n\n\n2001\nMSFT\n12\n\n\n2002\nNFLX\n12\n\n\n2004\nAAPL\n12\n\n\n\n\n\n\n\nBefore we can use .merge(), we have to make the index names match (tic does not match ticker):\n\ndf4.index.names = ['year','tic']\ndisplay(df3)\ndf4\n\n\n\n\n\n\n\n\n\nfy\n\n\nyear\ntic\n\n\n\n\n\n2001\nMSFT\n2002\n\n\n2002\nTSLA\n2003\n\n\n2003\nAAPL\n2004\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfy\n\n\nyear\ntic\n\n\n\n\n\n2001\nMSFT\n12\n\n\n2002\nNFLX\n12\n\n\n2004\nAAPL\n12\n\n\n\n\n\n\n\nNow, say we want to perform an outer merge on the index:\n\nouterm = df3.merge(df4, how='outer', \n                  left_index = True, right_index = True,\n                  suffixes = ('_df3', '_df4'))\nouterm\n\n\n\n\n\n\n\n\n\nfy_df3\nfy_df4\n\n\nyear\ntic\n\n\n\n\n\n\n2001\nMSFT\n2002.0\n12.0\n\n\n2002\nNFLX\nNaN\n12.0\n\n\nTSLA\n2003.0\nNaN\n\n\n2003\nAAPL\n2004.0\nNaN\n\n\n2004\nAAPL\nNaN\n12.0\n\n\n\n\n\n\n\nUsing the .join() function will not work unless we specify how the fy columns need to be renamed. We use the lsuffix and rsuffix parameters for this purpose.\nSyntax:\nDataFrame.join(other, on=None, how='left', lsuffix='', rsuffix='', sort=False)\nFor example, we can perform the same outer merge as above using:\n\nouterj = df3.join(df4, how = 'outer', \n                  lsuffix = '_df3', rsuffix = '_df4')\nouterj\n\n\n\n\n\n\n\n\n\nfy_df3\nfy_df4\n\n\nyear\ntic\n\n\n\n\n\n\n2001\nMSFT\n2002.0\n12.0\n\n\n2002\nNFLX\nNaN\n12.0\n\n\nTSLA\n2003.0\nNaN\n\n\n2003\nAAPL\n2004.0\nNaN\n\n\n2004\nAAPL\nNaN\n12.0"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture10_merging_reshaping.html#from-long-to-wide-unstacking-with-.pivot",
    "href": "teaching/FIN 525/lectures/lecture10_merging_reshaping.html#from-long-to-wide-unstacking-with-.pivot",
    "title": "L10: Merging, reshaping datasets",
    "section": "From long to wide (unstacking) with .pivot()",
    "text": "From long to wide (unstacking) with .pivot()\nSyntax:\nDataFrame.pivot(index=None, columns=None, values=None)\nLet’s create an example dataset:\n\nlong1 = pd.DataFrame({'portfolio':[1,1,2,2], \n                      'year': ['2005','2006','2005','2006'], \n                      'return': [0.1,0.15,0.05,0.01],\n                      'nfirms':[5,3,4,10]})\nlong1\n\n\n\n\n\n\n\n\nportfolio\nyear\nreturn\nnfirms\n\n\n\n\n0\n1\n2005\n0.10\n5\n\n\n1\n1\n2006\n0.15\n3\n\n\n2\n2\n2005\n0.05\n4\n\n\n3\n2\n2006\n0.01\n10\n\n\n\n\n\n\n\nFor example, if we want to reshape the return data so that each year gets its own row (index value), and each portfolio gets its own column, we would use:\n\nwide = long1.pivot(index='year', columns = 'portfolio', values = 'return')\nwide\n\n\n\n\n\n\n\nportfolio\n1\n2\n\n\nyear\n\n\n\n\n\n\n2005\n0.10\n0.05\n\n\n2006\n0.15\n0.01\n\n\n\n\n\n\n\n\nwide.columns\n\nInt64Index([1, 2], dtype='int64', name='portfolio')\n\n\n\nwide.index\n\nIndex(['2005', '2006'], dtype='object', name='year')\n\n\nIf we want to reshape both the return data and the nfirms data in the same way, at the same time:\n\nwide2 = long1.pivot(index='year', columns='portfolio', values=['return','nfirms'])\nwide2\n\n\n\n\n\n\n\n\nreturn\nnfirms\n\n\nportfolio\n1\n2\n1\n2\n\n\nyear\n\n\n\n\n\n\n\n\n2005\n0.10\n0.05\n5.0\n4.0\n\n\n2006\n0.15\n0.01\n3.0\n10.0\n\n\n\n\n\n\n\nNote that now the column labels are two-dimensional:\n\nwide2.columns\n\nMultiIndex([('return', 1),\n            ('return', 2),\n            ('nfirms', 1),\n            ('nfirms', 2)],\n           names=[None, 'portfolio'])\n\n\n\nwide2.index\n\nIndex(['2005', '2006'], dtype='object', name='year')\n\n\nUnstacking based on values in the index can be done using the .unstack() function (usually for datasets with a MultiIndex)."
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture10_merging_reshaping.html#from-wide-to-long-stacking-with-.stack",
    "href": "teaching/FIN 525/lectures/lecture10_merging_reshaping.html#from-wide-to-long-stacking-with-.stack",
    "title": "L10: Merging, reshaping datasets",
    "section": "From wide to long (stacking) with .stack()",
    "text": "From wide to long (stacking) with .stack()\nWe can stack the data back up to a “long” shape, based on information in the index of that dataframe.\nSyntax:\nDataFrame.stack(level=-1, dropna=True)\nThe level parameter is used in case the dataframe we want to reshape has a MultiIndex in the columns i.e. multi-dimensional column names (like wide2 above). The default level=-1 works for dataframes that were reshaped from a different dataframe (like wide2 above). In that case level=-1 tells Python to just undo that reshaping:\n\nlong3 = wide2.stack()\nlong3\n\n\n\n\n\n\n\n\n\nreturn\nnfirms\n\n\nyear\nportfolio\n\n\n\n\n\n\n2005\n1\n0.10\n5.0\n\n\n2\n0.05\n4.0\n\n\n2006\n1\n0.15\n3.0\n\n\n2\n0.01\n10.0\n\n\n\n\n\n\n\nIf we want to stack the information in the first dimension of the column MultiIndex (i.e. the part that contains “return” and “nfirms”), then we use level=0:\n\nwide2.stack(level=0)\n\n\n\n\n\n\n\n\nportfolio\n1\n2\n\n\nyear\n\n\n\n\n\n\n\n2005\nnfirms\n5.00\n4.00\n\n\nreturn\n0.10\n0.05\n\n\n2006\nnfirms\n3.00\n10.00\n\n\nreturn\n0.15\n0.01\n\n\n\n\n\n\n\n\nwide2\n\n\n\n\n\n\n\n\nreturn\nnfirms\n\n\nportfolio\n1\n2\n1\n2\n\n\nyear\n\n\n\n\n\n\n\n\n2005\n0.10\n0.05\n5.0\n4.0\n\n\n2006\n0.15\n0.01\n3.0\n10.0\n\n\n\n\n\n\n\nIf we want to stack the information in the second dimension of the column MultiIndex (i.e. the part that contains 1’s and 2’s), then we use level=1:\n\nwide2.stack(level=1)\n\n\n\n\n\n\n\n\n\nreturn\nnfirms\n\n\nyear\nportfolio\n\n\n\n\n\n\n2005\n1\n0.10\n5.0\n\n\n2\n0.05\n4.0\n\n\n2006\n1\n0.15\n3.0\n\n\n2\n0.01\n10.0"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture20_backtesting_dataprep.html",
    "href": "teaching/FIN 525/lectures/lecture20_backtesting_dataprep.html",
    "title": "L20: Backtesting - data prep",
    "section": "",
    "text": "# Import packages\nimport pandas as pd\npd.options.display.max_rows = 20"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture20_backtesting_dataprep.html#accounting-data-compustat",
    "href": "teaching/FIN 525/lectures/lecture20_backtesting_dataprep.html#accounting-data-compustat",
    "title": "L20: Backtesting - data prep",
    "section": "Accounting data (Compustat)",
    "text": "Accounting data (Compustat)\n\nData cleaning\n\n# Load cleaned Compustat data\nraw = pd.read_pickle('../data/compa.zip')\nraw.shape\n\n(278226, 23)\n\n\n\n# Keep only what we need\ncomp = raw[['permno','datadate','at','revt','cogs','dltt','dlc','ib']].copy()\ncomp.dtypes\n\npermno      float64\ndatadate     object\nat          float64\nrevt        float64\ncogs        float64\ndltt        float64\ndlc         float64\nib          float64\ndtype: object\n\n\n\n# Clean firmid and dates\ncomp['dtdate'] = pd.to_datetime(comp['datadate'])\ncomp['year'] = comp['dtdate'].dt.year\ncomp['permno'] = comp['permno'].astype('int64')\ncomp.dtypes\n\npermno               int64\ndatadate            object\nat                 float64\nrevt               float64\ncogs               float64\ndltt               float64\ndlc                float64\nib                 float64\ndtdate      datetime64[ns]\nyear                 int64\ndtype: object\n\n\n\n# Keep postive total assets, sort and drop duplicates\ncomp = comp[comp['at']&gt;0].copy()\ncomp = comp.sort_values(['permno','year'])\ncomp = comp.drop_duplicates(['permno','year'], keep='last', ignore_index=True)\ncomp.head()\n\n\n\n\n\n\n\n\npermno\ndatadate\nat\nrevt\ncogs\ndltt\ndlc\nib\ndtdate\nyear\n\n\n\n\n0\n10000\n1986-10-31\n2.115\n1.026\n0.511\n0.058\n0.968\n-0.730\n1986-10-31\n1986\n\n\n1\n10001\n1986-06-30\n12.242\n21.460\n19.565\n2.946\n0.343\n0.669\n1986-06-30\n1986\n\n\n2\n10001\n1987-06-30\n11.771\n16.621\n15.538\n2.750\n0.377\n0.312\n1987-06-30\n1987\n\n\n3\n10001\n1988-06-30\n11.735\n16.978\n15.556\n2.555\n0.325\n0.542\n1988-06-30\n1988\n\n\n4\n10001\n1989-06-30\n18.565\n22.910\n19.856\n7.370\n0.185\n1.208\n1989-06-30\n1989\n\n\n\n\n\n\n\n\ncomp.shape\n\n(236316, 10)\n\n\n\n\nNew variables\n\n# Calculate some of the variables in Table 1\ncomp['at_lag1'] = comp.groupby('permno')['at'].shift(1)\ncomp['AG'] = comp['at'] / comp['at_lag1'] - 1\ncomp['L2AG'] = comp.groupby('permno')['AG'].shift(1)\n\ncomp['Leverage'] = (comp['dltt'] + comp['dlc']) / comp['at_lag1'] \ncomp['ROA'] = comp['ib'] / comp['at']\n\n\n# Keep only the variables we need, and observations with non-missing AG\ncomp = comp.loc[comp['AG'].notnull() ,:].copy()\ncomp_cgs = comp[['permno','year','dtdate','AG','L2AG', 'at','Leverage', 'ROA']].copy()\ncomp_cgs.head()\n\n\n\n\n\n\n\n\npermno\nyear\ndtdate\nAG\nL2AG\nat\nLeverage\nROA\n\n\n\n\n2\n10001\n1987\n1987-06-30\n-0.038474\nNaN\n11.771\n0.255432\n0.026506\n\n\n3\n10001\n1988\n1988-06-30\n-0.003058\n-0.038474\n11.735\n0.244669\n0.046187\n\n\n4\n10001\n1989\n1989-06-30\n0.582020\n-0.003058\n18.565\n0.643801\n0.065069\n\n\n5\n10001\n1990\n1990-06-30\n0.017021\n0.582020\n18.881\n0.396984\n0.059901\n\n\n6\n10001\n1991\n1991-06-30\n0.038028\n0.017021\n19.599\n0.380012\n0.054748\n\n\n\n\n\n\n\n\ncomp_cgs.shape\n\n(212504, 8)\n\n\n\n# Save this for later use\ncomp_cgs.to_pickle('../data/comp_cgs.zip')"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture20_backtesting_dataprep.html#market-data-crsp",
    "href": "teaching/FIN 525/lectures/lecture20_backtesting_dataprep.html#market-data-crsp",
    "title": "L20: Backtesting - data prep",
    "section": "Market data (CRSP)",
    "text": "Market data (CRSP)\n\nData Cleaning\n\n# Load CRSP data\ncrsp = pd.read_pickle('../data/crspm.zip')\ncrsp.shape\n\n(2553287, 10)\n\n\n\ncrsp.dtypes\n\npermno    float64\npermco    float64\ndate       object\nprc       float64\nret       float64\nshrout    float64\nshrcd     float64\nexchcd    float64\nsiccd     float64\nticker     object\ndtype: object\n\n\n\n# Keep only what we need\ncrsp = crsp[['permno','date','ret','prc','shrout','siccd']].copy().dropna()\ncrsp.shape\n\n(2530665, 6)\n\n\n\n# Get rid of financials (first digit of siccd = 6)\ncrsp = crsp.loc[crsp['siccd'].astype('string').str[0] != '6', :].copy()\ncrsp.shape\n\n(2048105, 6)\n\n\n\n# Clean up firmid and dates\ncrsp['dtdate'] = pd.to_datetime(crsp['date'])\ncrsp['mdate'] = crsp['dtdate'].dt.to_period('M')\ncrsp['permno'] = crsp['permno'].astype('int64')\n\n# Drop duplicates and sort\ncrsp = crsp.sort_values(['permno','mdate'])\ncrsp = crsp.drop_duplicates(['permno','mdate'], keep='last',ignore_index=True)\ncrsp.shape\n\n(2048105, 8)\n\n\n\n# Calculate market capitalization (in millions)\ncrsp['MV'] = crsp['prc'].abs() * crsp['shrout'] / 1000\n\n\n# Calculate lagged market cap\ncrsp.sort_values(['permno','mdate'], inplace=True)\ncrsp['mktcap_lag1'] = crsp.groupby('permno')['MV'].shift(1)    \n\n\n# Keep only the variables we need\ncrsp_cgs = crsp[['permno','mdate','dtdate','ret','MV','mktcap_lag1']].copy()\ncrsp_cgs.head(2)\n\n\n\n\n\n\n\n\npermno\nmdate\ndtdate\nret\nMV\nmktcap_lag1\n\n\n\n\n0\n10000\n1986-02\n1986-02-28\n-0.257143\n11.96\nNaN\n\n\n1\n10000\n1986-03\n1986-03-31\n0.365385\n16.33\n11.96\n\n\n\n\n\n\n\n\n# Save for later use\ncrsp_cgs.to_pickle('../data/crsp_cgs.zip')\n\n\n\nCalculate CRSP variables for Table 1\n\n# Calculate cummulative returns over the past 6 months (BHRET6)\ncrsp['BHRET6'] = 1 \nfor i in range(0,6):\n    crsp['BHRET6'] = crsp['BHRET6'] * (1 + crsp.groupby('permno')['ret'].shift(i))  \n    \ncrsp['BHRET6'] = crsp['BHRET6'] - 1         \ncrsp['BHRET6'].describe()\n\ncount    1.957695e+06\nmean     6.840459e-02\nstd      5.437771e-01\nmin     -9.999818e-01\n25%     -1.941748e-01\n50%      1.254532e-02\n75%      2.265447e-01\nmax      6.691429e+01\nName: BHRET6, dtype: float64\n\n\nChallenge:\nWrite a function called compound that calculates cumulative returns over a sequence of months and adds them as another column in the crsp dataframe (i.e. exactly what we did above, with BHRET6 but for arbitrary number of lags, not just 6).\n\ndef compound(dset=None, outvar=None, \n             firmid='permno', datevar='mdate', retvar='ret',\n             startlag=None, endlag=None):\n    \n    dset.sort_values([firmid,datevar], inplace=True) #side effect\n    \n    dset[outvar] = 1\n    for i in range(startlag,endlag+1):    \n        dset[outvar] = dset[outvar] * (1 + dset.groupby(firmid)[retvar].shift(i))\n        \n    dset[outvar] = dset[outvar] - 1\n    return\n\n\ncompound(crsp,'mybhret6',startlag=0, endlag=5)\ncrsp.head(2)\n\n\n\n\n\n\n\n\npermno\ndate\nret\nprc\nshrout\nsiccd\ndtdate\nmdate\nMV\nmktcap_lag1\nBHRET6\nmybhret6\n\n\n\n\n0\n10000\n1986-02-28\n-0.257143\n-3.2500\n3680.0\n3990.0\n1986-02-28\n1986-02\n11.96\nNaN\nNaN\nNaN\n\n\n1\n10000\n1986-03-31\n0.365385\n-4.4375\n3680.0\n3990.0\n1986-03-31\n1986-03\n16.33\n11.96\nNaN\nNaN\n\n\n\n\n\n\n\n\ncompound(crsp,'BHRET36',startlag=0, endlag=35)\ncrsp.head(2)\n\n\n\n\n\n\n\n\npermno\ndate\nret\nprc\nshrout\nsiccd\ndtdate\nmdate\nMV\nmktcap_lag1\nBHRET6\nmybhret6\nBHRET36\n\n\n\n\n0\n10000\n1986-02-28\n-0.257143\n-3.2500\n3680.0\n3990.0\n1986-02-28\n1986-02\n11.96\nNaN\nNaN\nNaN\nNaN\n\n\n1\n10000\n1986-03-31\n0.365385\n-4.4375\n3680.0\n3990.0\n1986-03-31\n1986-03\n16.33\n11.96\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\nChallenge:\nCreate a new variable called FBHRET12 which equals the (net) cumulative returns in the 12 months FOLLOWING the current month (t+1 to t+12 inclusive).\n\ncompound(crsp,'FBHRET12',startlag = -12, endlag = -1)\ncrsp.head(2)\n\n\n\n\n\n\n\n\npermno\ndate\nret\nprc\nshrout\nsiccd\ndtdate\nmdate\nMV\nmktcap_lag1\nBHRET6\nmybhret6\nBHRET36\nFBHRET12\n\n\n\n\n0\n10000\n1986-02-28\n-0.257143\n-3.2500\n3680.0\n3990.0\n1986-02-28\n1986-02\n11.96\nNaN\nNaN\nNaN\nNaN\n-0.875000\n\n\n1\n10000\n1986-03-31\n0.365385\n-4.4375\n3680.0\n3990.0\n1986-03-31\n1986-03\n16.33\n11.96\nNaN\nNaN\nNaN\n-0.943662\n\n\n\n\n\n\n\n\n# Keep only the variables we need we need for Table 1\ncrsp_table1 = crsp[['permno','mdate','MV','BHRET6','BHRET36','FBHRET12']].copy()\ncrsp_table1.head(7)\n\n\n\n\n\n\n\n\npermno\nmdate\nMV\nBHRET6\nBHRET36\nFBHRET12\n\n\n\n\n0\n10000\n1986-02\n11.960000\nNaN\nNaN\n-0.875000\n\n\n1\n10000\n1986-03\n16.330000\nNaN\nNaN\n-0.943662\n\n\n2\n10000\n1986-04\n15.172000\nNaN\nNaN\n-0.941406\n\n\n3\n10000\n1986-05\n11.793859\nNaN\nNaN\n-0.929648\n\n\n4\n10000\n1986-06\n11.734594\nNaN\nNaN\nNaN\n\n\n5\n10000\n1986-07\n10.786344\n-0.350000\nNaN\nNaN\n\n\n6\n10000\n1986-08\n4.148594\n-0.663462\nNaN\nNaN\n\n\n\n\n\n\n\n\n# Save for later use\ncrsp_table1.to_pickle('../data/crsp_cgs_table1.zip')"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture05_Functions_and_Packages.html",
    "href": "teaching/FIN 525/lectures/lecture05_Functions_and_Packages.html",
    "title": "L05: Functions, packages",
    "section": "",
    "text": "In this lecture we introduce “functions” which give us a way to re-use a piece of code without having to type it all out every time we need to use it. We show how to construct functions (“define” them) and how to use them (“call” them).\nWe then introduce “packages”, which give us a way to bundle multiple functions under a single name.\nFinally, we show how functions and packages give us a way to chain together multiple Python commands on a single line of code."
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture05_Functions_and_Packages.html#positional-vs-keyword-arguments",
    "href": "teaching/FIN 525/lectures/lecture05_Functions_and_Packages.html#positional-vs-keyword-arguments",
    "title": "L05: Functions, packages",
    "section": "Positional vs keyword arguments",
    "text": "Positional vs keyword arguments\nWhen you call (use) a function, you can provide arguments based on their name (keyword arguments), or based on their position (positional arguments)\n\ndef msg(name,age):\n    print(f\"{name} is {age} years old\")\n\nUsing positional arguments:\n\nmsg(\"Mihai\", 104)\n\nMihai is 104 years old\n\n\nUsing keyword arguments:\n\nmsg(name = \"Mihai\", age = 104)\n\nMihai is 104 years old\n\n\nThe benefit of using keyword arguments is that you don’t have to remember their order in the function definition, and accidentally do something like this:\n\nmsg(104,\"Mihai\")\n\n104 is Mihai years old\n\n\nAs long as you remember the keywords (“age” and “name” in our example), you don’t have to worry about the order in which you supply them to the function call:\n\nmsg(age = 104, name = \"Mihai\")\n\nMihai is 104 years old\n\n\nThe drawback of keyword arguments is that you have to remember the keywords:\n\nmsg(age = 5, n = \"Mihai\")\n\nTypeError: msg() got an unexpected keyword argument 'n'"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture05_Functions_and_Packages.html#required-vs-optional-parameters",
    "href": "teaching/FIN 525/lectures/lecture05_Functions_and_Packages.html#required-vs-optional-parameters",
    "title": "L05: Functions, packages",
    "section": "Required vs optional parameters",
    "text": "Required vs optional parameters\nRegardless of whether we use positional or keyword arguments, in the examples above, we always had to supply the correct number of arguments:\n\nmsg(\"mihai\",5, [123])\n\nTypeError: msg() takes 2 positional arguments but 3 were given\n\n\n\nmsg(name = 'mihai')\n\nTypeError: msg() missing 1 required positional argument: 'age'\n\n\nTo get around this issue, we can supply default values for some of the parameters when we define the function, even if these default values are “empty” (using the None value):\n\ndef msg2(name, age = 12):\n    print(f\"{name} is {age} years old\")\n\n\nmsg2(name = \"mihai\")\n\nmihai is 12 years old\n\n\nIf we don’t have to give a meaningful default value to a parameter, we can use the “None” value:\n\ndef msg2(name, age = None):\n    print(f\"{name} is {age} years old\")\n\n\nmsg2('mihai')\n\nmihai is None years old\n\n\nThe one rule you need to remember about optional parameters (ones with a default value) is that they need to be specified AFTER all the require parameters:\n\ndef msg3(name = '', age):\n    print('this will not work')\n\nSyntaxError: non-default argument follows default argument (Temp/ipykernel_15972/4155387656.py, line 1)"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture05_Functions_and_Packages.html#advanced-properties-of-functions-optional",
    "href": "teaching/FIN 525/lectures/lecture05_Functions_and_Packages.html#advanced-properties-of-functions-optional",
    "title": "L05: Functions, packages",
    "section": "Advanced properties of functions (OPTIONAL)",
    "text": "Advanced properties of functions (OPTIONAL)\n\nSide-effects\nIn some instances, functions can alter the value of the objects used as their parameters:\n\ndef changeme(x):\n    x[0] = \"weird\"\n\n\nmylist = [1,2,3]\nchangeme(mylist)\nprint(mylist)\n\n['weird', 2, 3]\n\n\nIn some instances, functions can NOT alter the value of their arguments:\n\ndef changeme2(x):\n    x = \"weird\"\n\n\nmylist = 12\nchangeme2(mylist)\nprint(mylist)\n\n12\n\n\nAs a general rule, functions can modify the values of arguments of mutable type (e.g. list, dict) but not the values of arguments of immutable type (e.g. int, str, tuple). If you are interested in more details, a nice discussion is provided here: https://realpython.com/defining-your-own-python-function/#argument-passing\n\n\nVariable-length argument lists\nYou don’t have to decide how many arguments your function should take when you define that function. For example, the function below can take in ANY number of arguments and prints them all out:\n\ndef printme(*args):\n    print(args)\n\n\nprintme(1,2,3)\n\n(1, 2, 3)\n\n\n\nprintme(1,2,3,4,5)\n\n(1, 2, 3, 4, 5)\n\n\nWe will not be using this functionality in this course but you can read more about it here: https://realpython.com/defining-your-own-python-function/#variable-length-argument-lists"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture05_Functions_and_Packages.html#packages-vs-modules-vs-scripts-optional",
    "href": "teaching/FIN 525/lectures/lecture05_Functions_and_Packages.html#packages-vs-modules-vs-scripts-optional",
    "title": "L05: Functions, packages",
    "section": "Packages vs modules vs scripts (OPTIONAL)",
    "text": "Packages vs modules vs scripts (OPTIONAL)\nSome terminology:\nPython scripts are “.py” files that are primarily meant to be run.\nPython modules are “.py” files that are primarily meant to be imported.\nPython packages are collections of Python modules.\nSo, technically speaking, the “MyPackage.py” file is a module.\nYou do not have to remember this terminology for the purpose of this class. We will USE packages that other people have written, but, to keep things simple, we will not write any scripts, modules, or packages of our own: we will execute all our code inside Jupyter Notebooks like this one.\nHowever, if you are serious about Python programming beyond the scope of this class, it is very important to understand exactly how packages, modules, and scripts work. A nice discussion can be found here: https://realpython.com/python-modules-packages/"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture06_pandas_intro.html",
    "href": "teaching/FIN 525/lectures/lecture06_pandas_intro.html",
    "title": "L06: Pandas intro",
    "section": "",
    "text": "Almost all our data analysis in this course will be performed using a new data structure called Pandas DataFrames. These data structures are implemented in the Pandas package, which comes with the Anaconda installation. It is customary to import the Pandas package as below.\n\nimport pandas as pd\n\nThe Pandas package has a very rich functionality. In this course we will cover only a subset of what one can do with this package. If you are interested in a higher level of detail than what is covered in this course, I strongly recommend the official user guide for the package:\nhttps://pandas.pydata.org/docs/user_guide/index.html\nWe will be referencing different parts of this user guide at different points in the class. You are not expected to know all the details in the user guide, only the parts that are covered in class, or in the practice problems.\nThe pandas package comes with many different functions/attributes. We will introduce some of the more commonly used attributes gradually over the course of the semester. Remember, in Pyhton, an object attribute is anything that you can put (using dot notation) after the name of that object. We will use the terms “attribute”, “method” and “function” interchangeably."
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture06_pandas_intro.html#reset_index",
    "href": "teaching/FIN 525/lectures/lecture06_pandas_intro.html#reset_index",
    "title": "L06: Pandas intro",
    "section": ".reset_index()",
    "text": ".reset_index()\nWe can switch to a numerical index for our dataframe using the .reset_index() function.\nSyntax:\nreset_index(level=None, drop=False, inplace=False, col_level=0, col_fill='')\nFor example:\n\ndf.reset_index()\n\n\n\n\n\n\n\n\nindex\nticker\nprice\n\n\n\n\n0\nTesla\nTSLA\n1000\n\n\n1\nApple\nAAPL\n2000\n\n\n\n\n\n\n\nNote how this pushed the old index inside the table itself, as a new column.\nVery important: the code above did not actually change df:\n\ndf\n\n\n\n\n\n\n\n\nticker\nprice\n\n\n\n\nTesla\nTSLA\n1000\n\n\nApple\nAAPL\n2000\n\n\n\n\n\n\n\nTo do that, we have to set the inplace parameter to True:\n\ndf.reset_index(inplace = True)\n\n\ndf\n\n\n\n\n\n\n\n\nindex\nticker\nprice\n\n\n\n\n0\nTesla\nTSLA\n1000\n\n\n1\nApple\nAAPL\n2000\n\n\n\n\n\n\n\nOr we can simply re-write the df dataframe:\n\ndf = df.reset_index()\n\n\ndf\n\n\n\n\n\n\n\n\nlevel_0\nindex\nticker\nprice\n\n\n\n\n0\n0\nTesla\nTSLA\n1000\n\n\n1\n1\nApple\nAAPL\n2000"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture06_pandas_intro.html#set_index",
    "href": "teaching/FIN 525/lectures/lecture06_pandas_intro.html#set_index",
    "title": "L06: Pandas intro",
    "section": ".set_index()",
    "text": ".set_index()\nWe can use the data in one of the columns, as the new index of the table with the .set_index() function:\nSyntax:\nset_index(keys, drop=True, append=False, inplace=False, verify_integrity=False)\nFor example:\n\ndf = df.set_index('ticker')\n\n\ndf\n\n\n\n\n\n\n\n\nlevel_0\nindex\nprice\n\n\nticker\n\n\n\n\n\n\n\nTSLA\n0\nTesla\n1000\n\n\nAAPL\n1\nApple\n2000\n\n\n\n\n\n\n\nNote that, once we make the ‘ticker’ column into an index, it stops showing up as a column:\n\ndf.columns\n\nIndex(['level_0', 'index', 'price'], dtype='object')\n\n\nSo if we try to set the index to ‘ticker’ again, we will get an error, because Python can not find the ‘ticker’ column:\n\n#df.set_index('ticker') #this won't work"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture06_pandas_intro.html#rename",
    "href": "teaching/FIN 525/lectures/lecture06_pandas_intro.html#rename",
    "title": "L06: Pandas intro",
    "section": ".rename()",
    "text": ".rename()\nWe can rename one or more of the index values using the .rename() function.\nSyntax:\nrename(mapper=None, index=None, columns=None, axis=None, copy=True, inplace=False, level=None, errors='ignore')\nWe will supply a dictionary to the index parameter, where the keys are the current names we want to change in the index, and the values are the new names we want to use.\nFor example:\n\ndf.rename(index = {'TSLA': 'tsla'})\n\n\n\n\n\n\n\n\nlevel_0\nindex\nprice\n\n\nticker\n\n\n\n\n\n\n\ntsla\n0\nTesla\n1000\n\n\nAAPL\n1\nApple\n2000\n\n\n\n\n\n\n\nAs with .set_index(), .rename() does not actually change the df dataframe unless we set inplace = True or we redefine the df variable. This behavior should be expected of any pandas function that has an inplace parameter:\n\ndf\n\n\n\n\n\n\n\n\nlevel_0\nindex\nprice\n\n\nticker\n\n\n\n\n\n\n\nTSLA\n0\nTesla\n1000\n\n\nAAPL\n1\nApple\n2000"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture06_pandas_intro.html#rename-1",
    "href": "teaching/FIN 525/lectures/lecture06_pandas_intro.html#rename-1",
    "title": "L06: Pandas intro",
    "section": ".rename()",
    "text": ".rename()\nWe can also rename one or more of the columns using the .rename() function.\nThis time, we will supply a dictionary to the columns parameter, where the keys are the current names of the columns we want to change, and the values are the new names we want to use.\nFor example\n\nndf = ndf.rename(columns = {'ticker': 't', 'price':'p'})\n\n\nndf\n\n\n\n\n\n\n\n\nt\np\n\n\n\n\nTesla\nTSLA\n1000\n\n\nApple\nAAPL\n2000"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture06_pandas_intro.html#accessing-and-creating-entire-columns",
    "href": "teaching/FIN 525/lectures/lecture06_pandas_intro.html#accessing-and-creating-entire-columns",
    "title": "L06: Pandas intro",
    "section": "Accessing and creating entire columns",
    "text": "Accessing and creating entire columns\nWe can access the data in an entire column of a dataframe using square brackets after the dataframe name:\n\nndf['t']\n\nTesla    TSLA\nApple    AAPL\nName: t, dtype: object\n\n\nIf we want to access multiple columns at once, we have to put their names in a list:\n\nndf[['t','p']]\n\n\n\n\n\n\n\n\nt\np\n\n\n\n\nTesla\nTSLA\n1000\n\n\nApple\nAAPL\n2000\n\n\n\n\n\n\n\nWe can create new columns by just supplying the data manually:\n\nndf['new'] = [1,2]\nndf\n\n\n\n\n\n\n\n\nt\np\nnew\n\n\n\n\nTesla\nTSLA\n1000\n1\n\n\nApple\nAAPL\n2000\n2\n\n\n\n\n\n\n\nBut most of the time, new columns will be created by bringing them over from other dataframes (we’ll cover this in a future lecture) or by some calculation involving other existing columns from the dataframe.\nFor example:\n\nndf['somecalc'] = ndf['p'] + ndf['new']\nndf\n\n\n\n\n\n\n\n\nt\np\nnew\nsomecalc\n\n\n\n\nTesla\nTSLA\n1000\n1\n1001\n\n\nApple\nAAPL\n2000\n2\n2002"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture12_descriptive_stats_unconditional.html",
    "href": "teaching/FIN 525/lectures/lecture12_descriptive_stats_unconditional.html",
    "title": "L12: Descriptive stats",
    "section": "",
    "text": "import pandas as pd\nimport pandas_datareader as pdr\n\n\n# Download data on Fama-French three factors (we will use this data in all our examples)\nff3 = pdr.DataReader('F-F_Research_Data_Factors', 'famafrench', \n                     '1970-01-01','2020-12-31'\n                    )[0]/100\nff3\n\n\n\n\n\n\n\n\nMkt-RF\nSMB\nHML\nRF\n\n\nDate\n\n\n\n\n\n\n\n\n1970-01\n-0.0810\n0.0295\n0.0312\n0.0060\n\n\n1970-02\n0.0513\n-0.0256\n0.0393\n0.0062\n\n\n1970-03\n-0.0106\n-0.0230\n0.0399\n0.0057\n\n\n1970-04\n-0.1100\n-0.0612\n0.0617\n0.0050\n\n\n1970-05\n-0.0692\n-0.0456\n0.0332\n0.0053\n\n\n...\n...\n...\n...\n...\n\n\n2020-08\n0.0763\n-0.0022\n-0.0293\n0.0001\n\n\n2020-09\n-0.0363\n-0.0004\n-0.0266\n0.0001\n\n\n2020-10\n-0.0210\n0.0439\n0.0419\n0.0001\n\n\n2020-11\n0.1247\n0.0574\n0.0199\n0.0001\n\n\n2020-12\n0.0463\n0.0483\n-0.0156\n0.0001\n\n\n\n\n612 rows × 4 columns\n\n\n\n\n# Rename for convenience\nff3.rename(columns = {'Mkt-RF': 'MKT'}, inplace = True)\nff3.head(2)\n\n\n\n\n\n\n\n\nMKT\nSMB\nHML\nRF\n\n\nDate\n\n\n\n\n\n\n\n\n1970-01\n-0.0810\n0.0295\n0.0312\n0.0060\n\n\n1970-02\n0.0513\n-0.0256\n0.0393\n0.0062"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture12_descriptive_stats_unconditional.html#line-plots",
    "href": "teaching/FIN 525/lectures/lecture12_descriptive_stats_unconditional.html#line-plots",
    "title": "L12: Descriptive stats",
    "section": "Line plots",
    "text": "Line plots\nNote that, by default, plot() creates a “line” plot, using the index of the dataframe for the x axis (in our case, the Date):\n\nff3.plot();\n\n\n\n\nYou can specify which variables you want plotted by subsetting the overall dataframe first:\n\nff3[['MKT','RF']].plot();\n\n\n\n\nBelow, we show more of the functionality of .plot() through a more involved example:\n\nff3[['MKT','RF']].plot(kind = 'line', \n                       title = 'Market and risk-free returns',\n                       xlabel = 'Year', ylabel = 'Return(%)',\n                       legend = True,\n                       grid = True,\n                      figsize = (10,3));"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture12_descriptive_stats_unconditional.html#scatter-plots",
    "href": "teaching/FIN 525/lectures/lecture12_descriptive_stats_unconditional.html#scatter-plots",
    "title": "L12: Descriptive stats",
    "section": "Scatter plots",
    "text": "Scatter plots\nTo create a scatter plot, we need to change the kind parameter to “scatter” and also specify what is on the x axis and what is on the y axis:\n\nff3.plot(x = 'RF', y = 'MKT', \n         kind = 'scatter', \n         xlabel = 'Risk-free returns', ylabel = 'Market returns',\n         grid = True);"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture12_descriptive_stats_unconditional.html#histograms",
    "href": "teaching/FIN 525/lectures/lecture12_descriptive_stats_unconditional.html#histograms",
    "title": "L12: Descriptive stats",
    "section": "Histograms",
    "text": "Histograms\nFor a histogram, we use kind='hist' and then use subplots=True to specify that we want each variable to have its own histogram, in a separate subplot:\n\nff3[['MKT','RF']].plot(kind = 'hist', subplots = True, bins = 20);\n\n\n\n\nWe can change the position of the subplots relative to each other using the layout parameter:\n\nff3[['MKT','RF']].plot(kind = 'hist', subplots = True, bins = 20, \n                       layout = (1,2));\n\n\n\n\nWe can create a continuous approximation of the histogram using kind='density':\n\nff3[['MKT','RF']].plot(kind = 'density', subplots = True);"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture12_descriptive_stats_unconditional.html#box-plots",
    "href": "teaching/FIN 525/lectures/lecture12_descriptive_stats_unconditional.html#box-plots",
    "title": "L12: Descriptive stats",
    "section": "Box plots",
    "text": "Box plots\nFor box plots, we use kind='box':\n\nff3[['MKT','RF']].plot(kind = 'box', subplots = True, \n                       figsize = (10,4), sharey = False);"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture12_descriptive_stats_unconditional.html#the-.describe-function",
    "href": "teaching/FIN 525/lectures/lecture12_descriptive_stats_unconditional.html#the-.describe-function",
    "title": "L12: Descriptive stats",
    "section": "The .describe() function",
    "text": "The .describe() function\nWe can use the .describe() function to get some standard descriptive statistics for the entire dataset.\nSyntax:\nDataFrame.describe(percentiles=None, include=None, exclude=None, datetime_is_numeric=False)\nThe default is for .describe() to produce summary statistics only for numerical data types in the dataframe. You can change this with the include and exclude parameters.\n\nff3.describe()\n\n\n\n\n\n\n\n\nMKT\nSMB\nHML\nRF\n\n\n\n\ncount\n612.000000\n612.000000\n612.00000\n612.000000\n\n\nmean\n0.005926\n0.001404\n0.00253\n0.003723\n\n\nstd\n0.045952\n0.030765\n0.02963\n0.002801\n\n\nmin\n-0.232400\n-0.172900\n-0.14020\n0.000000\n\n\n25%\n-0.020200\n-0.017400\n-0.01435\n0.001300\n\n\n50%\n0.010250\n0.001150\n0.00215\n0.003900\n\n\n75%\n0.035775\n0.018950\n0.01735\n0.005300\n\n\nmax\n0.161000\n0.214800\n0.12480\n0.013500\n\n\n\n\n\n\n\nThe percentiles parameter allows you to specify which percentiles you want .describe() to calculate (default is 25th, 50th and 75th percentiles). For example, below, we only ask for the 50th percentile (the median):\n\nff3[['SMB','HML']].describe(percentiles = [0.5])\n\n\n\n\n\n\n\n\nSMB\nHML\n\n\n\n\ncount\n612.000000\n612.00000\n\n\nmean\n0.001404\n0.00253\n\n\nstd\n0.030765\n0.02963\n\n\nmin\n-0.172900\n-0.14020\n\n\n50%\n0.001150\n0.00215\n\n\nmax\n0.214800\n0.12480"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture12_descriptive_stats_unconditional.html#calculating-individual-statistics",
    "href": "teaching/FIN 525/lectures/lecture12_descriptive_stats_unconditional.html#calculating-individual-statistics",
    "title": "L12: Descriptive stats",
    "section": "Calculating individual statistics",
    "text": "Calculating individual statistics\nEach individual statistic produced by .describe() has its own function that can be applied either to the entire dataframe or to subsets of it. Below I only show examples for mean, variance, standard deviation and median (but you can also use .count(), .min(), .max(), .sum() and many others).\n\n.mean()\n\nprint('Means:') \nff3.mean()\n\nMeans:\n\n\nMKT    0.005926\nSMB    0.001404\nHML    0.002530\nRF     0.003723\ndtype: float64\n\n\n\n\n.var()\n\nprint('Variances:')\nff3.var()\n\nVariances:\n\n\nMKT    0.002112\nSMB    0.000946\nHML    0.000878\nRF     0.000008\ndtype: float64\n\n\n\n\n.std()\n\nprint('Standard deviations')\nff3.std()\n\nStandard deviations\n\n\nMKT    0.045952\nSMB    0.030765\nHML    0.029630\nRF     0.002801\ndtype: float64\n\n\n\n\n.median()\n\nprint('Medians:')\nff3.median()\n\nMedians:\n\n\nMKT    0.01025\nSMB    0.00115\nHML    0.00215\nRF     0.00390\ndtype: float64\n\n\nChallenge:\nNote that the output of .describe() is also a dataframe. So we can use .loc[] to access specific numbers in that output table.\nUse the space below to calculate and print the interquartile range (IQR = percentile 75 minus percentile 25) for the ‘MKT’ variable:\n\na = ff3.describe()\na\n\n\n\n\n\n\n\n\nMKT\nSMB\nHML\nRF\n\n\n\n\ncount\n612.000000\n612.000000\n612.00000\n612.000000\n\n\nmean\n0.005926\n0.001404\n0.00253\n0.003723\n\n\nstd\n0.045952\n0.030765\n0.02963\n0.002801\n\n\nmin\n-0.232400\n-0.172900\n-0.14020\n0.000000\n\n\n25%\n-0.020200\n-0.017400\n-0.01435\n0.001300\n\n\n50%\n0.010250\n0.001150\n0.00215\n0.003900\n\n\n75%\n0.035775\n0.018950\n0.01735\n0.005300\n\n\nmax\n0.161000\n0.214800\n0.12480\n0.013500\n\n\n\n\n\n\n\n\na.index\n\nIndex(['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max'], dtype='object')\n\n\n\nprint(a.loc['75%','MKT'] - a.loc['25%','MKT'])\n\n0.055975"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture12_descriptive_stats_unconditional.html#calculating-row-level-statistics",
    "href": "teaching/FIN 525/lectures/lecture12_descriptive_stats_unconditional.html#calculating-row-level-statistics",
    "title": "L12: Descriptive stats",
    "section": "Calculating row-level statistics",
    "text": "Calculating row-level statistics\nAll statistical functions in Pandas (e.g. .mean(), .median(), etc) have an axis argument that allows you to specify if you want that statistic to be calculated column-wise (axis=0, the default) or row-wise (axis=1).\nFor example, if we want to know, each month, which of the columns in ff3 had the highest return, we would use:\n\nff3.max(axis = 1)\n\nDate\n1970-01    0.0312\n1970-02    0.0513\n1970-03    0.0399\n1970-04    0.0617\n1970-05    0.0332\n            ...  \n2020-08    0.0763\n2020-09    0.0001\n2020-10    0.0439\n2020-11    0.1247\n2020-12    0.0483\nFreq: M, Length: 612, dtype: float64\n\n\n\nff3.head(2) #quick check if the above worked\n\n\n\n\n\n\n\n\nMKT\nSMB\nHML\nRF\n\n\nDate\n\n\n\n\n\n\n\n\n1970-01\n-0.0810\n0.0295\n0.0312\n0.0060\n\n\n1970-02\n0.0513\n-0.0256\n0.0393\n0.0062\n\n\n\n\n\n\n\nAs usual, we can also calculate row-wise statistics using only a subset of the columns:\n\nff3[['SMB','HML']].max(axis=1)\n\nDate\n1970-01    0.0312\n1970-02    0.0393\n1970-03    0.0399\n1970-04    0.0617\n1970-05    0.0332\n            ...  \n2020-08   -0.0022\n2020-09   -0.0004\n2020-10    0.0439\n2020-11    0.0574\n2020-12    0.0483\nFreq: M, Length: 612, dtype: float64"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture12_descriptive_stats_unconditional.html#creating-your-own-list-of-summary-statistics-with-the-.agg-function",
    "href": "teaching/FIN 525/lectures/lecture12_descriptive_stats_unconditional.html#creating-your-own-list-of-summary-statistics-with-the-.agg-function",
    "title": "L12: Descriptive stats",
    "section": "Creating your own list of summary statistics with the .agg() function",
    "text": "Creating your own list of summary statistics with the .agg() function\nIf we want a different selection of summary statistics than the one offered by the .describe() function, we can use the .agg() function to specify exactly which statistics we want:\nSyntax:\nDataFrame.agg(func=None, axis=0, *args, **kwargs)\nIf you want the same stats for all variables, just provide a list of the names of the functions you want to be used (e.g. use ‘mean’ for the .mean() function, ‘std’ for the .std() function etc.).\n\nff3.agg(func = ['mean','std','median','sum'])\n\n\n\n\n\n\n\n\nMKT\nSMB\nHML\nRF\n\n\n\n\nmean\n0.005926\n0.001404\n0.00253\n0.003723\n\n\nstd\n0.045952\n0.030765\n0.02963\n0.002801\n\n\nmedian\n0.010250\n0.001150\n0.00215\n0.003900\n\n\nsum\n3.626600\n0.859400\n1.54860\n2.278300\n\n\n\n\n\n\n\nYou can also specify different functions (stats) for each variable:\n\nff3.agg(func = {'MKT':['mean','median'], 'SMB':['mean','std']})\n\n\n\n\n\n\n\n\nMKT\nSMB\n\n\n\n\nmean\n0.005926\n0.001404\n\n\nmedian\n0.010250\nNaN\n\n\nstd\nNaN\n0.030765\n\n\n\n\n\n\n\nChallenge:\nCreate a table that shows just the mean and standard deviation for the SMB and HML variables\n\nff3[['SMB','HML']].agg(['mean','std'])\n\n\n\n\n\n\n\n\nSMB\nHML\n\n\n\n\nmean\n0.001404\n0.00253\n\n\nstd\n0.030765\n0.02963"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture12_descriptive_stats_unconditional.html#covariance-.cov",
    "href": "teaching/FIN 525/lectures/lecture12_descriptive_stats_unconditional.html#covariance-.cov",
    "title": "L12: Descriptive stats",
    "section": "Covariance: .cov()",
    "text": "Covariance: .cov()\nThe cov() function produces a covariance matrix for the variables (columns) in the dataframe. The numbers on the diagonal are actually variances. Each number on the off-diagonal is the covariance between the two variables specified in the column/row headers.\n\nff3.cov()\n\n\n\n\n\n\n\n\nMKT\nSMB\nHML\nRF\n\n\n\n\nMKT\n0.002112\n0.000405\n-0.000308\n-0.000011\n\n\nSMB\n0.000405\n0.000946\n-0.000163\n-0.000004\n\n\nHML\n-0.000308\n-0.000163\n0.000878\n0.000009\n\n\nRF\n-0.000011\n-0.000004\n0.000009\n0.000008\n\n\n\n\n\n\n\nThe output table above is a dataframe, so we can access individual numbers in it using the .loc[] operator.\nFor example, below, we extract the covariance between the ‘MKT’ and ‘SMB’ variables:\n\nff3.cov().loc['MKT', 'SMB']\n\n0.0004047944073253961\n\n\nRemember, if you want to use these estimates later on, you need to store them as new variables:\n\ncovmat = ff3.cov()\ncovmat\n\n\n\n\n\n\n\n\nMKT\nSMB\nHML\nRF\n\n\n\n\nMKT\n0.002112\n0.000405\n-0.000308\n-0.000011\n\n\nSMB\n0.000405\n0.000946\n-0.000163\n-0.000004\n\n\nHML\n-0.000308\n-0.000163\n0.000878\n0.000009\n\n\nRF\n-0.000011\n-0.000004\n0.000009\n0.000008\n\n\n\n\n\n\n\n\ncov_mkt_smb = covmat.loc['MKT', 'SMB']\ncov_mkt_smb\n\n0.0004047944073253961"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture12_descriptive_stats_unconditional.html#correlation-.corr",
    "href": "teaching/FIN 525/lectures/lecture12_descriptive_stats_unconditional.html#correlation-.corr",
    "title": "L12: Descriptive stats",
    "section": "Correlation: .corr()",
    "text": "Correlation: .corr()\nJust like with covariance, we can calculate a correlation matrix for the entire dataset:\n\nff3.corr()\n\n\n\n\n\n\n\n\nMKT\nSMB\nHML\nRF\n\n\n\n\nMKT\n1.000000\n0.286331\n-0.225911\n-0.089092\n\n\nSMB\n0.286331\n1.000000\n-0.178917\n-0.044725\n\n\nHML\n-0.225911\n-0.178917\n1.000000\n0.108959\n\n\nRF\n-0.089092\n-0.044725\n0.108959\n1.000000\n\n\n\n\n\n\n\nOr we can extract the correlation of a particular pair of variables in your dataset:\n\nff3.corr().loc['MKT', 'SMB']\n\n0.2863310434640219\n\n\nChallenge:\nCalculate the correlation between ‘MKT’ returns in the current month and the SMB return from 12 months ago.\n\nsmb = ff3['SMB'].to_frame()\nsmb['smb_lag12'] = smb['SMB'].shift(12)\nsmb.corr()\n\n\n\n\n\n\n\n\nSMB\nsmb_lag12\n\n\n\n\nSMB\n1.000000\n0.094283\n\n\nsmb_lag12\n0.094283\n1.000000"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture12_descriptive_stats_unconditional.html#autocorrelation-.autocorr",
    "href": "teaching/FIN 525/lectures/lecture12_descriptive_stats_unconditional.html#autocorrelation-.autocorr",
    "title": "L12: Descriptive stats",
    "section": "Autocorrelation: .autocorr()",
    "text": "Autocorrelation: .autocorr()\nThe autocorrelation of a variable is the correlation between its current value and a value from the past. So there is not one single autocorrelation for any given variable, there is one autocorrelation for every “lag” between the current value and the value from the past. For example, below, we calculate the “1-month autocorrelation” and “12-month autocorrelation” for the market portfolio returns:\n\nmkt_acor1 = ff3['MKT'].autocorr(lag = 1) \nprint(\"1-month autocorrelation of market returns:\", mkt_acor1)\n\n1-month autocorrelation of market returns: 0.06457337999767149\n\n\n\nmkt_acor12 = ff3['MKT'].autocorr(lag = 12) \nprint(\"12-month autocorrelation of market returns:\", mkt_acor12)\n\n12-month autocorrelation of market returns: 0.02828759092532415\n\n\nAnd below we verify that the autocorrelation is nothing but the correlation between the current value and a lagged value:\n\nff3['mkt_lag1'] = ff3['MKT'].shift(1)\nff3.head()\n\n\n\n\n\n\n\n\nMKT\nSMB\nHML\nRF\nmkt_lag1\n\n\nDate\n\n\n\n\n\n\n\n\n\n1970-01\n-0.0810\n0.0295\n0.0312\n0.0060\nNaN\n\n\n1970-02\n0.0513\n-0.0256\n0.0393\n0.0062\n-0.0810\n\n\n1970-03\n-0.0106\n-0.0230\n0.0399\n0.0057\n0.0513\n\n\n1970-04\n-0.1100\n-0.0612\n0.0617\n0.0050\n-0.0106\n\n\n1970-05\n-0.0692\n-0.0456\n0.0332\n0.0053\n-0.1100\n\n\n\n\n\n\n\n\nff3[['MKT','mkt_lag1']].corr()\n\n\n\n\n\n\n\n\nMKT\nmkt_lag1\n\n\n\n\nMKT\n1.000000\n0.064573\n\n\nmkt_lag1\n0.064573\n1.000000"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture12_descriptive_stats_unconditional.html#fixed-window-rolling-statistics",
    "href": "teaching/FIN 525/lectures/lecture12_descriptive_stats_unconditional.html#fixed-window-rolling-statistics",
    "title": "L12: Descriptive stats",
    "section": "Fixed-window rolling statistics",
    "text": "Fixed-window rolling statistics\nWe use the .rolling() function to calculate summary statistics at each point in time “t” using only the observations from “t - w” to “t”, where “w” is referred to as the “window” length.\nSyntax:\nDataFrame.rolling(window, min_periods=None, center=False, win_type=None, on=None, axis=0, closed=None, method='single')\nAs an example, below, we calculate 60-month rolling means (i.e. “w” is 60) for all the variables in ff3:\n\nrolling_means = ff3.rolling(60).mean()\nrolling_means\n\n\n\n\n\n\n\n\nMKT\nSMB\nHML\nRF\nmkt_lag1\n\n\nDate\n\n\n\n\n\n\n\n\n\n1970-01\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1970-02\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1970-03\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1970-04\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1970-05\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n\n\n2020-08\n0.011497\n-0.001702\n-0.008360\n0.000888\n0.009218\n\n\n2020-09\n0.011403\n-0.001270\n-0.008900\n0.000890\n0.011497\n\n\n2020-10\n0.009762\n-0.000227\n-0.008127\n0.000892\n0.011403\n\n\n2020-11\n0.011747\n0.000130\n-0.007732\n0.000893\n0.009762\n\n\n2020-12\n0.012880\n0.001403\n-0.007560\n0.000893\n0.011747\n\n\n\n\n612 rows × 5 columns\n\n\n\n\nrolling_means['MKT'].plot();\n\n\n\n\nWe can calculate rolling versions for all summary statistics that the pandas package knows how to calculate. For example, below, we calculate the rolling, 36-month standard deviations of market returns, and we plot these over time:\n\nff3['MKT'].rolling(36).std().plot();\n\n\n\n\nWe can even calculate rolling versions of two-variable summary statistics (like correlation and covariance). However, we have to remember that .corr() and .cov() produce matrices not single numbers. So if we want rolling correlations between, say, market returns and the risk-free rate, the cell below will produce a correlation matrix at each point in time:\n\nnot_like_this = ff3[['MKT','RF']].rolling(60).corr()\nnot_like_this\n\n\n\n\n\n\n\n\n\nMKT\nRF\n\n\nDate\n\n\n\n\n\n\n\n1970-01\nMKT\nNaN\nNaN\n\n\nRF\nNaN\nNaN\n\n\n1970-02\nMKT\nNaN\nNaN\n\n\nRF\nNaN\nNaN\n\n\n1970-03\nMKT\nNaN\nNaN\n\n\n...\n...\n...\n...\n\n\n2020-10\nRF\n-0.171608\n1.000000\n\n\n2020-11\nMKT\n1.000000\n-0.212314\n\n\nRF\n-0.212314\n1.000000\n\n\n2020-12\nMKT\n1.000000\n-0.240182\n\n\nRF\n-0.240182\n1.000000\n\n\n\n\n1224 rows × 2 columns\n\n\n\nInstead, we need to supply one of the variables as a parameter to the .corr() function:\n\njust_the_coeff = ff3['MKT'].rolling(60).corr(ff3['RF'])\njust_the_coeff\n\nDate\n1970-01         NaN\n1970-02         NaN\n1970-03         NaN\n1970-04         NaN\n1970-05         NaN\n             ...   \n2020-08   -0.213687\n2020-09   -0.213742\n2020-10   -0.171608\n2020-11   -0.212314\n2020-12   -0.240182\nFreq: M, Length: 612, dtype: float64\n\n\n\njust_the_coeff.plot();"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture12_descriptive_stats_unconditional.html#expanding-window-rolling-statistics",
    "href": "teaching/FIN 525/lectures/lecture12_descriptive_stats_unconditional.html#expanding-window-rolling-statistics",
    "title": "L12: Descriptive stats",
    "section": "Expanding-window rolling statistics",
    "text": "Expanding-window rolling statistics\nWith expanding-window summary statistics, at each point in time, we use all the available data up to that point to calculate the statistic. We use the .expanding() function for this purpose, which also gives us the option to specify that we want to calculate the statistic only if we have a minimum number of observations available at that point (see the min_period parameter below):\nSyntax:\nDataFrame.expanding(min_periods=1, center=None, axis=0, method='single')\nNote that, if we don’t supply a large enough min_periods, in the beginning of the sample, the statistics will be calculated using a very low number of observations (starting with 1), so they will be quite volatile:\n\nff3['MKT'].expanding().mean().plot();\n\n\n\n\nThis looks a lot more stable if we make sure each statistic is calculated using at least 36 observations:\n\nff3['MKT'].expanding(min_periods = 36).mean().plot();\n\n\n\n\nAs another example, let’s look at the behavior of market volatility over time:\n\nff3['MKT'].expanding(min_periods = 60).std().plot();\n\n\n\n\nFinally, below, we see that the correlation between market returns and tbill yields, while changing over time, is negative throughout (when we do not restrict ourselves to just the prior 60 observations):\n\nff3['MKT'].expanding(min_periods = 60).corr(ff3['RF']).plot();"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture21_sortvar_sumstats.html",
    "href": "teaching/FIN 525/lectures/lecture21_sortvar_sumstats.html",
    "title": "L21: Backtesting - sumstats",
    "section": "",
    "text": "import pandas as pd\nimport statsmodels.api as sm\npd.options.display.max_rows = 20"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture21_sortvar_sumstats.html#load-cleaned-data-from-last-lecture",
    "href": "teaching/FIN 525/lectures/lecture21_sortvar_sumstats.html#load-cleaned-data-from-last-lecture",
    "title": "L21: Backtesting - sumstats",
    "section": "Load cleaned data from last lecture",
    "text": "Load cleaned data from last lecture\n\n# Load cleaned Compustat data from last lecture\ncomp_cgs = pd.read_pickle('../data/comp_cgs.zip')\ncomp_cgs.head(2)\n\n\n\n\n\n\n\n\npermno\nyear\ndtdate\nAG\nL2AG\nat\nLeverage\nROA\n\n\n\n\n2\n10001\n1987\n1987-06-30\n-0.038474\nNaN\n11.771\n0.255432\n0.026506\n\n\n3\n10001\n1988\n1988-06-30\n-0.003058\n-0.038474\n11.735\n0.244669\n0.046187\n\n\n\n\n\n\n\n\n# Load cleaned CRSP data (for table 1) from last lecture\ncrsp_table1 = pd.read_pickle('../data/crsp_cgs_table1.zip')\ncrsp_table1.head(2)\n\n\n\n\n\n\n\n\npermno\nmdate\nMV\nBHRET6\nBHRET36\nFBHRET12\n\n\n\n\n0\n10000\n1986-02\n11.96\nNaN\nNaN\n-0.875000\n\n\n1\n10000\n1986-03\n16.33\nNaN\nNaN\n-0.943662"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture21_sortvar_sumstats.html#merge-compustat-data-from-year-t-with-crsp-data-from-june-year-t1",
    "href": "teaching/FIN 525/lectures/lecture21_sortvar_sumstats.html#merge-compustat-data-from-year-t-with-crsp-data-from-june-year-t1",
    "title": "L21: Backtesting - sumstats",
    "section": "Merge Compustat data from year t, with CRSP data from June, year t+1",
    "text": "Merge Compustat data from year t, with CRSP data from June, year t+1\n\n# Keep only the CRSP data in Junes\ncrsp_table1 = crsp_table1.loc[crsp_table1['mdate'].dt.month==6,:].copy()\ncrsp_table1.head()\n\n\n\n\n\n\n\n\npermno\nmdate\nMV\nBHRET6\nBHRET36\nFBHRET12\n\n\n\n\n4\n10000\n1986-06\n11.734594\nNaN\nNaN\nNaN\n\n\n20\n10001\n1986-06\n6.033125\nNaN\nNaN\n0.023884\n\n\n32\n10001\n1987-06\n5.822125\n-0.131644\nNaN\n0.140535\n\n\n44\n10001\n1988-06\n6.200000\n0.101995\nNaN\n0.199810\n\n\n56\n10001\n1989-06\n7.007000\n0.136718\n0.401108\n0.468434\n\n\n\n\n\n\n\n\n# Create year variable in CRSP data\ncrsp_table1['crsp_year'] = crsp_table1['mdate'].dt.year\ncrsp_table1.head()\n\n\n\n\n\n\n\n\npermno\nmdate\nMV\nBHRET6\nBHRET36\nFBHRET12\ncrsp_year\n\n\n\n\n4\n10000\n1986-06\n11.734594\nNaN\nNaN\nNaN\n1986\n\n\n20\n10001\n1986-06\n6.033125\nNaN\nNaN\n0.023884\n1986\n\n\n32\n10001\n1987-06\n5.822125\n-0.131644\nNaN\n0.140535\n1987\n\n\n44\n10001\n1988-06\n6.200000\n0.101995\nNaN\n0.199810\n1988\n\n\n56\n10001\n1989-06\n7.007000\n0.136718\n0.401108\n0.468434\n1989\n\n\n\n\n\n\n\n\n# Add 1 to the year in Compustat, so that accounting data in year t gets merged with returns from year t+1\ncomp_cgs['next_year'] = comp_cgs['year'] + 1\n\n\n# Merge Compustat data from year t, with CRSP data from June, year t+1\nandata = comp_cgs.merge(crsp_table1, how = 'inner',\n                        left_on = ['permno','next_year'], right_on = ['permno','crsp_year'])\nandata.head()\n\n\n\n\n\n\n\n\npermno\nyear\ndtdate\nAG\nL2AG\nat\nLeverage\nROA\nnext_year\nmdate\nMV\nBHRET6\nBHRET36\nFBHRET12\ncrsp_year\n\n\n\n\n0\n10001\n1987\n1987-06-30\n-0.038474\nNaN\n11.771\n0.255432\n0.026506\n1988\n1988-06\n6.20000\n0.101995\nNaN\n0.199810\n1988\n\n\n1\n10001\n1988\n1988-06-30\n-0.003058\n-0.038474\n11.735\n0.244669\n0.046187\n1989\n1989-06\n7.00700\n0.136718\n0.401108\n0.468434\n1989\n\n\n2\n10001\n1989\n1989-06-30\n0.582020\n-0.003058\n18.565\n0.643801\n0.065069\n1990\n1990-06\n10.05225\n-0.011095\n1.009441\n0.140009\n1990\n\n\n3\n10001\n1990\n1990-06-30\n0.017021\n0.582020\n18.881\n0.396984\n0.059901\n1991\n1991-06\n11.26650\n0.137278\n1.008516\n0.175542\n1991\n\n\n4\n10001\n1991\n1991-06-30\n0.038028\n0.017021\n19.599\n0.380012\n0.054748\n1992\n1992-06\n12.63125\n-0.168310\n0.967890\n0.466211\n1992\n\n\n\n\n\n\n\n\nandata.shape\n\n(138746, 15)"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture21_sortvar_sumstats.html#differences-between-high--and-low-ag-firms-spreads",
    "href": "teaching/FIN 525/lectures/lecture21_sortvar_sumstats.html#differences-between-high--and-low-ag-firms-spreads",
    "title": "L21: Backtesting - sumstats",
    "section": "Differences between high- and low-AG firms (Spreads)",
    "text": "Differences between high- and low-AG firms (Spreads)\n\n# Calculate spreads (diffences) in meadians between AG deciles 1 and 10\nd1 = medians.loc[1,:]\nd1 \n\n\n\n\n\n\n\n\nAG\nL2AG\nat\nMV\nLeverage\nROA\nBHRET6\nBHRET36\n\n\nyear\n\n\n\n\n\n\n\n\n\n\n\n\n1981\n-0.156534\nNaN\n21.1345\n7.203563\n0.229742\n-0.015460\n-7.453705e-02\nNaN\n\n\n1982\n-0.238448\n0.027319\n9.2700\n12.730500\n0.183496\n-0.139899\n5.442360e-01\n0.095549\n\n\n1983\n-0.229104\n-0.039075\n14.6480\n8.373375\n0.201748\n-0.142807\n-1.514806e-01\n-0.583333\n\n\n1984\n-0.234161\n-0.022007\n12.7145\n8.697719\n0.190783\n-0.196343\n5.555555e-02\n-0.407168\n\n\n1985\n-0.313274\n-0.010333\n10.1480\n9.214063\n0.180496\n-0.300224\n7.352940e-02\n-0.722479\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1998\n-0.318542\n-0.022003\n22.4040\n41.305687\n0.061048\n-0.422993\n8.552632e-02\n-0.562500\n\n\n1999\n-0.292172\n-0.033717\n25.9085\n46.752984\n0.073241\n-0.329393\n-6.060653e-09\n-0.400000\n\n\n2000\n-0.299783\n-0.010038\n45.3865\n28.847051\n0.059689\n-0.364766\n2.000000e-01\n-0.694769\n\n\n2001\n-0.473008\n0.111816\n42.4650\n23.101199\n0.023454\n-0.904887\n-3.348821e-01\n-0.823768\n\n\n2002\n-0.421147\n-0.178360\n47.3360\n50.193000\n0.041295\n-0.583128\n4.857143e-01\n-0.824803\n\n\n\n\n22 rows × 8 columns\n\n\n\n\nd10 = medians.loc[10,:]\nd10\n\n\n\n\n\n\n\n\nAG\nL2AG\nat\nMV\nLeverage\nROA\nBHRET6\nBHRET36\n\n\nyear\n\n\n\n\n\n\n\n\n\n\n\n\n1981\n0.851511\nNaN\n27.2525\n27.287156\n0.519440\n0.055673\n-0.234994\nNaN\n\n\n1982\n0.587385\n0.313213\n34.2610\n58.212000\n0.531385\n0.044799\n0.406984\n1.948354\n\n\n1983\n0.960828\n0.167609\n30.0000\n38.586000\n0.369032\n0.049919\n-0.201324\n0.445114\n\n\n1984\n0.815741\n0.204685\n29.5945\n32.943000\n0.527136\n0.043982\n0.083349\n0.823529\n\n\n1985\n0.786492\n0.221875\n41.1470\n63.686250\n0.604671\n0.041829\n0.131579\n0.243881\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1998\n1.229145\n0.248939\n167.6615\n169.757875\n0.785783\n0.021279\n-0.029114\n0.116597\n\n\n1999\n1.357963\n0.212174\n209.6170\n311.982062\n0.493111\n0.011233\n-0.109476\n0.591549\n\n\n2000\n1.922917\n0.187752\n229.2415\n344.086705\n0.107849\n-0.058293\n-0.130450\n0.488128\n\n\n2001\n0.765903\n0.211994\n234.1040\n280.906659\n0.271175\n0.020858\n-0.251107\n0.222919\n\n\n2002\n0.571702\n0.124097\n249.1590\n375.585791\n0.275448\n0.038501\n0.190315\n0.366243\n\n\n\n\n22 rows × 8 columns\n\n\n\n\nd_spread = d10 - d1 \nd_spread\n\n\n\n\n\n\n\n\nAG\nL2AG\nat\nMV\nLeverage\nROA\nBHRET6\nBHRET36\n\n\nyear\n\n\n\n\n\n\n\n\n\n\n\n\n1981\n1.008045\nNaN\n6.1180\n20.083594\n0.289698\n0.071133\n-0.160457\nNaN\n\n\n1982\n0.825834\n0.285894\n24.9910\n45.481500\n0.347889\n0.184698\n-0.137252\n1.852805\n\n\n1983\n1.189933\n0.206684\n15.3520\n30.212625\n0.167284\n0.192726\n-0.049843\n1.028447\n\n\n1984\n1.049902\n0.226692\n16.8800\n24.245281\n0.336353\n0.240325\n0.027793\n1.230697\n\n\n1985\n1.099766\n0.232208\n30.9990\n54.472188\n0.424175\n0.342053\n0.058050\n0.966360\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1998\n1.547688\n0.270942\n145.2575\n128.452188\n0.724735\n0.444273\n-0.114640\n0.679097\n\n\n1999\n1.650134\n0.245891\n183.7085\n265.229078\n0.419870\n0.340626\n-0.109476\n0.991549\n\n\n2000\n2.222700\n0.197790\n183.8550\n315.239654\n0.048161\n0.306473\n-0.330450\n1.182897\n\n\n2001\n1.238911\n0.100178\n191.6390\n257.805460\n0.247722\n0.925745\n0.083775\n1.046687\n\n\n2002\n0.992850\n0.302457\n201.8230\n325.392791\n0.234152\n0.621630\n-0.295399\n1.191046\n\n\n\n\n22 rows × 8 columns\n\n\n\n\n# Calculate means for the spreads\nSpread = d_spread.mean()\nSpread\n\nAG            1.292361\nL2AG          0.224090\nat           70.501523\nMV          105.858883\nLeverage      0.347954\nROA           0.334991\nBHRET6       -0.074527\nBHRET36       1.111352\ndtype: float64\n\n\n\n# Convert spreads to a dataframe\nSpread_df = pd.DataFrame(Spread, columns = ['Spread'], index = Table1.columns).transpose()\nSpread_df\n\n\n\n\n\n\n\n\nAG\nL2AG\nat\nMV\nLeverage\nROA\nBHRET6\nBHRET36\n\n\n\n\nSpread\n1.292361\n0.22409\n70.501523\n105.858883\n0.347954\n0.334991\n-0.074527\n1.111352\n\n\n\n\n\n\n\n\n# Calculate tstatistics for the spreads\ntstat = sm.stats.ttest_ind(d10.dropna(), d1.dropna())[0]\ntstat\n\narray([18.24803015, 14.81442456,  4.29417608,  4.53683213,  8.52451531,\n        9.47088405, -1.23559743, 10.80911867])\n\n\n\n# Convert it to a dataframe\ntstat_df = pd.DataFrame(tstat, columns = ['tstat'], index = Table1.columns).transpose()\ntstat_df\n\n\n\n\n\n\n\n\nAG\nL2AG\nat\nMV\nLeverage\nROA\nBHRET6\nBHRET36\n\n\n\n\ntstat\n18.24803\n14.814425\n4.294176\n4.536832\n8.524515\n9.470884\n-1.235597\n10.809119\n\n\n\n\n\n\n\n\n# Append spreads and tstats to the table\nTable1_full = pd.concat([Table1, Spread_df, tstat_df], axis = 0)\nTable1_full\n\n\n\n\n\n\n\n\nAG\nL2AG\nat\nMV\nLeverage\nROA\nBHRET6\nBHRET36\n\n\n\n\n1\n-0.292132\n-0.027666\n19.849545\n20.086016\n0.128903\n-0.307385\n0.080930\n-0.512516\n\n\n2\n-0.113047\n0.002830\n47.862864\n31.512131\n0.198275\n-0.057847\n0.094522\n-0.265142\n\n\n3\n-0.038602\n0.022827\n101.886864\n61.553354\n0.234033\n0.008679\n0.096292\n-0.031078\n\n\n4\n0.008625\n0.040892\n155.610909\n106.641705\n0.241524\n0.030996\n0.089255\n0.159314\n\n\n5\n0.047712\n0.060157\n186.173614\n136.166537\n0.248173\n0.041317\n0.088291\n0.279760\n\n\n6\n0.089594\n0.083457\n171.516727\n140.993278\n0.234498\n0.050478\n0.080265\n0.341050\n\n\n7\n0.142033\n0.110151\n155.325705\n153.609485\n0.224782\n0.055813\n0.079332\n0.378675\n\n\n8\n0.222567\n0.146551\n127.493750\n148.647825\n0.241273\n0.058865\n0.066603\n0.462167\n\n\n9\n0.385785\n0.184534\n100.010364\n140.504525\n0.298602\n0.054623\n0.047157\n0.546036\n\n\n10\n1.000230\n0.196424\n90.351068\n125.944899\n0.476857\n0.027607\n0.006403\n0.598836\n\n\nSpread\n1.292361\n0.224090\n70.501523\n105.858883\n0.347954\n0.334991\n-0.074527\n1.111352\n\n\ntstat\n18.248030\n14.814425\n4.294176\n4.536832\n8.524515\n9.470884\n-1.235597\n10.809119"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture13_wins_groupby_apply_transform.html",
    "href": "teaching/FIN 525/lectures/lecture13_wins_groupby_apply_transform.html",
    "title": "L13: Conditional stats, outliers",
    "section": "",
    "text": "Lecture overview\nIn this lecture we introduce a set of Pandas functions that are very useful in describing subsamples of your data (this is often called “subsample analysis”). Looking at subsamples of your data individually is important because patterns that show up in your overall dataset may look quite different if you limit yourself to a subset of the dataset. This is exemplified in Simpson’s Paradox: https://en.wikipedia.org/wiki/Simpson%27s_paradox.\nWe finish the lecture with a discussion of the impact of outliers on your descriptive statistics, and a method of mitigating that impact called “windsorization”.\n\n\nPreliminaries\n\nimport pandas as pd\nimport numpy as np\nimport pandas_datareader as pdr\npd.options.display.max_rows = 20\n\nWe’ll use data on the Fama-French 5-industry portfolio returns for this lecture:\n\nraw = pdr.DataReader(name = '5_Industry_Portfolios', data_source = 'famafrench', \n                     start = '2011-01-01', end = '2020-12-31')\nraw\n\n{0:          Cnsmr  Manuf  HiTec  Hlth   Other\n Date                                      \n 2011-01  -1.34   4.20   3.00  -0.71   1.92\n 2011-02   2.89   4.87   3.45   3.33   2.63\n 2011-03   1.89   1.81  -0.83   2.29  -0.86\n 2011-04   4.36   2.58   3.16   6.37   1.11\n 2011-05   0.92  -2.55  -1.13   1.96  -2.40\n ...        ...    ...    ...    ...    ...\n 2020-08  10.07   3.15  10.00   2.45   7.20\n 2020-09  -4.01  -2.24  -4.82  -1.47  -2.98\n 2020-10  -2.64  -0.03  -2.02  -4.42  -1.88\n 2020-11  11.44  12.91  11.16   9.51  15.76\n 2020-12   4.09   2.65   4.99   4.77   5.27\n \n [120 rows x 5 columns],\n 1:          Cnsmr  Manuf  HiTec  Hlth   Other\n Date                                      \n 2011-01  -0.54   3.64   3.18   1.26   1.88\n 2011-02   4.14   5.50   5.49   3.46   2.92\n 2011-03   1.15   3.33   0.88   2.77  -0.48\n 2011-04   2.32   0.47   1.45   5.02   0.23\n 2011-05  -1.00  -3.48  -1.78  -0.18  -2.72\n ...        ...    ...    ...    ...    ...\n 2020-08   9.79   3.39   4.02   1.41   5.59\n 2020-09  -1.96  -4.15  -2.97  -1.14  -3.47\n 2020-10   2.54   0.97   0.23  -2.71   4.03\n 2020-11  22.36  23.25  22.36  18.58  18.45\n 2020-12   6.13   9.22  11.49   9.74   9.02\n \n [120 rows x 5 columns],\n 2:       Cnsmr  Manuf  HiTec  Hlth   Other\n Date                                   \n 2011   9.05   4.47   0.49  10.87  -9.89\n 2012  16.30   8.81  16.73  20.35  22.27\n 2013  32.99  29.18  33.65  41.19  40.86\n 2014  12.63   3.38  14.48  24.46  12.34\n 2015   6.79 -11.64   3.83   5.80  -1.15\n 2016   5.48  20.34  13.98  -1.85  20.17\n 2017  18.68  18.17  29.50  22.20  21.52\n 2018  -3.58 -11.10  -0.59   4.82  -9.23\n 2019  25.88  21.21  41.85  20.20  29.50\n 2020  37.64   0.26  42.65  18.31   6.88,\n 3:       Cnsmr  Manuf  HiTec  Hlth   Other\n Date                                   \n 2011  -6.73  -6.02 -13.44 -10.95 -10.41\n 2012  21.99   8.97  10.64  25.44  25.82\n 2013  43.72  37.33  50.77  56.15  42.26\n 2014   4.87  -8.46   2.41  14.20   7.92\n 2015  -9.62 -22.43  -3.48   2.35  -3.05\n 2016  16.05  33.62  17.30 -11.21  30.99\n 2017   6.42  10.37  25.78  25.49  14.59\n 2018 -14.13 -22.04  -6.41 -22.48 -13.87\n 2019  17.49  12.04  27.63  25.96  26.27\n 2020  42.47  18.60  65.97  63.85  12.45,\n 4:          Cnsmr  Manuf  HiTec  Hlth   Other\n Date                                      \n 2011-01    613    720    790    457   1182\n 2011-02    611    715    783    453   1180\n 2011-03    604    711    778    450   1174\n 2011-04    600    709    767    445   1162\n 2011-05    596    706    761    443   1154\n ...        ...    ...    ...    ...    ...\n 2020-08    481    557    633    719   1022\n 2020-09    479    554    630    718   1020\n 2020-10    479    550    628    712   1013\n 2020-11    479    548    627    710   1011\n 2020-12    475    548    624    706   1010\n \n [120 rows x 5 columns],\n 5:             Cnsmr    Manuf     HiTec    Hlth     Other\n Date                                                  \n 2011-01   4021.78  4907.75   4617.97  2797.17  2823.79\n 2011-02   3978.53  5133.19   4789.86  2794.87  2880.75\n 2011-03   4122.09  5396.87   4968.89  2890.04  2966.86\n 2011-04   4213.62  5503.13   4974.81  2936.50  2966.20\n 2011-05   4422.42  5657.10   5157.11  3134.23  3012.30\n ...           ...      ...       ...      ...      ...\n 2020-08  12344.23  7837.49  19959.78  4733.39  6902.46\n 2020-09  13628.75  8094.48  22037.83  4845.20  7402.98\n 2020-10  13059.65  7942.45  21034.62  4752.32  7187.19\n 2020-11  12707.13  7957.86  20625.15  4534.52  7057.75\n 2020-12  14223.39  8951.70  22998.28  4986.10  8166.06\n \n [120 rows x 5 columns],\n 6:       Cnsmr  Manuf  HiTec  Hlth   Other\n Date                                   \n 2011   0.31   0.41   0.34   0.35   0.67\n 2012   0.30   0.47   0.34   0.33   0.74\n 2013   0.27   0.44   0.32   0.27   0.62\n 2014   0.26   0.38   0.28   0.23   0.55\n 2015   0.24   0.44   0.26   0.20   0.54\n 2016   0.23   0.41   0.28   0.22   0.60\n 2017   0.24   0.38   0.24   0.20   0.48\n 2018   0.22   0.38   0.21   0.19   0.46\n 2019   0.22   0.41   0.20   0.19   0.46\n 2020   0.22   0.48   0.18   0.19   0.57,\n 7:       Cnsmr  Manuf  HiTec  Hlth   Other\n Date                                   \n 2011   0.32   0.44   0.35   0.39   0.65\n 2012   0.32   0.47   0.38   0.36   0.84\n 2013   0.31   0.48   0.36   0.32   0.74\n 2014   0.27   0.42   0.29   0.25   0.57\n 2015   0.24   0.40   0.26   0.21   0.55\n 2016   0.24   0.47   0.29   0.22   0.56\n 2017   0.24   0.37   0.26   0.23   0.51\n 2018   0.21   0.39   0.22   0.20   0.44\n 2019   0.24   0.45   0.25   0.21   0.53\n 2020   0.20   0.52   0.18   0.19   0.42,\n 'DESCR': '5 Industry Portfolios\\n---------------------\\n\\nThis file was created by CMPT_IND_RETS using the 202201 CRSP database. It contains value- and equal-weighted returns for 5 industry portfolios. The portfolios are constructed at the end of June. The annual returns are from January to December. Missing data are indicated by -99.99 or -999. Copyright 2022 Kenneth R. French\\n\\n  0 : Average Value Weighted Returns -- Monthly (120 rows x 5 cols)\\n  1 : Average Equal Weighted Returns -- Monthly (120 rows x 5 cols)\\n  2 : Average Value Weighted Returns -- Annual (10 rows x 5 cols)\\n  3 : Average Equal Weighted Returns -- Annual (10 rows x 5 cols)\\n  4 : Number of Firms in Portfolios (120 rows x 5 cols)\\n  5 : Average Firm Size (120 rows x 5 cols)\\n  6 : Sum of BE / Sum of ME (10 rows x 5 cols)\\n  7 : Value-Weighted Average of BE/ME (10 rows x 5 cols)'}\n\n\nExtract equal-weighted annual industry returns, and turn them to decimal (they are in percentage points):\n\new = raw[3]/100\new\n\n\n\n\n\n\n\n\nCnsmr\nManuf\nHiTec\nHlth\nOther\n\n\nDate\n\n\n\n\n\n\n\n\n\n2011\n-0.0673\n-0.0602\n-0.1344\n-0.1095\n-0.1041\n\n\n2012\n0.2199\n0.0897\n0.1064\n0.2544\n0.2582\n\n\n2013\n0.4372\n0.3733\n0.5077\n0.5615\n0.4226\n\n\n2014\n0.0487\n-0.0846\n0.0241\n0.1420\n0.0792\n\n\n2015\n-0.0962\n-0.2243\n-0.0348\n0.0235\n-0.0305\n\n\n2016\n0.1605\n0.3362\n0.1730\n-0.1121\n0.3099\n\n\n2017\n0.0642\n0.1037\n0.2578\n0.2549\n0.1459\n\n\n2018\n-0.1413\n-0.2204\n-0.0641\n-0.2248\n-0.1387\n\n\n2019\n0.1749\n0.1204\n0.2763\n0.2596\n0.2627\n\n\n2020\n0.4247\n0.1860\n0.6597\n0.6385\n0.1245\n\n\n\n\n\n\n\nLet’s take a look at the data:\n\new.plot();\n\n\n\n\nCalculate cumulative products of gross returns (i.e. compound returns over time) and plot them:\n\n(1+ew).cumprod().plot();\n\n\n\n\nStack industry returns on top of each other for the purpose of this class:\n\new_long = ew.stack().to_frame(name = 'ewret')\new_long.head(10)\n\n\n\n\n\n\n\n\n\newret\n\n\nDate\n\n\n\n\n\n\n2011\nCnsmr\n-0.0673\n\n\nManuf\n-0.0602\n\n\nHiTec\n-0.1344\n\n\nHlth\n-0.1095\n\n\nOther\n-0.1041\n\n\n2012\nCnsmr\n0.2199\n\n\nManuf\n0.0897\n\n\nHiTec\n0.1064\n\n\nHlth\n0.2544\n\n\nOther\n0.2582\n\n\n\n\n\n\n\nAnd bring date and industry names as data inside the dataframe:\n\new_long = ew_long.reset_index().rename(columns = {'level_1':'Industry'})\new_long.head(2)\n\n\n\n\n\n\n\n\nDate\nIndustry\newret\n\n\n\n\n0\n2011\nCnsmr\n-0.0673\n\n\n1\n2011\nManuf\n-0.0602\n\n\n\n\n\n\n\nChallenge:\nDo the same for value-weighted annual returns (i.e. create a “vw_long” dataframe, using the same steps we used for “ew_long”:\n\nvw = raw[2]/100\nvw\n\n\n\n\n\n\n\n\nCnsmr\nManuf\nHiTec\nHlth\nOther\n\n\nDate\n\n\n\n\n\n\n\n\n\n2011\n0.0905\n0.0447\n0.0049\n0.1087\n-0.0989\n\n\n2012\n0.1630\n0.0881\n0.1673\n0.2035\n0.2227\n\n\n2013\n0.3299\n0.2918\n0.3365\n0.4119\n0.4086\n\n\n2014\n0.1263\n0.0338\n0.1448\n0.2446\n0.1234\n\n\n2015\n0.0679\n-0.1164\n0.0383\n0.0580\n-0.0115\n\n\n2016\n0.0548\n0.2034\n0.1398\n-0.0185\n0.2017\n\n\n2017\n0.1868\n0.1817\n0.2950\n0.2220\n0.2152\n\n\n2018\n-0.0358\n-0.1110\n-0.0059\n0.0482\n-0.0923\n\n\n2019\n0.2588\n0.2121\n0.4185\n0.2020\n0.2950\n\n\n2020\n0.3764\n0.0026\n0.4265\n0.1831\n0.0688\n\n\n\n\n\n\n\n\nvw_long = vw.stack().to_frame(name = 'vwret')\\\n            .reset_index().rename(columns={'level_1':'Industry'})\n\nvw_long.head()\n\n\n\n\n\n\n\n\nDate\nIndustry\nvwret\n\n\n\n\n0\n2011\nCnsmr\n0.0905\n\n\n1\n2011\nManuf\n0.0447\n\n\n2\n2011\nHiTec\n0.0049\n\n\n3\n2011\nHlth\n0.1087\n\n\n4\n2011\nOther\n-0.0989\n\n\n\n\n\n\n\nMerge the EW returns and VW returns into a single dataframe called “ireturns”:\n\nireturns = ew_long.merge(vw_long, how='inner', on = ['Date','Industry'])\nireturns.head(10)\n\n\n\n\n\n\n\n\nDate\nIndustry\newret\nvwret\n\n\n\n\n0\n2011\nCnsmr\n-0.0673\n0.0905\n\n\n1\n2011\nManuf\n-0.0602\n0.0447\n\n\n2\n2011\nHiTec\n-0.1344\n0.0049\n\n\n3\n2011\nHlth\n-0.1095\n0.1087\n\n\n4\n2011\nOther\n-0.1041\n-0.0989\n\n\n5\n2012\nCnsmr\n0.2199\n0.1630\n\n\n6\n2012\nManuf\n0.0897\n0.0881\n\n\n7\n2012\nHiTec\n0.1064\n0.1673\n\n\n8\n2012\nHlth\n0.2544\n0.2035\n\n\n9\n2012\nOther\n0.2582\n0.2227\n\n\n\n\n\n\n\n\n\nGrouping your data: the .groupby() function\nThe .groupby() function can be used to tell Python that you want to split your data into groups. The parameters of the .groupby() function tell Python how those groups should be created. The purpose is usually to apply some function (e.g. the “.mean()” function) to each of these groups separately.\nAbbreviated syntax:\nDataFrame.groupby(by=None, axis=0, level=None, as_index=True, sort=True, dropna=True)\nThe most important parameter is by. This is where you tell Python which column (or index) in your DataFrame contains the information based on which you want to group your data. Python will split your DataFrame into “mini” dataframes, one for each unique value of the variable(s) you supplied to the by parameter.\nFor example, the line below splits ireturns into 5 different dataframes, one for each unique entry found in the “Industry” column, and then applies the .mean() function for each of these 5 dataframes separately. Finally, these subsample means are all collected into a new dataframe ind_means:\n\nind_means = ireturns.groupby(by = 'Industry').mean()\nind_means\n\n\n\n\n\n\n\n\newret\nvwret\n\n\nIndustry\n\n\n\n\n\n\nCnsmr\n0.12253\n0.16186\n\n\nHiTec\n0.17717\n0.19657\n\n\nHlth\n0.16880\n0.16635\n\n\nManuf\n0.06198\n0.08308\n\n\nOther\n0.13297\n0.13327\n\n\n\n\n\n\n\nIf you don’t want the by variable (i.e. “Industry” in the example above) to be the index of the resulting dataframe:\n\nind_means = ireturns.groupby(by = 'Industry', as_index = False).mean()\nind_means\n\n\n\n\n\n\n\n\nIndustry\newret\nvwret\n\n\n\n\n0\nCnsmr\n0.12253\n0.16186\n\n\n1\nHiTec\n0.17717\n0.19657\n\n\n2\nHlth\n0.16880\n0.16635\n\n\n3\nManuf\n0.06198\n0.08308\n\n\n4\nOther\n0.13297\n0.13327\n\n\n\n\n\n\n\nAnother example, with a different by variable and a different function applied to each group (i.e. median instead of mean):\n\nan_means = ireturns.groupby(by = 'Date').median()\nan_means\n\n\n\n\n\n\n\n\newret\nvwret\n\n\nDate\n\n\n\n\n\n\n2011\n-0.1041\n0.0447\n\n\n2012\n0.2199\n0.1673\n\n\n2013\n0.4372\n0.3365\n\n\n2014\n0.0487\n0.1263\n\n\n2015\n-0.0348\n0.0383\n\n\n2016\n0.1730\n0.1398\n\n\n2017\n0.1459\n0.2152\n\n\n2018\n-0.1413\n-0.0358\n\n\n2019\n0.2596\n0.2588\n\n\n2020\n0.4247\n0.1831\n\n\n\n\n\n\n\nYou can group by more than one variable:\n\ntwodim = ireturns.groupby(by = ['Date','Industry']).mean()\ntwodim.head(10)\n\n\n\n\n\n\n\n\n\newret\nvwret\n\n\nDate\nIndustry\n\n\n\n\n\n\n2011\nCnsmr\n-0.0673\n0.0905\n\n\nHiTec\n-0.1344\n0.0049\n\n\nHlth\n-0.1095\n0.1087\n\n\nManuf\n-0.0602\n0.0447\n\n\nOther\n-0.1041\n-0.0989\n\n\n2012\nCnsmr\n0.2199\n0.1630\n\n\nHiTec\n0.1064\n0.1673\n\n\nHlth\n0.2544\n0.2035\n\n\nManuf\n0.0897\n0.0881\n\n\nOther\n0.2582\n0.2227\n\n\n\n\n\n\n\nThe example above did not really change the ireturns dataframe, since each “Date” x “Industry” pair has a single entry for both “ewret” and “vwret”. Since the mean of a single number is the number itself, the twodim dataframe will be identical to ireturns. Note that this is not necessarily the case if we used a different function instead of .mean(), for example .count():\n\ntwodim = ireturns.groupby(by = ['Date','Industry']).count()\ntwodim.head(10)\n\n\n\n\n\n\n\n\n\newret\nvwret\n\n\nDate\nIndustry\n\n\n\n\n\n\n2011\nCnsmr\n1\n1\n\n\nHiTec\n1\n1\n\n\nHlth\n1\n1\n\n\nManuf\n1\n1\n\n\nOther\n1\n1\n\n\n2012\nCnsmr\n1\n1\n\n\nHiTec\n1\n1\n\n\nHlth\n1\n1\n\n\nManuf\n1\n1\n\n\nOther\n1\n1\n\n\n\n\n\n\n\nYou can specify which variable(s) you want to apply the function to, in brackets, right before the function name (if you leave this out (like above), the function will be applied to all the columns in the dataframe):\n\nind_ew_medians = ireturns.groupby('Industry')['ewret'].median()\nind_ew_medians\n\nIndustry\nCnsmr    0.11235\nHiTec    0.13970\nHlth     0.19820\nManuf    0.09670\nOther    0.13520\nName: ewret, dtype: float64\n\n\n\n\nThe .apply() and .transform() methods\nThe .apply() and .transform() methods do similar things: they can be used to tell Python to apply a given function to some data from a dataframe. As the examples above show, there are many Pandas functions, like .mean() and .median() that can do this without the help of .apply() or .transform() (we just have to add the names of these functions after the .groupby() statement, just like we did above). But what if the function we want to apply is not a built-in Pandas function that can be applied with a dot after the name of a dataframe? This is where .apply() and .transform() come in handy. These methods are especially useful when we want to apply a particular function, separately, to each group we created with a .groupby statement.\nHere is their syntax:\nSyntax for .transform():\nDataFrame.transform(func, axis=0, *args, **kwargs)\nSyntax for .apply():\nDataFrame.apply(func, axis=0, raw=False, result_type=None, args=(), **kwargs)\nThe most important argument is func which is where we tell Python which function we want to apply to the data.\nThe main difference between .transform() and .apply() is that .transform() returns a sequence of the same length as the dataframe to which it is applied, while .apply() returns a DataFrame or Series of the same size as the number of groups to which it is applied.\n\nireturns.groupby('Industry')['ewret'].apply(func = np.median)\n\nIndustry\nCnsmr    0.11235\nHiTec    0.13970\nHlth     0.19820\nManuf    0.09670\nOther    0.13520\nName: ewret, dtype: float64\n\n\n\nireturns.groupby('Industry')['ewret'].transform(func = np.median)\n\n0     0.11235\n1     0.09670\n2     0.13970\n3     0.19820\n4     0.13520\n       ...   \n45    0.11235\n46    0.09670\n47    0.13970\n48    0.19820\n49    0.13520\nName: ewret, Length: 50, dtype: float64\n\n\nWe usually add the results of .transform() as a new column to the same dataframe:\n\nmycopy = ireturns.copy()\nmycopy['ind_medians'] = mycopy.groupby('Industry')['ewret'].transform(np.median)\nmycopy.head(10)\n\n\n\n\n\n\n\n\nDate\nIndustry\newret\nvwret\nind_medians\n\n\n\n\n0\n2011\nCnsmr\n-0.0673\n0.0905\n0.11235\n\n\n1\n2011\nManuf\n-0.0602\n0.0447\n0.09670\n\n\n2\n2011\nHiTec\n-0.1344\n0.0049\n0.13970\n\n\n3\n2011\nHlth\n-0.1095\n0.1087\n0.19820\n\n\n4\n2011\nOther\n-0.1041\n-0.0989\n0.13520\n\n\n5\n2012\nCnsmr\n0.2199\n0.1630\n0.11235\n\n\n6\n2012\nManuf\n0.0897\n0.0881\n0.09670\n\n\n7\n2012\nHiTec\n0.1064\n0.1673\n0.13970\n\n\n8\n2012\nHlth\n0.2544\n0.2035\n0.19820\n\n\n9\n2012\nOther\n0.2582\n0.2227\n0.13520\n\n\n\n\n\n\n\nNote, also, that with .transform(), you can pass the name of the function you want as a string to the func argument, whereas with .apply() you can not:\n\nireturns.groupby('Industry')[['ewret','vwret']].transform('median') \n\n\n\n\n\n\n\n\newret\nvwret\n\n\n\n\n0\n0.11235\n0.14465\n\n\n1\n0.09670\n0.06640\n\n\n2\n0.13970\n0.15605\n\n\n3\n0.19820\n0.19255\n\n\n4\n0.13520\n0.16255\n\n\n...\n...\n...\n\n\n45\n0.11235\n0.14465\n\n\n46\n0.09670\n0.06640\n\n\n47\n0.13970\n0.15605\n\n\n48\n0.19820\n0.19255\n\n\n49\n0.13520\n0.16255\n\n\n\n\n50 rows × 2 columns\n\n\n\nWhereas the line below will not work. You have to specify which package the “median” function belongs to (which is why we used .apply(np.median) above):\n\n#ireturns.groupby('Industry')[['ewret','vwret']].apply('median') #this gives an error\n\nWe are not restricted to applying functions that come with a package that we have installed. We can also use a function that we created ourselves.\nFor example, below, we create a function that can take in a Series or a DataFrame of returns, and compounds them:\n\ndef compound_returns(x):\n    return (1+x).prod()-1\n\nNow we can apply that function to the returns of each industry:\n\nireturns.groupby('Industry')[['ewret','vwret']].apply(compound_returns)\n\n\n\n\n\n\n\n\newret\nvwret\n\n\nIndustry\n\n\n\n\n\n\nCnsmr\n1.751321\n3.242533\n\n\nHiTec\n3.174702\n4.532945\n\n\nHlth\n2.634393\n3.435179\n\n\nManuf\n0.525684\n1.060787\n\n\nOther\n2.079929\n2.161870\n\n\n\n\n\n\n\nLet’s see if it worked:\n\n(1+ew).cumprod()-1 #look at the bottom row and compare it with our results above\n\n\n\n\n\n\n\n\nCnsmr\nManuf\nHiTec\nHlth\nOther\n\n\nDate\n\n\n\n\n\n\n\n\n\n2011\n-0.067300\n-0.060200\n-0.134400\n-0.109500\n-0.104100\n\n\n2012\n0.137801\n0.024100\n-0.042300\n0.117043\n0.127221\n\n\n2013\n0.635247\n0.406397\n0.443924\n0.744263\n0.603585\n\n\n2014\n0.714884\n0.287415\n0.478723\n0.991948\n0.730589\n\n\n2015\n0.549912\n-0.001352\n0.427263\n1.038759\n0.677806\n\n\n2016\n0.798673\n0.334394\n0.674180\n0.810214\n1.197758\n\n\n2017\n0.914148\n0.472770\n1.105783\n1.271638\n1.518411\n\n\n2018\n0.643679\n0.148172\n0.970802\n0.760974\n1.169108\n\n\n2019\n0.931158\n0.286412\n1.515335\n1.218122\n1.738932\n\n\n2020\n1.751321\n0.525684\n3.174702\n2.634393\n2.079929\n\n\n\n\n\n\n\n\n\nWinsorizing outliers\n“Winsorizing” a variable means replacing its most extreme values with less extreme values. For example, winsorizing a variable “at the 5 and 95 percentiles”, means that the values of that variable that are smaller than the 5th percentile will be made equal to the 5th percentile and the values that are larger than the 95th percentile will be made equal to the 95th percentile.\nYou can pick other values for the percentiles at which you want to winsorize but (1,99) and (5, 95) are by far the most common ones.\nTo winsorize a variable, in a Pandas dataframe, we use the .clip() function as below. This also requires us to use the .quantile() function to calculate the 5th and 95th percentiles. First, let’s sort the returns so we can easily see its most extreme values (top and bottom):\n\new_long.sort_values('ewret')\n\n\n\n\n\n\n\n\nDate\nIndustry\newret\n\n\n\n\n38\n2018\nHlth\n-0.2248\n\n\n21\n2015\nManuf\n-0.2243\n\n\n36\n2018\nManuf\n-0.2204\n\n\n35\n2018\nCnsmr\n-0.1413\n\n\n39\n2018\nOther\n-0.1387\n\n\n...\n...\n...\n...\n\n\n10\n2013\nCnsmr\n0.4372\n\n\n12\n2013\nHiTec\n0.5077\n\n\n13\n2013\nHlth\n0.5615\n\n\n48\n2020\nHlth\n0.6385\n\n\n47\n2020\nHiTec\n0.6597\n\n\n\n\n50 rows × 3 columns\n\n\n\nLet’s calculate the 5th and 95th percentiles:\n\nq5 = ew_long['ewret'].quantile(0.05)\nq5\n\n-0.184805\n\n\n\nq95 = ew_long['ewret'].quantile(0.95)\nq95\n\n0.5372899999999998\n\n\nAnd now let’s create a version of ewret that is winsorized at the 5 and 95 percentiles:\n\new_long['ew_wins'] = ew_long['ewret'].clip(lower = q5, upper = q95)\n\nLet’s see if it worked:\n\new_long.sort_values('ewret')\n\n\n\n\n\n\n\n\nDate\nIndustry\newret\new_wins\n\n\n\n\n38\n2018\nHlth\n-0.2248\n-0.184805\n\n\n21\n2015\nManuf\n-0.2243\n-0.184805\n\n\n36\n2018\nManuf\n-0.2204\n-0.184805\n\n\n35\n2018\nCnsmr\n-0.1413\n-0.141300\n\n\n39\n2018\nOther\n-0.1387\n-0.138700\n\n\n...\n...\n...\n...\n...\n\n\n10\n2013\nCnsmr\n0.4372\n0.437200\n\n\n12\n2013\nHiTec\n0.5077\n0.507700\n\n\n13\n2013\nHlth\n0.5615\n0.537290\n\n\n48\n2020\nHlth\n0.6385\n0.537290\n\n\n47\n2020\nHiTec\n0.6597\n0.537290\n\n\n\n\n50 rows × 4 columns"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture16_regression_intro2.html",
    "href": "teaching/FIN 525/lectures/lecture16_regression_intro2.html",
    "title": "L16: Linear regression applications",
    "section": "",
    "text": "# Import packages\nimport pandas as pd\nimport numpy as np\nimport yfinance as yf\nimport pandas_datareader as pdr\nimport statsmodels.api as sm\n\n\n# Load Fama-French factor data\nff3f = pdr.DataReader('F-F_Research_Data_Factors', 'famafrench', '2012-01-01')[0]/100\nff3f.head(2)\n\n\n\n\n\n\n\n\nMkt-RF\nSMB\nHML\nRF\n\n\nDate\n\n\n\n\n\n\n\n\n2012-01\n0.0505\n0.0206\n-0.0094\n0.0\n\n\n2012-02\n0.0442\n-0.0186\n0.0043\n0.0\n\n\n\n\n\n\n\nLoad data on TSLA and clean it:\n\n    # Download monthly prices (keep only Adjusted Close prices)\nfirm_prices = yf.download('TSLA', '2012-12-01', '2020-12-31', interval = '1mo')['Adj Close'].dropna().to_frame()\n\n    # Calculate monthly returns, drop missing, convert from Series to DataFrame\nfirm_ret = firm_prices.pct_change().dropna()\n\n    # Rename \"Adj Close\" to \"TSLA\"\nfirm_ret.rename(columns = {'Adj Close': 'TSLA'}, inplace = True)\n\n    # Convert index to monthly period date\nfirm_ret.index = firm_ret.index.to_period('M')\nfirm_ret.head(2)\n\n[*********************100%***********************]  1 of 1 completed\n\n\n\n\n\n\n\n\n\nTSLA\n\n\nDate\n\n\n\n\n\n2013-01\n0.107470\n\n\n2013-02\n-0.071448\n\n\n\n\n\n\n\n\n# Merge the two datasets\ndata = firm_ret.join(ff3f)\ndata['const'] = 1\ndata.head(2)\n\n\n\n\n\n\n\n\nTSLA\nMkt-RF\nSMB\nHML\nRF\nconst\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n2013-01\n0.107470\n0.0557\n0.0031\n0.0095\n0.0\n1\n\n\n2013-02\n-0.071448\n0.0129\n-0.0033\n0.0010\n0.0\n1\n\n\n\n\n\n\n\n\n# Set up the data\n    # Dependent variable (left side of the equal sign)\ny = data['TSLA'] - data['RF']\ny.head(2)\n\nDate\n2013-01    0.107470\n2013-02   -0.071448\nFreq: M, dtype: float64\n\n\n\n    # Independent variable(s) (right side of the equal sign)\nX = data[['const','Mkt-RF']]\nX.head(2)\n\n\n\n\n\n\n\n\nconst\nMkt-RF\n\n\nDate\n\n\n\n\n\n\n2013-01\n1\n0.0557\n\n\n2013-02\n1\n0.0129\n\n\n\n\n\n\n\n\n# Run regression and store results in \"res\" object\nres = sm.OLS(y,X).fit()\nprint(res.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.155\nModel:                            OLS   Adj. R-squared:                  0.146\nMethod:                 Least Squares   F-statistic:                     17.26\nDate:                Mon, 28 Mar 2022   Prob (F-statistic):           7.19e-05\nTime:                        08:49:43   Log-Likelihood:                 30.143\nNo. Observations:                  96   AIC:                            -56.29\nDf Residuals:                      94   BIC:                            -51.16\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0416      0.019      2.185      0.031       0.004       0.079\nMkt-RF         1.8300      0.441      4.154      0.000       0.955       2.705\n==============================================================================\nOmnibus:                       29.750   Durbin-Watson:                   1.561\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               56.488\nSkew:                           1.227   Prob(JB):                     5.42e-13\nKurtosis:                       5.846   Cond. No.                         24.2\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nChallenge:\nEstimate the Fama-French three factor model using the data gathered above\n\n# Set up X variables\nX3 = data[['const','Mkt-RF','SMB','HML']]\n# Run regression\nres3 = sm.OLS(y,X3).fit()\nprint(res3.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.164\nModel:                            OLS   Adj. R-squared:                  0.137\nMethod:                 Least Squares   F-statistic:                     6.021\nDate:                Mon, 28 Mar 2022   Prob (F-statistic):           0.000862\nTime:                        08:49:43   Log-Likelihood:                 30.657\nNo. Observations:                  96   AIC:                            -53.31\nDf Residuals:                      92   BIC:                            -43.06\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0371      0.020      1.873      0.064      -0.002       0.076\nMkt-RF         1.8838      0.481      3.919      0.000       0.929       2.838\nSMB            0.1771      0.794      0.223      0.824      -1.400       1.755\nHML           -0.6607      0.667     -0.991      0.324      -1.985       0.664\n==============================================================================\nOmnibus:                       31.331   Durbin-Watson:                   1.586\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               62.428\nSkew:                           1.268   Prob(JB):                     2.78e-14\nKurtosis:                       6.029   Cond. No.                         45.0\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture16_regression_intro2.html#conditional-predictions",
    "href": "teaching/FIN 525/lectures/lecture16_regression_intro2.html#conditional-predictions",
    "title": "L16: Linear regression applications",
    "section": "Conditional predictions",
    "text": "Conditional predictions\nWe can use the results of our regression to estimate what we should expect the value of the dependent variable to be, if we knew the value of the independent variable(s). Mathematically, this is given by:\n\\[ E[y_t | x_t] = \\alpha + \\beta \\cdot x_t \\]\nExample:\nUsing the results from the single-factor regression above, what is the expected excess return of TSLA if the market excess return is 2%?\n\n# Extract coefficients from the results object\nalpha = res.params['const']\nbeta = res.params['Mkt-RF']\nprint(\"alpha=\",alpha,'\\n','beta=',beta)\n\nalpha= 0.041630987538345834 \n beta= 1.829989418169259\n\n\n\n# Conditional prediction\ntsla_cond_prediction = alpha + beta * 0.02\nprint(\"Expected excess return of TSLA if market excess return is 2%: \", tsla_cond_prediction)\n\nExpected excess return of TSLA if market excess return is 2%:  0.07823077590173103\n\n\nChallenge:\nUsing the results from the three-factor regression above, what is the expected excess return of TSLA if the market excess return is 2%, the SMB return -1% and the HML return is 0.5%?\n\n# Extract params\nalpha3 = res3.params['const']\nbeta_mkt = res3.params['Mkt-RF']\nbeta_smb = res3.params['SMB']\nbeta_hml = res3.params['HML']\n\n\n# Prediction\ntsla_uncond_pred3 = alpha3 + beta_mkt * 0.02 + beta_smb * (-0.01) + beta_hml * 0.005\ntsla_uncond_pred3\n\n0.06971839299927778\n\n\n\n[1, 0.02, -0.01, 0.005] @ res3.params\n\n0.06971839299927778"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture16_regression_intro2.html#unconditional-predictions",
    "href": "teaching/FIN 525/lectures/lecture16_regression_intro2.html#unconditional-predictions",
    "title": "L16: Linear regression applications",
    "section": "Unconditional predictions",
    "text": "Unconditional predictions\nWe can use the results of our regression to estimate what we should expect the value of the dependent variable to be, using our best guess for the value of the independent variable(s). Mathematically, this is given by:\n\\[ E[y_t] = \\alpha + \\beta \\cdot E[x_t] \\]\nExample:\nUsing the results from the regression above, what is the expected excess return of TSLA (i.e the risk premium on TSLA)? To answer this question, we must first estimate \\(E[R_m - R_f]\\) (i.e. the market risk premium). We do so by taking an average of the excess returns on the market over a very long time (below we use the last 90 years).\n\n# Download 100 years of data on market excess returns\nff3f_long = pdr.DataReader('F-F_Research_Data_Factors', 'famafrench', '1930-01-01','2020-12-31')[0]/100\nff3f_long.head(2)\n\n\n\n\n\n\n\n\nMkt-RF\nSMB\nHML\nRF\n\n\nDate\n\n\n\n\n\n\n\n\n1930-01\n0.0561\n0.0353\n-0.0092\n0.0014\n\n\n1930-02\n0.0250\n0.0019\n0.0020\n0.0030\n\n\n\n\n\n\n\n\n# Estimate (monthly) market risk premium\nmkt_risk_premium = ff3f_long['Mkt-RF'].mean()\nmkt_risk_premium\n\n0.0066291208791208825\n\n\n\n# Estimate TSLA risk premium\ntsla_uncond_prediction = alpha + beta * mkt_risk_premium\nprint(\"TSLA risk premium = \", tsla_uncond_prediction)\n\nTSLA risk premium =  0.05376220859890195\n\n\nChallenge:\nEstimate the risk-premium of TSLA using the three-factor model, and risk-premia estimated using the last 90 years of data.\n\n# Estimate risk-premia\npremia = ff3f_long.mean()\npremia\n\nMkt-RF    0.006629\nSMB       0.002504\nHML       0.003253\nRF        0.002693\ndtype: float64\n\n\n\n# Estimate TSLA risk premium\ntsla_premium_3f = alpha3 + beta_mkt * premia['Mkt-RF'] + beta_smb * premia['SMB'] + beta_hml * premia['HML']\ntsla_premium_3f\n\n0.047898710808046654"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture16_regression_intro2.html#variance-decomposition",
    "href": "teaching/FIN 525/lectures/lecture16_regression_intro2.html#variance-decomposition",
    "title": "L16: Linear regression applications",
    "section": "Variance decomposition",
    "text": "Variance decomposition\nThe regression results can allow us to decompose the total variance of the dependent variable into the portion that can be explained by the variance in the explanatory variables and the portion that can not be explained by these variables. Mathematically, the regression equation implies:\n\\[ Var[Y] = \\beta^2 \\cdot Var[X] + Var[\\epsilon] \\]\nExample:\nUsing the results from the regression above, calculate the total variance of TSLA, as well as its systematic variance and its idiosyncratic variance.\n\n# Total risk of tesla (variance)\ntot_risk  = y.var()\ntot_risk\n\n0.037372232872271184\n\n\n\n# Systematic risk\nsys_risk  = (beta**2) * data['Mkt-RF'].var() \nsys_risk\n\n0.005796835794247544\n\n\n\n# Idiosyncratic risk\nidio_risk = tot_risk - sys_risk\nidio_risk\n\n0.03157539707802364\n\n\n\n# Another way of calculating idiosyncratic risk (=variance of residuals (epsilon) from the regression )\nidio_risk2 = res.resid.var() # res.resid gives us the residuals from the regression (the epsilons)\nidio_risk2\n\n0.03157539707802364\n\n\n\n# Print all three of them out\nprint(f'TSLA risk: \\n total = {tot_risk: .4f} \\n systematic = {sys_risk: .4f} \\n idiosyncratic = {idio_risk: .4f}')\n\nTSLA risk: \n total =  0.0374 \n systematic =  0.0058 \n idiosyncratic =  0.0316\n\n\n\n# Print as percentages of total risk\npct_sys_risk = sys_risk / tot_risk\npct_idio_risk = idio_risk / tot_risk\nprint(f'\\n percent systematic risk = {pct_sys_risk: .4f} \\n percent idiosyncratic risk = {pct_idio_risk: .4f}')\n\n\n percent systematic risk =  0.1551 \n percent idiosyncratic risk =  0.8449\n\n\nChallenge:\nUsing the Fama-French three factor model, what percentage of TSLA total risk is diversifiable and what percentage is undiversifiable?\n\nprint(\"TSLA pct diversifiable / idio risk = \", 1 - res3.rsquared)\nprint(\"TSLA pct non-diversifiable / sys risk = \", res3.rsquared)\n\nTSLA pct diversifiable / idio risk =  0.8358803520627377\nTSLA pct non-diversifiable / sys risk =  0.1641196479372623"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture22_portfolio_returns.html",
    "href": "teaching/FIN 525/lectures/lecture22_portfolio_returns.html",
    "title": "L22: Backtesting -returns",
    "section": "",
    "text": "# Import packages\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\n\n# Set option to display more rows\npd.options.display.max_rows = 30"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture22_portfolio_returns.html#equal-weighted-returns",
    "href": "teaching/FIN 525/lectures/lecture22_portfolio_returns.html#equal-weighted-returns",
    "title": "L22: Backtesting -returns",
    "section": "Equal-weighted returns",
    "text": "Equal-weighted returns\n\n# Summarize the data for a quick check\nmdata.groupby('portf_nr')['ret'].describe()\n# the means in this table are NOT average portfolio returns. why?\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nportf_nr\n\n\n\n\n\n\n\n\n\n\n\n\n1.0\n147074.0\n0.018182\n0.309638\n-0.973684\n-0.117514\n-0.006211\n0.096154\n24.000000\n\n\n2.0\n153599.0\n0.017011\n0.226782\n-0.972173\n-0.085714\n0.000000\n0.085714\n7.480000\n\n\n3.0\n156326.0\n0.016424\n0.195320\n-0.965789\n-0.066667\n0.000000\n0.076923\n19.000000\n\n\n4.0\n157548.0\n0.014418\n0.163905\n-0.943662\n-0.057143\n0.003167\n0.069767\n7.000000\n\n\n5.0\n158202.0\n0.014142\n0.150070\n-0.923077\n-0.053154\n0.005464\n0.068209\n5.500000\n\n\n6.0\n158581.0\n0.013548\n0.153137\n-0.928571\n-0.054348\n0.005400\n0.068966\n8.071428\n\n\n7.0\n158833.0\n0.012584\n0.161347\n-0.904247\n-0.058824\n0.004080\n0.071675\n14.000000\n\n\n8.0\n158437.0\n0.012378\n0.169273\n-0.926702\n-0.066176\n0.002985\n0.077250\n10.344000\n\n\n9.0\n158250.0\n0.009130\n0.181519\n-0.981295\n-0.078486\n0.000000\n0.082192\n7.093687\n\n\n10.0\n157495.0\n0.003622\n0.216186\n-0.993600\n-0.101009\n-0.004916\n0.087336\n13.495050\n\n\n\n\n\n\n\n\n# Equal-weighted portfolio returns each month\new_ret_monthly = mdata.groupby(['mdate', 'portf_nr'])['ret'].mean()\new_ret_monthly\n\nmdate    portf_nr\n1982-07  1.0        -0.003948\n         2.0        -0.011054\n         3.0         0.002117\n         4.0        -0.009741\n         5.0        -0.009275\n                       ...   \n2020-12  6.0         0.071565\n         7.0         0.084991\n         8.0         0.113125\n         9.0         0.092299\n         10.0        0.090850\nName: ret, Length: 4620, dtype: float64\n\n\n\n# Reshape to have returns of each portfolio side by side\new_ret = ew_ret_monthly.unstack(level = 'portf_nr')\new_ret\n\n\n\n\n\n\n\nportf_nr\n1.0\n2.0\n3.0\n4.0\n5.0\n6.0\n7.0\n8.0\n9.0\n10.0\n\n\nmdate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1982-07\n-0.003948\n-0.011054\n0.002117\n-0.009741\n-0.009275\n-0.003169\n-0.004816\n-0.006250\n-0.034018\n-0.071587\n\n\n1982-08\n0.043106\n0.058108\n0.067768\n0.071293\n0.070272\n0.076378\n0.080404\n0.078438\n0.076014\n0.069051\n\n\n1982-09\n0.037267\n0.034962\n0.034323\n0.037597\n0.034146\n0.047353\n0.034485\n0.030422\n0.009593\n-0.020953\n\n\n1982-10\n0.154922\n0.122708\n0.106485\n0.107380\n0.119641\n0.122316\n0.112235\n0.135088\n0.142025\n0.161400\n\n\n1982-11\n0.116354\n0.115989\n0.096393\n0.089470\n0.069214\n0.090190\n0.091515\n0.082948\n0.097205\n0.050597\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2020-08\n-0.017905\n0.033181\n0.046718\n0.054351\n0.058918\n0.061198\n0.049882\n0.057001\n0.035579\n0.086949\n\n\n2020-09\n-0.019968\n-0.032700\n-0.013606\n-0.023062\n-0.026950\n-0.032610\n-0.037596\n-0.021940\n-0.034342\n-0.024595\n\n\n2020-10\n-0.009192\n0.000250\n0.013428\n0.008144\n0.004559\n0.008367\n0.025419\n0.014769\n-0.001629\n-0.004516\n\n\n2020-11\n0.277547\n0.270265\n0.255649\n0.216389\n0.174494\n0.172263\n0.170583\n0.177468\n0.226676\n0.218597\n\n\n2020-12\n0.143785\n0.091213\n0.119753\n0.064887\n0.068347\n0.071565\n0.084991\n0.113125\n0.092299\n0.090850\n\n\n\n\n462 rows × 10 columns\n\n\n\n\n# Create new column that stores the returns of the \"spread\" portfolio\new_ret['Spread'] = ew_ret[1] - ew_ret[10]\new_ret\n\n\n\n\n\n\n\nportf_nr\n1.0\n2.0\n3.0\n4.0\n5.0\n6.0\n7.0\n8.0\n9.0\n10.0\nSpread\n\n\nmdate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1982-07\n-0.003948\n-0.011054\n0.002117\n-0.009741\n-0.009275\n-0.003169\n-0.004816\n-0.006250\n-0.034018\n-0.071587\n0.067639\n\n\n1982-08\n0.043106\n0.058108\n0.067768\n0.071293\n0.070272\n0.076378\n0.080404\n0.078438\n0.076014\n0.069051\n-0.025945\n\n\n1982-09\n0.037267\n0.034962\n0.034323\n0.037597\n0.034146\n0.047353\n0.034485\n0.030422\n0.009593\n-0.020953\n0.058220\n\n\n1982-10\n0.154922\n0.122708\n0.106485\n0.107380\n0.119641\n0.122316\n0.112235\n0.135088\n0.142025\n0.161400\n-0.006478\n\n\n1982-11\n0.116354\n0.115989\n0.096393\n0.089470\n0.069214\n0.090190\n0.091515\n0.082948\n0.097205\n0.050597\n0.065757\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2020-08\n-0.017905\n0.033181\n0.046718\n0.054351\n0.058918\n0.061198\n0.049882\n0.057001\n0.035579\n0.086949\n-0.104854\n\n\n2020-09\n-0.019968\n-0.032700\n-0.013606\n-0.023062\n-0.026950\n-0.032610\n-0.037596\n-0.021940\n-0.034342\n-0.024595\n0.004627\n\n\n2020-10\n-0.009192\n0.000250\n0.013428\n0.008144\n0.004559\n0.008367\n0.025419\n0.014769\n-0.001629\n-0.004516\n-0.004676\n\n\n2020-11\n0.277547\n0.270265\n0.255649\n0.216389\n0.174494\n0.172263\n0.170583\n0.177468\n0.226676\n0.218597\n0.058951\n\n\n2020-12\n0.143785\n0.091213\n0.119753\n0.064887\n0.068347\n0.071565\n0.084991\n0.113125\n0.092299\n0.090850\n0.052935\n\n\n\n\n462 rows × 11 columns\n\n\n\n\n# Save the data for later use\new_ret.to_pickle('../data/AG_ew_returns.zip')"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture22_portfolio_returns.html#value-weighted-returns",
    "href": "teaching/FIN 525/lectures/lecture22_portfolio_returns.html#value-weighted-returns",
    "title": "L22: Backtesting -returns",
    "section": "Value-weighted returns",
    "text": "Value-weighted returns\n\n# Calculate returns times lagged market cap and sum it up for each portfolio, each month\nmdata['ret_x_size'] = mdata['ret'] * mdata['mktcap_lag1']\nsum_ret_x_size = mdata.groupby(['mdate','portf_nr'])['ret_x_size'].sum()\nsum_ret_x_size\n\nmdate    portf_nr\n1982-07  1.0           -198.892418\n         2.0          -1262.453785\n         3.0           -457.243419\n         4.0          -2130.685234\n         5.0          -1447.946100\n                         ...      \n2020-12  6.0          59020.024241\n         7.0         314564.215682\n         8.0         140453.373218\n         9.0         145595.726573\n         10.0        143887.220882\nName: ret_x_size, Length: 4620, dtype: float64\n\n\n\n# Calculate sum of lagged market cap for each portfolio each month\nsum_size = mdata.groupby(['mdate','portf_nr'])['mktcap_lag1'].sum()\nsum_size\n\nmdate    portf_nr\n1982-07  1.0         1.205728e+04\n         2.0         3.883712e+04\n         3.0         5.122119e+04\n         4.0         1.056480e+05\n         5.0         1.776978e+05\n                         ...     \n2020-12  6.0         2.856337e+06\n         7.0         4.875469e+06\n         8.0         3.383027e+06\n         9.0         4.879776e+06\n         10.0        1.983976e+06\nName: mktcap_lag1, Length: 4620, dtype: float64\n\n\n\n# Calculate monthly VW returns\nvw_ret_monthly = sum_ret_x_size / sum_size\nvw_ret_monthly\n\nmdate    portf_nr\n1982-07  1.0        -0.016496\n         2.0        -0.032506\n         3.0        -0.008927\n         4.0        -0.020168\n         5.0        -0.008148\n                       ...   \n2020-12  6.0         0.020663\n         7.0         0.064520\n         8.0         0.041517\n         9.0         0.029837\n         10.0        0.072525\nLength: 4620, dtype: float64\n\n\n\n# Reshape to have returns of each portfolio side by side\nvw_ret = vw_ret_monthly.unstack(level = 'portf_nr')\nvw_ret\n\n\n\n\n\n\n\nportf_nr\n1.0\n2.0\n3.0\n4.0\n5.0\n6.0\n7.0\n8.0\n9.0\n10.0\n\n\nmdate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1982-07\n-0.016496\n-0.032506\n-0.008927\n-0.020168\n-0.008148\n-0.014216\n-0.016144\n-0.026691\n-0.052065\n-0.063027\n\n\n1982-08\n0.112879\n0.126619\n0.133671\n0.111961\n0.122147\n0.108101\n0.125463\n0.129065\n0.111937\n0.149069\n\n\n1982-09\n0.035342\n-0.011639\n0.009310\n0.021978\n0.013155\n0.008232\n0.015515\n0.021661\n0.006081\n-0.006955\n\n\n1982-10\n0.180624\n0.147793\n0.118260\n0.096087\n0.082554\n0.109715\n0.099294\n0.108276\n0.163611\n0.183572\n\n\n1982-11\n0.075535\n0.093501\n0.050342\n0.041856\n0.039193\n0.049904\n0.040065\n0.032300\n0.074103\n0.070121\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2020-08\n0.032559\n0.139042\n0.035979\n0.056023\n0.058803\n0.038203\n0.111807\n0.092513\n0.092279\n0.057890\n\n\n2020-09\n0.004011\n-0.082032\n-0.042604\n-0.012615\n-0.016740\n-0.012480\n-0.048656\n-0.032464\n-0.058615\n-0.023852\n\n\n2020-10\n-0.026101\n-0.057324\n-0.011819\n-0.013965\n-0.032251\n-0.025216\n-0.022518\n-0.021996\n-0.025194\n-0.005164\n\n\n2020-11\n0.167945\n0.119816\n0.128203\n0.115096\n0.120896\n0.111345\n0.129932\n0.119658\n0.076356\n0.167235\n\n\n2020-12\n0.053599\n0.098469\n0.029864\n0.019953\n0.032998\n0.020663\n0.064520\n0.041517\n0.029837\n0.072525\n\n\n\n\n462 rows × 10 columns\n\n\n\n\n# Create new column that stores the returns of the \"spread\" portfolio\nvw_ret['Spread'] = vw_ret[1] - vw_ret[10]\nvw_ret\n\n\n\n\n\n\n\nportf_nr\n1.0\n2.0\n3.0\n4.0\n5.0\n6.0\n7.0\n8.0\n9.0\n10.0\nSpread\n\n\nmdate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1982-07\n-0.016496\n-0.032506\n-0.008927\n-0.020168\n-0.008148\n-0.014216\n-0.016144\n-0.026691\n-0.052065\n-0.063027\n0.046531\n\n\n1982-08\n0.112879\n0.126619\n0.133671\n0.111961\n0.122147\n0.108101\n0.125463\n0.129065\n0.111937\n0.149069\n-0.036190\n\n\n1982-09\n0.035342\n-0.011639\n0.009310\n0.021978\n0.013155\n0.008232\n0.015515\n0.021661\n0.006081\n-0.006955\n0.042297\n\n\n1982-10\n0.180624\n0.147793\n0.118260\n0.096087\n0.082554\n0.109715\n0.099294\n0.108276\n0.163611\n0.183572\n-0.002948\n\n\n1982-11\n0.075535\n0.093501\n0.050342\n0.041856\n0.039193\n0.049904\n0.040065\n0.032300\n0.074103\n0.070121\n0.005414\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2020-08\n0.032559\n0.139042\n0.035979\n0.056023\n0.058803\n0.038203\n0.111807\n0.092513\n0.092279\n0.057890\n-0.025331\n\n\n2020-09\n0.004011\n-0.082032\n-0.042604\n-0.012615\n-0.016740\n-0.012480\n-0.048656\n-0.032464\n-0.058615\n-0.023852\n0.027864\n\n\n2020-10\n-0.026101\n-0.057324\n-0.011819\n-0.013965\n-0.032251\n-0.025216\n-0.022518\n-0.021996\n-0.025194\n-0.005164\n-0.020937\n\n\n2020-11\n0.167945\n0.119816\n0.128203\n0.115096\n0.120896\n0.111345\n0.129932\n0.119658\n0.076356\n0.167235\n0.000710\n\n\n2020-12\n0.053599\n0.098469\n0.029864\n0.019953\n0.032998\n0.020663\n0.064520\n0.041517\n0.029837\n0.072525\n-0.018926\n\n\n\n\n462 rows × 11 columns\n\n\n\n\n# Save the data for later use\nvw_ret.to_pickle('../data/AG_vw_returns.zip')\nvw_ret\n\n\n\n\n\n\n\nportf_nr\n1.0\n2.0\n3.0\n4.0\n5.0\n6.0\n7.0\n8.0\n9.0\n10.0\nSpread\n\n\nmdate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1982-07\n-0.016496\n-0.032506\n-0.008927\n-0.020168\n-0.008148\n-0.014216\n-0.016144\n-0.026691\n-0.052065\n-0.063027\n0.046531\n\n\n1982-08\n0.112879\n0.126619\n0.133671\n0.111961\n0.122147\n0.108101\n0.125463\n0.129065\n0.111937\n0.149069\n-0.036190\n\n\n1982-09\n0.035342\n-0.011639\n0.009310\n0.021978\n0.013155\n0.008232\n0.015515\n0.021661\n0.006081\n-0.006955\n0.042297\n\n\n1982-10\n0.180624\n0.147793\n0.118260\n0.096087\n0.082554\n0.109715\n0.099294\n0.108276\n0.163611\n0.183572\n-0.002948\n\n\n1982-11\n0.075535\n0.093501\n0.050342\n0.041856\n0.039193\n0.049904\n0.040065\n0.032300\n0.074103\n0.070121\n0.005414\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2020-08\n0.032559\n0.139042\n0.035979\n0.056023\n0.058803\n0.038203\n0.111807\n0.092513\n0.092279\n0.057890\n-0.025331\n\n\n2020-09\n0.004011\n-0.082032\n-0.042604\n-0.012615\n-0.016740\n-0.012480\n-0.048656\n-0.032464\n-0.058615\n-0.023852\n0.027864\n\n\n2020-10\n-0.026101\n-0.057324\n-0.011819\n-0.013965\n-0.032251\n-0.025216\n-0.022518\n-0.021996\n-0.025194\n-0.005164\n-0.020937\n\n\n2020-11\n0.167945\n0.119816\n0.128203\n0.115096\n0.120896\n0.111345\n0.129932\n0.119658\n0.076356\n0.167235\n0.000710\n\n\n2020-12\n0.053599\n0.098469\n0.029864\n0.019953\n0.032998\n0.020663\n0.064520\n0.041517\n0.029837\n0.072525\n-0.018926\n\n\n\n\n462 rows × 11 columns"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture09_data_cleaning.html",
    "href": "teaching/FIN 525/lectures/lecture09_data_cleaning.html",
    "title": "L09: Pandas data cleaning",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\n\ndf = pd.read_excel('./rawdata.xlsx')\ndf\n\n\n\n\n\n\n\n\nfirmid\ndate\nreturn\nindustry\nind_code\n\n\n\n\n0\n1\n12/31/2008\n0.05\nFinance\n1\n\n\n1\n2\nNaN\n0.45\nConstruction\n2\n\n\n2\n3\n12/31/2008\n23.00\nNaN\nM\n\n\n3\n1\n12/31/2009\n0.87\nFinance\n1\n\n\n4\n2\n12/31/2009\nNaN\nConstruct\n2\n\n\n5\n2\n12/31/2009\n0.34\nConstruction\n2\n\n\n\n\n\n\n\n\ndf.dtypes\n\nfirmid        int64\ndate         object\nreturn      float64\nindustry     object\nind_code     object\ndtype: object\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 6 entries, 0 to 5\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   firmid    6 non-null      int64  \n 1   date      5 non-null      object \n 2   return    5 non-null      float64\n 3   industry  5 non-null      object \n 4   ind_code  6 non-null      object \ndtypes: float64(1), int64(1), object(3)\nmemory usage: 368.0+ bytes\n\n\nThe “object” data type (for the industry and ind_code columns) is a catch-all term for when Pandas can not determine the exact data type of that column (e.g. int, float, str, etc). Many times, columns containing strings will have this data type.\n\ndf.describe()\n\n\n\n\n\n\n\n\nfirmid\nreturn\n\n\n\n\ncount\n6.000000\n5.000000\n\n\nmean\n1.833333\n4.942000\n\n\nstd\n0.752773\n10.099018\n\n\nmin\n1.000000\n0.050000\n\n\n25%\n1.250000\n0.340000\n\n\n50%\n2.000000\n0.450000\n\n\n75%\n2.000000\n0.870000\n\n\nmax\n3.000000\n23.000000"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture09_data_cleaning.html#the-.astype-attribute",
    "href": "teaching/FIN 525/lectures/lecture09_data_cleaning.html#the-.astype-attribute",
    "title": "L09: Pandas data cleaning",
    "section": "The “.astype” attribute",
    "text": "The “.astype” attribute\nSpecify the new datatype that you want to convert to as an argument to .astype():\n\ndf2 = df.copy()\ndf2.dtypes\n\nfirmid        int64\ndate         object\nreturn      float64\nindustry     object\nind_code     object\ndtype: object\n\n\n\ndf2['firmid'] = df2['firmid'].astype('float64')\ndf2.dtypes\n\nfirmid      float64\ndate         object\nreturn      float64\nindustry     object\nind_code     object\ndtype: object\n\n\n\ndf2\n\n\n\n\n\n\n\n\nfirmid\ndate\nreturn\nindustry\nind_code\n\n\n\n\n0\n1.0\n12/31/2008\n0.05\nFinance\n1\n\n\n1\n2.0\nNaN\n0.45\nConstruction\n2\n\n\n2\n3.0\n12/31/2008\n23.00\nNaN\nM\n\n\n3\n1.0\n12/31/2009\n0.87\nFinance\n1\n\n\n4\n2.0\n12/31/2009\nNaN\nConstruct\n2\n\n\n5\n2.0\n12/31/2009\n0.34\nConstruction\n2\n\n\n\n\n\n\n\n\ndf2['firmid'] = df2['firmid'].astype('string')\ndf2.dtypes\n\nfirmid       string\ndate         object\nreturn      float64\nindustry     object\nind_code     object\ndtype: object\n\n\n\ndf2\n\n\n\n\n\n\n\n\nfirmid\ndate\nreturn\nindustry\nind_code\n\n\n\n\n0\n1.0\n12/31/2008\n0.05\nFinance\n1\n\n\n1\n2.0\nNaN\n0.45\nConstruction\n2\n\n\n2\n3.0\n12/31/2008\n23.00\nNaN\nM\n\n\n3\n1.0\n12/31/2009\n0.87\nFinance\n1\n\n\n4\n2.0\n12/31/2009\nNaN\nConstruct\n2\n\n\n5\n2.0\n12/31/2009\n0.34\nConstruction\n2\n\n\n\n\n\n\n\nIt may not look like firmid is a string data type now but it is. For example, the below command would not work if firmid was still numeric:\n\ndf2['newid'] = df2['firmid'] + \"abc\"\ndf2\n\n\n\n\n\n\n\n\nfirmid\ndate\nreturn\nindustry\nind_code\nnewid\n\n\n\n\n0\n1.0\n12/31/2008\n0.05\nFinance\n1\n1.0abc\n\n\n1\n2.0\nNaN\n0.45\nConstruction\n2\n2.0abc\n\n\n2\n3.0\n12/31/2008\n23.00\nNaN\nM\n3.0abc\n\n\n3\n1.0\n12/31/2009\n0.87\nFinance\n1\n1.0abc\n\n\n4\n2.0\n12/31/2009\nNaN\nConstruct\n2\n2.0abc\n\n\n5\n2.0\n12/31/2009\n0.34\nConstruction\n2\n2.0abc"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture09_data_cleaning.html#the-.to_numeric-attribute",
    "href": "teaching/FIN 525/lectures/lecture09_data_cleaning.html#the-.to_numeric-attribute",
    "title": "L09: Pandas data cleaning",
    "section": "The “.to_numeric” attribute",
    "text": "The “.to_numeric” attribute\nThis is generally used to convert string (or object) data types to a numeric data type. Unlike .astype() which can be applied after the name of the dataframe we want to convert, with .to_numeric(), you have to supply that dataframe as an argument:\n\ndf2.dtypes\n\nfirmid       string\ndate         object\nreturn      float64\nindustry     object\nind_code     object\nnewid        string\ndtype: object\n\n\n\ndf2['firmid'] = pd.to_numeric(df2['firmid'])\ndf2.dtypes\n\nfirmid      float64\ndate         object\nreturn      float64\nindustry     object\nind_code     object\nnewid        string\ndtype: object\n\n\nIn some situations, the .to_numeric() function will not be successful unless you specify the parameter errors = `coerce'. For example, the code below would not work without that parameter (which is why I always specify it):\n\ndf2['ind_code'] = pd.to_numeric(df2['ind_code'], errors = 'coerce')\ndf2.dtypes\n\nfirmid      float64\ndate         object\nreturn      float64\nindustry     object\nind_code    float64\nnewid        string\ndtype: object\n\n\nNote that this converted the non-numeric values in the ind_code column to NaN:\n\ndf2\n\n\n\n\n\n\n\n\nfirmid\ndate\nreturn\nindustry\nind_code\nnewid\n\n\n\n\n0\n1.0\n12/31/2008\n0.05\nFinance\n1.0\n1.0abc\n\n\n1\n2.0\nNaN\n0.45\nConstruction\n2.0\n2.0abc\n\n\n2\n3.0\n12/31/2008\n23.00\nNaN\nNaN\n3.0abc\n\n\n3\n1.0\n12/31/2009\n0.87\nFinance\n1.0\n1.0abc\n\n\n4\n2.0\n12/31/2009\nNaN\nConstruct\n2.0\n2.0abc\n\n\n5\n2.0\n12/31/2009\n0.34\nConstruction\n2.0\n2.0abc"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture09_data_cleaning.html#duplicated",
    "href": "teaching/FIN 525/lectures/lecture09_data_cleaning.html#duplicated",
    "title": "L09: Pandas data cleaning",
    "section": ".duplicated",
    "text": ".duplicated\nSyntax:\nDataFrame.duplicated(subset=None, keep='first')\nwhere the subset parameter allows us to specifies where in the dataset (which columns) we are looking for duplicated rows (if unspecified, Pandas will look for instances where an entire row is duplicated). The keep parameter allows us to specify,\n\ndf.duplicated()\n\n0    False\n1    False\n2    False\n3    False\n4    False\n5    False\ndtype: bool\n\n\n\ndf.duplicated(subset = ['firmid','date'])\n\n0    False\n1    False\n2    False\n3    False\n4    False\n5     True\ndtype: bool\n\n\n\ndf.duplicated(subset = ['firmid','date'], keep='last')\n\n0    False\n1    False\n2    False\n3    False\n4     True\n5    False\ndtype: bool\n\n\n\ndf.duplicated(subset = ['firmid','date'], keep=False)\n\n0    False\n1    False\n2    False\n3    False\n4     True\n5     True\ndtype: bool\n\n\nTo drop duplicated data, we can use the .duplicated() function inside a .loc[]:\n\ndf2 = df.loc[~df.duplicated(['firmid','date'])]\ndf2\n\n\n\n\n\n\n\n\nfirmid\ndate\nreturn\nindustry\nind_code\n\n\n\n\n0\n1\n12/31/2008\n0.05\nFinance\n1\n\n\n1\n2\nNaN\n0.45\nConstruction\n2\n\n\n2\n3\n12/31/2008\n23.00\nNaN\nM\n\n\n3\n1\n12/31/2009\n0.87\nFinance\n1\n\n\n4\n2\n12/31/2009\nNaN\nConstruct\n2\n\n\n\n\n\n\n\nor, more commonly, using the .drop_duplicates() function:\n\ndf2 = df.drop_duplicates(['firmid','date'])\ndf2\n\n\n\n\n\n\n\n\nfirmid\ndate\nreturn\nindustry\nind_code\n\n\n\n\n0\n1\n12/31/2008\n0.05\nFinance\n1\n\n\n1\n2\nNaN\n0.45\nConstruction\n2\n\n\n2\n3\n12/31/2008\n23.00\nNaN\nM\n\n\n3\n1\n12/31/2009\n0.87\nFinance\n1\n\n\n4\n2\n12/31/2009\nNaN\nConstruct\n2\n\n\n\n\n\n\n\nNote that the above still keeps the 4th row, and drops the 5th (a duplicate of the 4th). This is because keep='first' by default for the .drop_duplicates() function. To eliminate both duplicated rows, we would have to set keep=False:\n\ndf2 = df.drop_duplicates(['firmid','date'], keep = False)\ndf2\n\n\n\n\n\n\n\n\nfirmid\ndate\nreturn\nindustry\nind_code\n\n\n\n\n0\n1\n12/31/2008\n0.05\nFinance\n1\n\n\n1\n2\nNaN\n0.45\nConstruction\n2\n\n\n2\n3\n12/31/2008\n23.00\nNaN\nM\n\n\n3\n1\n12/31/2009\n0.87\nFinance\n1\n\n\n\n\n\n\n\nNote also that the meaning of “first” and “last” for the keep parameter depends on how your dataframe happens to be sorted at the time you drop the duplicates."
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture09_data_cleaning.html#value_counts",
    "href": "teaching/FIN 525/lectures/lecture09_data_cleaning.html#value_counts",
    "title": "L09: Pandas data cleaning",
    "section": ".value_counts()",
    "text": ".value_counts()\nSyntax:\nDataFrame.value_counts(subset=None, normalize=False, sort=True, ascending=False, dropna=True)\n\ndf['industry'].value_counts()\n\nFinance         2\nConstruction    2\nConstruct       1\nName: industry, dtype: int64\n\n\n\ndf.value_counts('industry')\n\nindustry\nConstruction    2\nFinance         2\nConstruct       1\ndtype: int64"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture09_data_cleaning.html#slicing-into-string-data",
    "href": "teaching/FIN 525/lectures/lecture09_data_cleaning.html#slicing-into-string-data",
    "title": "L09: Pandas data cleaning",
    "section": "Slicing into string data",
    "text": "Slicing into string data\n\ndf['industry'].str[0:3]\n\n0     Fin\n1     Con\n2    &lt;NA&gt;\n3     Fin\n4     Con\n5     Con\nName: industry, dtype: string"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture09_data_cleaning.html#converting-to-lower-case-or-upper-case",
    "href": "teaching/FIN 525/lectures/lecture09_data_cleaning.html#converting-to-lower-case-or-upper-case",
    "title": "L09: Pandas data cleaning",
    "section": "Converting to lower case or upper case",
    "text": "Converting to lower case or upper case\n\ndf['industry'].str.lower()\n\n0         finance\n1    construction\n2            &lt;NA&gt;\n3         finance\n4       construct\n5    construction\nName: industry, dtype: string\n\n\n\ndf.columns.str.upper()\n\nIndex(['FIRMID', 'DATE', 'RETURN', 'INDUSTRY', 'IND_CODE'], dtype='object')"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture09_data_cleaning.html#substrings",
    "href": "teaching/FIN 525/lectures/lecture09_data_cleaning.html#substrings",
    "title": "L09: Pandas data cleaning",
    "section": "Substrings",
    "text": "Substrings\n\ndf.loc[df['industry'].str.contains(\"Cons\"),:]\n\n\n\n\n\n\n\n\nfirmid\ndate\nreturn\nindustry\nind_code\n\n\n\n\n1\n2\nNaN\n0.45\nConstruction\n2\n\n\n4\n2\n12/31/2009\nNaN\nConstruct\n2\n\n\n5\n2\n12/31/2009\n0.34\nConstruction\n2\n\n\n\n\n\n\n\n\ndf['industry'] = df['industry'].str.replace(\"Construct\",\"Construction\")\ndf\n\n\n\n\n\n\n\n\nfirmid\ndate\nreturn\nindustry\nind_code\n\n\n\n\n0\n1\n12/31/2008\n0.05\nFinance\n1\n\n\n1\n2\nNaN\n0.45\nConstructionion\n2\n\n\n2\n3\n12/31/2008\n23.00\n&lt;NA&gt;\nM\n\n\n3\n1\n12/31/2009\n0.87\nFinance\n1\n\n\n4\n2\n12/31/2009\nNaN\nConstruction\n2\n\n\n5\n2\n12/31/2009\n0.34\nConstructionion\n2\n\n\n\n\n\n\n\n\ndf.loc[df['industry'].str.contains('Cons'), 'industry'] = 'Construction'\ndf\n\n\n\n\n\n\n\n\nfirmid\ndate\nreturn\nindustry\nind_code\n\n\n\n\n0\n1\n12/31/2008\n0.05\nFinance\n1\n\n\n1\n2\nNaN\n0.45\nConstruction\n2\n\n\n2\n3\n12/31/2008\n23.00\n&lt;NA&gt;\nM\n\n\n3\n1\n12/31/2009\n0.87\nFinance\n1\n\n\n4\n2\n12/31/2009\nNaN\nConstruction\n2\n\n\n5\n2\n12/31/2009\n0.34\nConstruction\n2"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture09_data_cleaning.html#splitting",
    "href": "teaching/FIN 525/lectures/lecture09_data_cleaning.html#splitting",
    "title": "L09: Pandas data cleaning",
    "section": "Splitting",
    "text": "Splitting\n\ndf[['month','day','year']] = df['date'].str.split(pat = \"/\", expand = True)\ndf\n\n\n\n\n\n\n\n\nfirmid\ndate\nreturn\nindustry\nind_code\nmonth\nday\nyear\n\n\n\n\n0\n1\n12/31/2008\n0.05\nFinance\n1\n12\n31\n2008\n\n\n1\n2\nNaN\n0.45\nConstruction\n2\nNaN\nNaN\nNaN\n\n\n2\n3\n12/31/2008\n23.00\n&lt;NA&gt;\nM\n12\n31\n2008\n\n\n3\n1\n12/31/2009\n0.87\nFinance\n1\n12\n31\n2009\n\n\n4\n2\n12/31/2009\nNaN\nConstruction\n2\n12\n31\n2009\n\n\n5\n2\n12/31/2009\n0.34\nConstruction\n2\n12\n31\n2009"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture09_data_cleaning.html#stripping-white-spaces",
    "href": "teaching/FIN 525/lectures/lecture09_data_cleaning.html#stripping-white-spaces",
    "title": "L09: Pandas data cleaning",
    "section": "Stripping white spaces",
    "text": "Stripping white spaces\n\nnewdf = pd.DataFrame(np.random.randn(3, 2), columns=[\" Column A \", \" Column B \"])\nnewdf\n\n\n\n\n\n\n\n\nColumn A\nColumn B\n\n\n\n\n0\n1.764450\n0.697717\n\n\n1\n-0.865316\n-0.428507\n\n\n2\n0.165881\n0.089415\n\n\n\n\n\n\n\n\n#newdf['Column A'] #this will not work\n\n\nnewdf.columns = newdf.columns.str.strip()\nnewdf\n\n\n\n\n\n\n\n\nColumn A\nColumn B\n\n\n\n\n0\n1.764450\n0.697717\n\n\n1\n-0.865316\n-0.428507\n\n\n2\n0.165881\n0.089415\n\n\n\n\n\n\n\n\nnewdf['Column A'] #this will work\n\n0    1.764450\n1   -0.865316\n2    0.165881\nName: Column A, dtype: float64"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture09_data_cleaning.html#chaining-.str-methods",
    "href": "teaching/FIN 525/lectures/lecture09_data_cleaning.html#chaining-.str-methods",
    "title": "L09: Pandas data cleaning",
    "section": "Chaining .str methods",
    "text": "Chaining .str methods\n\nnewdf.columns = newdf.columns.str.strip().str.replace(\" \",\"_\").str.lower()\nnewdf\n\n\n\n\n\n\n\n\ncolumn_a\ncolumn_b\n\n\n\n\n0\n1.764450\n0.697717\n\n\n1\n-0.865316\n-0.428507\n\n\n2\n0.165881\n0.089415"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture02_codeExecution_variables_types.html",
    "href": "teaching/FIN 525/lectures/lecture02_codeExecution_variables_types.html",
    "title": "L02: Variables, types, operators",
    "section": "",
    "text": "This lecture gives you a quick preview of some fundamental concepts in any programming language: “variables”, “types”, and “operations”.\nWe’ll write some python code in each notebook cell below, and then we’ll “run” or “execute” that code by hiting “CTL+ENTER” (or “CMD+ENTER” on Mac). You can also use “SHIFT+ENTER”, which will execute the cell and move the cursor to the cell below.\nWe finish by looking at several other ways in which you can get your computer to execute Python code (this is optional material).\nLet’s start with some very simple python code:\n\n# This is a comment (it is ignored by python)\nprint('Hello world!') # This is also a comment\n\nHere’s a more complicated example. We’ll spend the next few lectures learning all the fundamentals needed to understand what this code is doing and how it works:\n\nimport yfinance\nyfinance.download(['TSLA','META'], interval='1mo')[['Adj Close']].dropna().plot()"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture02_codeExecution_variables_types.html#mathematical-operators",
    "href": "teaching/FIN 525/lectures/lecture02_codeExecution_variables_types.html#mathematical-operators",
    "title": "L02: Variables, types, operators",
    "section": "Mathematical operators",
    "text": "Mathematical operators\n\nprint(1 + 2.3)\n\n\nprint(1 - 2.3)\n\n\nprint(10*2)\n\n\nprint(10/2)\n\n\n#Exponentiation\nprint(2**3)"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture02_codeExecution_variables_types.html#comparison-operators",
    "href": "teaching/FIN 525/lectures/lecture02_codeExecution_variables_types.html#comparison-operators",
    "title": "L02: Variables, types, operators",
    "section": "Comparison operators",
    "text": "Comparison operators\nChecking for equality\nWe use the == (double equal) operator to check if two values are equal. Note that this is different from the = (i.e. “assignment”) operator, which is used to create variables (see examples above.\n\nprint(1==1)\n\n\nprint(1==2)\n\nChecking for inequality\n\n# not equal\nprint(1 != 2)\n\n\n# strictly smaller than\nprint(1 &lt; 2)\n\n\n# strictly larger than\nprint(1 &gt; 2)\n\n\n# smaller or equal\nprint(1 &lt;= 1)\n\n\n# larger or equal\nprint(1 &gt;= 1)\n\nAll these operators work on any variables of numeric type you have previously declared.\n\na = 10\nb = 20\nprint(a != b)\n\nBut be careful what you are comparing because these operators also work on variables that are not of numeric type (and the results may surprise you):\n\na = \"A\"\nb = \"1\"\nprint(a &gt; b)\n\nThe result of a comparison can be assigned a variable name:\n\na = 10\nb = 20\nc = a &gt; b\nprint(c)"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture02_codeExecution_variables_types.html#logical-operators",
    "href": "teaching/FIN 525/lectures/lecture02_codeExecution_variables_types.html#logical-operators",
    "title": "L02: Variables, types, operators",
    "section": "Logical operators",
    "text": "Logical operators\nPython interprets 1 as True and 0 as False. We’ll talk more about this when we introduce conditional statements. If you want to read ahead on this topic, take a look here: https://docs.python.org/3/library/stdtypes.html\n\na = True\nb = False\nc = 1\nd = 0\n\n\n#Logical \"and\"\nprint(a and b)\n\n\nprint(a and c)\n\n\nprint(c and a)\n\n\n#Logical \"or\"\nprint(a or b)\n\n\n#Logical \"not\"\nprint(not a)"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture02_codeExecution_variables_types.html#string-operators",
    "href": "teaching/FIN 525/lectures/lecture02_codeExecution_variables_types.html#string-operators",
    "title": "L02: Variables, types, operators",
    "section": "String operators",
    "text": "String operators\nIn Python, a string is represented as a sequence of characters.\n\n# Concatenation (joining)\na = 'fin'\nb = \"525\" #note that both single quotes and double quotes can be used\nc = a + b\nprint(c)\nprint('fin'+'525')\n\n\n# Repetition (with the * operator)\nd = a * 3\nprint(d)\n\n\n# Slicing a single character (with the [] operator)\ne = a[1] #PYTHON STARTS COUNTING FROM 0 (0 means first, 1 means second, etc)\nprint(e)\nprint('abcd'[-2]) #the - sign means \"starting from the back\"\n\n\n# Slicing a range of characters (with the [:] operator)\nprint('Mihai Ion'[0:5]) #print from the first to the fifth characters of 'Mihai Ion'\nprint('Mihai Ion'[:5]) #same thing\nprint('Mihai Ion'[:-2]) #print all but the last 2 characters\n\n\n# Membership (with \"in\" operator)\nprint('s' in 'Mihai')\n\n\n# Membership (with \"not in\" operator)\nprint('s' not in 'Mihai')\n\nString formatting with f-Strings\nf-Strings are a relatively new addition to the Python language. They allow you insert the result of an expression inside another string.\n\n#Inserting the value of a variable inside another string\nname = 'Mihai'\nage = 104\ns = f'My name is {name}, and I am {age} years old' #the f at the beginning makes this an f-string. Note the {} inside.\nprint(s)\n\n\n#Insert the result of a computation inside another string\nprint(f\"Two plus two equals {2*2}\")\n\nWe can also specify how the value that we want to insert inside the f-string needs to be formatted:\n\nage = 104.123\nprint(f'My name is {name}, and I am {age: .2f} years old')\n\nThe “.2f” in the code above, tells Python we want “age” to be formatted as a float (hence the f in .2f) with only 2 digits after the decimal point (hence the .2 in .2f).\nWARNING\nf-strings may execute any piece of Python code you supply inside curly brackets. Never include in an f-string a piece of input you obtain from a third party (say a response someone puts into a form somewhere on your website). If they provide malicious code as their input, you may end up inadvertently running that code on your computer.\nFor example:\n\nf\"{__import__('os').remove('./TEST.txt')}\""
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture02_codeExecution_variables_types.html#using-jupyter-notebook",
    "href": "teaching/FIN 525/lectures/lecture02_codeExecution_variables_types.html#using-jupyter-notebook",
    "title": "L02: Variables, types, operators",
    "section": "Using Jupyter Notebook",
    "text": "Using Jupyter Notebook\nThis is the method used in this course. You have to make sure that the code you want to run is in a “code cell” rather than a “Markdown cell”. If you are not sure, you can convert a cell to a code cell by clicking on it, then pressing “esc” and then “Y” (or using the menu option, usually right below Navigate or Widgets).\nHow to run it: - Press “Shift + Enter” (at the same time) or “Ctrl + Enter” to run the code in that cell."
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture02_codeExecution_variables_types.html#using-a-repl",
    "href": "teaching/FIN 525/lectures/lecture02_codeExecution_variables_types.html#using-a-repl",
    "title": "L02: Variables, types, operators",
    "section": "Using a REPL",
    "text": "Using a REPL\nNot very useful when your program is large. It allows you to run code one instruction at a time, which is useful if you have some simple tasks that you want to quickly try out. However, if you want to execute hundreds of commands, executing them one at a time will not only be tedious, but also has the inconvenience that you can not save those commands so you can execute them at a later time.\nHow to run it: - Open a Anacond Prompt (or a terminal like PowerShell in Windows or Terminal in macOS and Bash in Linux) - Type “python” - Start typying your commands (you execute a command by pressing “Enter” after you type it)"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture02_codeExecution_variables_types.html#running-scripts-in-the-terminal",
    "href": "teaching/FIN 525/lectures/lecture02_codeExecution_variables_types.html#running-scripts-in-the-terminal",
    "title": "L02: Variables, types, operators",
    "section": "Running scripts in the terminal",
    "text": "Running scripts in the terminal\nInvolves writing your code in a text file (script) or multiple text files (saved with a “.py” extension) and then running them from a terminal (e.g. PowerShell in Windows and Bash in Linux or macOS). Many programmers use a text editor like Virtual Studio Code or Sublime Text (or an IDE. see below) to help organize their project, get syntax highlighting and completion, etc.\nHow to do it: - Write your code in a text file (using a text editor of your choice e.g. Notepad, TextEdit, gedit, SublimeText, VS Code) - Save the file with a “.py” extension (let’s call it “XXX.py” for this example). Note that, if you are using Notepad, when you save the file, you have to select “All Files” under the “Save as type” tab. - Open a Anaconda Prompt or any other terminal - Change directory to the folder where you saved your script, by typing “cd” followed by the path to that folder - Type “python XXX.py” and press “Enter”"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture02_codeExecution_variables_types.html#using-an-ide",
    "href": "teaching/FIN 525/lectures/lecture02_codeExecution_variables_types.html#using-an-ide",
    "title": "L02: Variables, types, operators",
    "section": "Using an IDE",
    "text": "Using an IDE\nFree, third-party software that integrates all the different parts of the Python programming process: code writing, code execution, data/results visualization, project organization etc. The Spyder IDE is most similar to the interfaces used by SAS, Stata, and Matlab but most of its funtionality is available in VS Code too.\nHow to do it: - Every IDE will be different, but most of them have a “Run” button that will just run the current script for you (so you’ll have to navigate to your script first). - Some IDEs will have a terminal embedded in them, in which case you can run your script in that terminal (using the python command followed by your script’s path and filename). - Other IDEs, like Spyder, will have something called an “IPython” console embedded in them. In these cases, you can use that console to run scripts as well. Just type %run followed by your script’s path and filename."
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture17_regression_application_panel.html",
    "href": "teaching/FIN 525/lectures/lecture17_regression_application_panel.html",
    "title": "L17: Panel regression intro",
    "section": "",
    "text": "# Import packages\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture17_regression_application_panel.html#the-effect-of-outliers",
    "href": "teaching/FIN 525/lectures/lecture17_regression_application_panel.html#the-effect-of-outliers",
    "title": "L17: Panel regression intro",
    "section": "The effect of outliers",
    "text": "The effect of outliers\nNow let’s use the winsorized variables and look at the difference. Check the coefficient on the investment variable in particular.\n\n# Using winsorized data\nresults_wins = sm.OLS(endog = comp['w_future_roa'],\n                      exog = comp[['const','w_cash','w_leverage','w_investment']],\n                      missing = 'drop').fit()\nprint(results_wins.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:           w_future_roa   R-squared:                       0.096\nModel:                            OLS   Adj. R-squared:                  0.096\nMethod:                 Least Squares   F-statistic:                     6524.\nDate:                Fri, 25 Feb 2022   Prob (F-statistic):               0.00\nTime:                        14:32:28   Log-Likelihood:                 5498.7\nNo. Observations:              185315   AIC:                        -1.099e+04\nDf Residuals:                  185311   BIC:                        -1.095e+04\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n================================================================================\n                   coef    std err          t      P&gt;|t|      [0.025      0.975]\n--------------------------------------------------------------------------------\nconst            0.0314      0.001     32.312      0.000       0.030       0.033\nw_cash          -0.3712      0.003   -134.431      0.000      -0.377      -0.366\nw_leverage      -0.0727      0.003    -23.971      0.000      -0.079      -0.067\nw_investment     0.1634      0.006     25.736      0.000       0.151       0.176\n==============================================================================\nOmnibus:                   126392.896   Durbin-Watson:                   1.015\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          1848286.569\nSkew:                          -3.171   Prob(JB):                         0.00\nKurtosis:                      17.112   Cond. No.                         12.0\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture17_regression_application_panel.html#economic-significance-vs-statistical-significance",
    "href": "teaching/FIN 525/lectures/lecture17_regression_application_panel.html#economic-significance-vs-statistical-significance",
    "title": "L17: Panel regression intro",
    "section": "Economic significance vs statistical significance",
    "text": "Economic significance vs statistical significance\nIt is easy to use the results in the regression output above and decide (based on p-values or t-statistics) if the independent variables have a statistically significant relation with the dependent variable. But it is not clear if there relations are large or small in magnitude (does investment have a large impact on future profitability? larger than leverage?). That is what we mean by economically significant.\nTo help ease the interpretation of these economic magnitudes, we generally standardize all the variables in the regression by subtracting their mean and dividing by their standard deviation (see below). After doing this, the regression coefficient on any independent variable X, will tell us by how many standard deviations we expect the dependent variable Y to move, when the X variable changes by one standard deviation.\nSo after the normalization, the X variables with larger coefficients have a larger economic impact on the Y variable.\n\n# Create list of names we want to give to the normalized varsions of these variables\nnormalized_vars = ['n_' + x for x in main_vars]\nnormalized_vars\n\n['n_future_roa', 'n_cash', 'n_leverage', 'n_investment']\n\n\n\n# Create normalized variables\ncomp[normalized_vars] = (comp[main_vars_wins] - comp[main_vars_wins].mean()) / comp[main_vars_wins].std()\n\n\n# Check that all means are 0 and all std deviations are 1\ncomp[normalized_vars].describe()\n\n\n\n\n\n\n\n\nn_future_roa\nn_cash\nn_leverage\nn_investment\n\n\n\n\ncount\n2.129550e+05\n2.364170e+05\n2.359690e+05\n2.074580e+05\n\n\nmean\n4.288606e-15\n-1.581191e-14\n-2.359298e-14\n2.044987e-15\n\n\nstd\n1.000000e+00\n1.000000e+00\n1.000000e+00\n1.000000e+00\n\n\nmin\n-5.656264e+00\n-7.966420e-01\n-9.061689e-01\n-4.625934e+00\n\n\n25%\n5.906184e-02\n-6.817643e-01\n-8.650839e-01\n-2.364965e-01\n\n\n50%\n2.524840e-01\n-4.287752e-01\n-3.294019e-01\n-1.252835e-01\n\n\n75%\n4.173736e-01\n2.767601e-01\n5.675461e-01\n2.116544e-01\n\n\nmax\n1.218065e+00\n3.454800e+00\n3.476980e+00\n3.877626e+00\n\n\n\n\n\n\n\n\n# Using winsorized, then normalized data\nresults_norm = sm.OLS(endog = comp['n_future_roa'],\n                      exog = comp[['const','n_cash','n_leverage','n_investment']],\n                      missing = 'drop').fit()\nprint(results_norm.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:           n_future_roa   R-squared:                       0.096\nModel:                            OLS   Adj. R-squared:                  0.096\nMethod:                 Least Squares   F-statistic:                     6524.\nDate:                Fri, 25 Feb 2022   Prob (F-statistic):               0.00\nTime:                        14:32:28   Log-Likelihood:            -2.4834e+05\nNo. Observations:              185315   AIC:                         4.967e+05\nDf Residuals:                  185311   BIC:                         4.967e+05\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n================================================================================\n                   coef    std err          t      P&gt;|t|      [0.025      0.975]\n--------------------------------------------------------------------------------\nconst            0.0037      0.002      1.737      0.082      -0.000       0.008\nn_cash          -0.3195      0.002   -134.431      0.000      -0.324      -0.315\nn_leverage      -0.0558      0.002    -23.971      0.000      -0.060      -0.051\nn_investment     0.0576      0.002     25.736      0.000       0.053       0.062\n==============================================================================\nOmnibus:                   126392.896   Durbin-Watson:                   1.015\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          1848286.569\nSkew:                          -3.171   Prob(JB):                         0.00\nKurtosis:                      17.112   Cond. No.                         1.40\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture17_regression_application_panel.html#multicollinearity",
    "href": "teaching/FIN 525/lectures/lecture17_regression_application_panel.html#multicollinearity",
    "title": "L17: Panel regression intro",
    "section": "Multicollinearity",
    "text": "Multicollinearity\nOne common way that multicollinearity arises when two or more of your independent variables (X) are very highly correlated (close to 1). The usual way to deal with this issue, is to calculate the correlation matrix between all the variables in your study, to identify which group of variables are highly correlated with each other. Then we simply drop all but one of them from the analysis.\nBelow, we artificially create this problem in our example application, by introducing in our regression a variable that equals the leverage variable times 100. This will have a correlation of 1 with the leverage variable. However, as we’ll see below, “statsmodels” will NOT give us an error. So it’s up to us to make sure that we don’t have this problem in our data by always looking at the correlation matrix of our data.\n\n# Add variable that is collinear with size\ncomp['lev100'] = comp['n_leverage']*100\n\n\n# Run regression with multicollinearity problem\nX = comp[['const','n_cash','n_leverage','n_investment','lev100']]\nresults_mc = sm.OLS(comp['n_future_roa'],X,missing = 'drop').fit()\nprint(results_mc.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:           n_future_roa   R-squared:                       0.096\nModel:                            OLS   Adj. R-squared:                  0.096\nMethod:                 Least Squares   F-statistic:                     6524.\nDate:                Fri, 25 Feb 2022   Prob (F-statistic):               0.00\nTime:                        14:32:28   Log-Likelihood:            -2.4834e+05\nNo. Observations:              185315   AIC:                         4.967e+05\nDf Residuals:                  185311   BIC:                         4.967e+05\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n================================================================================\n                   coef    std err          t      P&gt;|t|      [0.025      0.975]\n--------------------------------------------------------------------------------\nconst            0.0037      0.002      1.737      0.082      -0.000       0.008\nn_cash          -0.3195      0.002   -134.431      0.000      -0.324      -0.315\nn_leverage   -5.583e-06   2.33e-07    -23.971      0.000   -6.04e-06   -5.13e-06\nn_investment     0.0576      0.002     25.736      0.000       0.053       0.062\nlev100          -0.0006   2.33e-05    -23.971      0.000      -0.001      -0.001\n==============================================================================\nOmnibus:                   126392.896   Durbin-Watson:                   1.015\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          1848286.569\nSkew:                          -3.171   Prob(JB):                         0.00\nKurtosis:                      17.112   Cond. No.                     2.61e+17\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The smallest eigenvalue is 2.56e-26. This might indicate that there are\nstrong multicollinearity problems or that the design matrix is singular.\n\n\nNote how the coefficient on n_leverage has changed. Also, look at Notes [2] above.\n\n# Check for correlations to see which which variable to drop\nX.corr()\n\n\n\n\n\n\n\n\nconst\nn_cash\nn_leverage\nn_investment\nlev100\n\n\n\n\nconst\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nn_cash\nNaN\n1.000000\n-0.315295\n-0.070145\n-0.315295\n\n\nn_leverage\nNaN\n-0.315295\n1.000000\n0.074024\n1.000000\n\n\nn_investment\nNaN\n-0.070145\n0.074024\n1.000000\n0.074024\n\n\nlev100\nNaN\n-0.315295\n1.000000\n0.074024\n1.000000\n\n\n\n\n\n\n\nMulticollinearity can arise even if a “linear combination” (a weighted sum or difference) of our variables is highly correlated with some other variable in the regression. To see this in action, we will add to our explanatory variables, a variable called illiquid which measures the non-cash assets of the firm (divided by total assets). In this case, the sum of cash and illiquid will equal 1 at all times, which is equal to another explanatory variable in our regression: the constant term.\n\ncomp['illiquid'] = (comp['at'] - comp['che']) / comp['at']\n\n\n# Run regression with multicollinearity problem\nX = comp[['const','cash','leverage','investment','illiquid']]\nresults_mc = sm.OLS(comp['future_roa'],X,missing = 'drop').fit()\nprint(results_mc.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             future_roa   R-squared:                       0.001\nModel:                            OLS   Adj. R-squared:                  0.001\nMethod:                 Least Squares   F-statistic:                     65.43\nDate:                Fri, 25 Feb 2022   Prob (F-statistic):           2.82e-42\nTime:                        14:32:29   Log-Likelihood:            -3.4663e+05\nNo. Observations:              185315   AIC:                         6.933e+05\nDf Residuals:                  185311   BIC:                         6.933e+05\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -0.0712      0.005    -15.175      0.000      -0.080      -0.062\ncash          -0.1568      0.011    -14.354      0.000      -0.178      -0.135\nleverage      -0.1111      0.018     -6.133      0.000      -0.147      -0.076\ninvestment    -0.0013      0.000     -3.528      0.000      -0.002      -0.001\nilliquid       0.0856      0.008     11.360      0.000       0.071       0.100\n==============================================================================\nOmnibus:                   560269.038   Durbin-Watson:                   0.913\nProb(Omnibus):                  0.000   Jarque-Bera (JB):     887102384117.364\nSkew:                          43.457   Prob(JB):                         0.00\nKurtosis:                   10721.226   Cond. No.                     7.92e+16\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The smallest eigenvalue is 2.74e-27. This might indicate that there are\nstrong multicollinearity problems or that the design matrix is singular.\n\n\nAgain, we did not get an error, but the results above can not be trusted. To see this, you can check Notes [2] above, but you can also print out the correlation matrix.\n\nX.corr()\n\n\n\n\n\n\n\n\nconst\ncash\nleverage\ninvestment\nilliquid\n\n\n\n\nconst\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\ncash\nNaN\n1.000000\n-0.244805\n-0.013678\n-1.000000\n\n\nleverage\nNaN\n-0.244805\n1.000000\n0.000063\n0.244805\n\n\ninvestment\nNaN\n-0.013678\n0.000063\n1.000000\n0.013678\n\n\nilliquid\nNaN\n-1.000000\n0.244805\n0.013678\n1.000000\n\n\n\n\n\n\n\n\nthesum = comp['cash'] + comp['illiquid']\nthesum\n\n188730    1.0\n188550    1.0\n188566    1.0\n188567    1.0\n188568    1.0\n         ... \n493016    1.0\n493017    1.0\n493020    1.0\n493021    1.0\n493024    1.0\nLength: 237017, dtype: float64\n\n\nAgain, dropping one of the problem variables (“cash” or “illiquid”) would solve our problem."
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture08_filtering.html",
    "href": "teaching/FIN 525/lectures/lecture08_filtering.html",
    "title": "L08: Pandas filtering",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\n\ndf = pd.DataFrame(data=[['TSLA',1000, 0.1],['AAPL',2000, 0.05], ['MSFT', 100, 0.07]], \n                  index = ['Tesla','Apple', 'Microsoft'], \n                  columns = ['ticker','price', 'return'])\ndf\n\n\n\n\n\n\n\n\nticker\nprice\nreturn\n\n\n\n\nTesla\nTSLA\n1000\n0.10\n\n\nApple\nAAPL\n2000\n0.05\n\n\nMicrosoft\nMSFT\n100\n0.07"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture08_filtering.html#the-.loc-attribute",
    "href": "teaching/FIN 525/lectures/lecture08_filtering.html#the-.loc-attribute",
    "title": "L08: Pandas filtering",
    "section": "The “.loc” attribute",
    "text": "The “.loc” attribute\nThe most common way to access a subset of the data in a dataframe is through the “.loc” attribute. This attribute uses square brackets instead of parentheses and contains two arguments: the first one tells Python which rows you want, and the second one tells it which columns you want, which generally looks like this:\nDataFrame.loc[&lt;which_rows&gt;, &lt;which_columns&gt;]\nwhere, instead of DataFrame you wouls use the name of the full dataframe you want to subset. Pandas allows for a lot of flexibility as to what you can use instead of &lt;which_rows&gt; and &lt;which_column&gt; above. See the examples in the official documentation to get a more complete picture of what is possible with .loc[]: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html\nBelow, I cover the most common ways to specify which rows and columns you want:\n\nBy explicitly specifying the names (labels) of the index and/or column(s)\nUsing slices (ranges) on the index labels or common labels\nAnything that returns a boolean sequence (True will be interpreted as “I want this row/column”)\n\n\nSubsetting with explicit labels\n\ndf\n\n\n\n\n\n\n\n\nticker\nprice\nreturn\n\n\n\n\nTesla\nTSLA\n1000\n0.10\n\n\nApple\nAAPL\n2000\n0.05\n\n\nMicrosoft\nMSFT\n100\n0.07\n\n\n\n\n\n\n\n\ndf.loc['Tesla', 'price']\n\n1000\n\n\n\ndf.loc[['Tesla', 'Microsoft'], ['ticker','return']]\n\n\n\n\n\n\n\n\nticker\nreturn\n\n\n\n\nTesla\nTSLA\n0.10\n\n\nMicrosoft\nMSFT\n0.07\n\n\n\n\n\n\n\n\ndf.loc[:,['ticker']] #this returns a Pandas DataFrame\n\n\n\n\n\n\n\n\nticker\n\n\n\n\nTesla\nTSLA\n\n\nApple\nAAPL\n\n\nMicrosoft\nMSFT\n\n\n\n\n\n\n\n\ndf.loc[:,'ticker'] #this returns a Pandas Series\n\nTesla        TSLA\nApple        AAPL\nMicrosoft    MSFT\nName: ticker, dtype: object\n\n\n\ndf.loc[['Apple'],:]\n\n\n\n\n\n\n\n\nticker\nprice\nreturn\n\n\n\n\nApple\nAAPL\n2000\n0.05\n\n\n\n\n\n\n\n\ndf.loc['Apple']\n\nticker    AAPL\nprice     2000\nreturn    0.05\nName: Apple, dtype: object\n\n\n\n\nSubsetting with slices on labels\n\ndf\n\n\n\n\n\n\n\n\nticker\nprice\nreturn\n\n\n\n\nTesla\nTSLA\n1000\n0.10\n\n\nApple\nAAPL\n2000\n0.05\n\n\nMicrosoft\nMSFT\n100\n0.07\n\n\n\n\n\n\n\n\ndf.loc['Apple':'Microsoft', 'price':'return'] #note that endpoints of the range are included\n\n\n\n\n\n\n\n\nprice\nreturn\n\n\n\n\nApple\n2000\n0.05\n\n\nMicrosoft\n100\n0.07\n\n\n\n\n\n\n\n\ndf.loc['Tesla':'Apple']\n\n\n\n\n\n\n\n\nticker\nprice\nreturn\n\n\n\n\nTesla\nTSLA\n1000\n0.10\n\n\nApple\nAAPL\n2000\n0.05\n\n\n\n\n\n\n\n\ndf.loc[:,'ticker':'price']\n\n\n\n\n\n\n\n\nticker\nprice\n\n\n\n\nTesla\nTSLA\n1000\n\n\nApple\nAAPL\n2000\n\n\nMicrosoft\nMSFT\n100\n\n\n\n\n\n\n\n\n\nSubsetting with boolean arrays\n\ndf\n\n\n\n\n\n\n\n\nticker\nprice\nreturn\n\n\n\n\nTesla\nTSLA\n1000\n0.10\n\n\nApple\nAAPL\n2000\n0.05\n\n\nMicrosoft\nMSFT\n100\n0.07\n\n\n\n\n\n\n\n\ndf.loc[[True,True,False],\n      [True, False, True]]\n\n\n\n\n\n\n\n\nticker\nreturn\n\n\n\n\nTesla\nTSLA\n0.10\n\n\nApple\nAAPL\n0.05\n\n\n\n\n\n\n\n\ndf.loc[df['return'] &gt; 0.05, :]\n\n\n\n\n\n\n\n\nticker\nprice\nreturn\n\n\n\n\nTesla\nTSLA\n1000\n0.10\n\n\nMicrosoft\nMSFT\n100\n0.07\n\n\n\n\n\n\n\n\ndf.loc[:, df.columns.str.contains(\"tic\")] #we'll talk more about \".str\" next class\n\n\n\n\n\n\n\n\nticker\n\n\n\n\nTesla\nTSLA\n\n\nApple\nAAPL\n\n\nMicrosoft\nMSFT\n\n\n\n\n\n\n\n\ndf.loc[df['return'] &gt; 0.07,\n       df.columns.str.contains(\"tic\")]\n\n\n\n\n\n\n\n\nticker\n\n\n\n\nTesla\nTSLA"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture08_filtering.html#the-.iloc-attribute",
    "href": "teaching/FIN 525/lectures/lecture08_filtering.html#the-.iloc-attribute",
    "title": "L08: Pandas filtering",
    "section": "The “.iloc” attribute",
    "text": "The “.iloc” attribute\nWorks almost identically to .loc() with one crucial exception: .iloc() uses index/column integer positions (as opposed to labels like .loc()).\n\ndf\n\n\n\n\n\n\n\n\nticker\nprice\nreturn\n\n\n\n\nTesla\nTSLA\n1000\n0.10\n\n\nApple\nAAPL\n2000\n0.05\n\n\nMicrosoft\nMSFT\n100\n0.07\n\n\n\n\n\n\n\n\ndf.iloc[[0,2], [0,2]]\n\n\n\n\n\n\n\n\nticker\nreturn\n\n\n\n\nTesla\nTSLA\n0.10\n\n\nMicrosoft\nMSFT\n0.07\n\n\n\n\n\n\n\n\ndf.loc[['Tesla', 'Microsoft'], ['ticker','return']]\n\n\n\n\n\n\n\n\nticker\nreturn\n\n\n\n\nTesla\nTSLA\n0.10\n\n\nMicrosoft\nMSFT\n0.07\n\n\n\n\n\n\n\nSlicing also works, but this time we have to use index/column numbers, and the right-most end of the range is not included:\n\ndf.iloc[1:2, 0:2]\n\n\n\n\n\n\n\n\nticker\nprice\n\n\n\n\nApple\nAAPL\n2000\n\n\n\n\n\n\n\nWe rarely use boolean arrays with .iloc() so we will not cover it here."
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture08_filtering.html#the-.filter-attribute",
    "href": "teaching/FIN 525/lectures/lecture08_filtering.html#the-.filter-attribute",
    "title": "L08: Pandas filtering",
    "section": "The “.filter()” attribute",
    "text": "The “.filter()” attribute\nThe .filter() attribute comes in handy if we want to subset based on index or column names (labels). In particular, its like parameter allows us to specify that we want all rows/columns that contain a particular piece of text in their label.\nSyntax:\nDataFrame.filter(items=None, like=None, regex=None, axis=None)\nFor example:\n\ndf\n\n\n\n\n\n\n\n\nticker\nprice\nreturn\n\n\n\n\nTesla\nTSLA\n1000\n0.10\n\n\nApple\nAAPL\n2000\n0.05\n\n\nMicrosoft\nMSFT\n100\n0.07\n\n\n\n\n\n\n\n\ndf.filter(like=\"esla\", axis=0)\n\n\n\n\n\n\n\n\nticker\nprice\nreturn\n\n\n\n\nTesla\nTSLA\n1000\n0.1\n\n\n\n\n\n\n\n\ndf.filter(like='ret', axis=1)\n\n\n\n\n\n\n\n\nreturn\n\n\n\n\nTesla\n0.10\n\n\nApple\n0.05\n\n\nMicrosoft\n0.07"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture08_filtering.html#copies-vs-views",
    "href": "teaching/FIN 525/lectures/lecture08_filtering.html#copies-vs-views",
    "title": "L08: Pandas filtering",
    "section": "Copies vs “views”",
    "text": "Copies vs “views”\nLet’s make a copy of df that we can safely change for this section:\n\nnewdf = df\nnewdf\n\n\n\n\n\n\n\n\nticker\nprice\nreturn\n\n\n\n\nTesla\nTSLA\n1000\n0.10\n\n\nApple\nAAPL\n2000\n0.05\n\n\nMicrosoft\nMSFT\n100\n0.07\n\n\n\n\n\n\n\nMany times, we want to store a subset of a dataframe inside a new dataframe. For example:\n\nsub = newdf.loc[:,'price']\nsub\n\nTesla        1000\nApple        2000\nMicrosoft     100\nName: price, dtype: int64\n\n\nNow suppose we have to make a change to the larger dataframe newdf. For example:\n\nnewdf.loc['Tesla','price'] = 0\nnewdf\n\n\n\n\n\n\n\n\nticker\nprice\nreturn\n\n\n\n\nTesla\nTSLA\n0\n0.10\n\n\nApple\nAAPL\n2000\n0.05\n\n\nMicrosoft\nMSFT\n100\n0.07\n\n\n\n\n\n\n\nThis change will be passed to sub, even tough we never created this change explicitly ourselves:\n\nsub\n\nTesla           0\nApple        2000\nMicrosoft     100\nName: price, dtype: int64\n\n\nThis happened because, when we created sub with the command sub = newdf.loc[:,'price'], Pyhton did not actually create an entirely new dataframe. Instead, it just returned something like an address of where in newdf the price data can be found. This is called a view of the data.\nThis is done to preserve memory and speed up the code, but, like we saw above, it can cause some of our dataframes change when we edit other dataframes.\nTo avoid this possible problem, I recommend always telling Python to create a copy of the subset of data you want, using the .copy() attribute. In our example above, sub should have been created like this:\n\nsub = newdf.loc[:,'price'].copy()\nsub\n\nTesla           0\nApple        2000\nMicrosoft     100\nName: price, dtype: int64\n\n\nNow, changes to newdf, like this:\n\nnewdf.loc['Apple','price'] = 123\nnewdf\n\n\n\n\n\n\n\n\nticker\nprice\nreturn\n\n\n\n\nTesla\nTSLA\n0\n0.10\n\n\nApple\nAAPL\n123\n0.05\n\n\nMicrosoft\nMSFT\n100\n0.07\n\n\n\n\n\n\n\nWill not cause sub to change:\n\nsub\n\nTesla           0\nApple        2000\nMicrosoft     100\nName: price, dtype: int64"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture04_Conditional_Statements_and_Iteration.html#the-range-function",
    "href": "teaching/FIN 525/lectures/lecture04_Conditional_Statements_and_Iteration.html#the-range-function",
    "title": "L04: Conditionals, loops",
    "section": "The range() function",
    "text": "The range() function\nA range is another type of iterable that is commonly used in for loops. Ranges are sequences of integers created using the range() function.\nThe syntax of the range function is as follows:\nrange(&lt;start&gt;, &lt;end&gt;, &lt;step&gt;)\nThis gives us all the integers from “start” to “end” (excluding “end”) in increments of “step”. Both the “start” and “step” arguments can be omitted, but “end” must be supplied.\nLet’s work through some examples:\n\nfor i in range(1,10,3): #\"start\" is 1, \"end\" is 10, \"step\" is 3\n    print(i)\n\n\nfor i in range(3): #here we only supply the \"end\" of the range. note how 3 is excluded\n    print(i)"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture04_Conditional_Statements_and_Iteration.html#list-comprehensions",
    "href": "teaching/FIN 525/lectures/lecture04_Conditional_Statements_and_Iteration.html#list-comprehensions",
    "title": "L04: Conditionals, loops",
    "section": "List comprehensions",
    "text": "List comprehensions\nIf we want to apply some simple transformation to all the elements in an iterable and store these transformations in a new list, we can use a Python construct called list comprehensions.\nThe syntax for list comprehensions is as follows:\n&lt;newList&gt; = [&lt;statement&gt; for i in &lt;existingList&gt; if &lt;condition&gt; ]\nIn the above notation, Python goes through all the elements in “existingList” that satisfy “condition”, inputs each of these elements into “statement” (i.e. some Python code) one by one, and collects the results of these statements into a new list called “newList”.\nThat sounds pretty complicated but it really isn’t once you look at some examples:\n\nmyiter = [1,2,3]\nsq = [i**2 for i in myiter if i&lt;3]\nprint(sq)\n\nThe if statement is not required:\n\nsq = [i**2 for i in myiter]\nprint(sq)"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture18_robust_panel_regression.html",
    "href": "teaching/FIN 525/lectures/lecture18_robust_panel_regression.html",
    "title": "L18: Robust panel regression",
    "section": "",
    "text": "Remember from last lecture that we are using the following empirical question to showcase the statistical tools needed to run robust regression analysis:\nWhich of the following firm characteristics (if any) have statistically significant predictive power over firms’ profitability: the firm’s cash holdings, its book leverage or its capital investments?\nIn the previous lecture, we collected the data we needed for this analysis, produced some summary statistics and ran a basic linear regression where firm future profitability is the dependent variable, and firm cash holdings, book leverage, and investment are the explanatory variables. In this lecture, we continue this analysis by tackling two very common issues with linear regression analysis:\n\nThe potential presence of “fixed-effects” in the data\nThe issue of correlated error terms in the regression\n\nThe statsmodels package we used for the introductory regression materials does not implement some of the tools we will discuss in this lecture. So in these lecture notes, we will be using the linearmodels package, which can be installed by typing:\npip install linearmodels\nin a Terminal or Anaconda Prompt. Once you install the package, import the PanelOLS subpackage as below:"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture18_robust_panel_regression.html#firm-fixed-effects",
    "href": "teaching/FIN 525/lectures/lecture18_robust_panel_regression.html#firm-fixed-effects",
    "title": "L18: Robust panel regression",
    "section": "Firm fixed effects",
    "text": "Firm fixed effects\n\nresults_firmfe = PanelOLS(dependent = comp[yvar], \n                          exog = comp[xvars], \n                          entity_effects = True\n                         ).fit();\nprint(results_firmfe.summary)\n\n                          PanelOLS Estimation Summary                           \n================================================================================\nDep. Variable:           w_future_roa   R-squared:                        0.0035\nEstimator:                   PanelOLS   R-squared (Between):             -0.0776\nNo. Observations:              185315   R-squared (Within):               0.0035\nDate:                Fri, Feb 25 2022   R-squared (Overall):             -0.0129\nTime:                        14:36:53   Log-likelihood                 8.094e+04\nCov. Estimator:            Unadjusted                                           \n                                        F-statistic:                      196.81\nEntities:                       19139   P-value                           0.0000\nAvg Obs:                       9.6826   Distribution:                F(3,166173)\nMin Obs:                       1.0000                                           \nMax Obs:                       80.000   F-statistic (robust):             196.81\n                                        P-value                           0.0000\nTime periods:                      41   Distribution:                F(3,166173)\nAvg Obs:                       4519.9                                           \nMin Obs:                       4.0000                                           \nMax Obs:                       6276.0                                           \n                                                                                \n                              Parameter Estimates                               \n================================================================================\n              Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n--------------------------------------------------------------------------------\nconst           -0.0357     0.0010    -34.112     0.0000     -0.0377     -0.0336\nw_cash           0.0208     0.0038     5.4863     0.0000      0.0134      0.0283\nw_leverage      -0.0557     0.0036    -15.452     0.0000     -0.0628     -0.0487\nw_investment     0.0862     0.0050     17.261     0.0000      0.0764      0.0960\n================================================================================\n\nF-test for Poolability: 10.917\nP-value: 0.0000\nDistribution: F(19138,166173)\n\nIncluded effects: Entity\n\n\nThe P-value under F-test for Poolability is very low, which tells us that the firm fixed effects are jointly statistically significant in our regression (i.e. we should keep them in our regression).\nNote how the coefficients have changed now that we have included firm fixed effects in our regression. In particular, note that the coefficient on w_cash has changed sign."
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture18_robust_panel_regression.html#time-fixed-effects",
    "href": "teaching/FIN 525/lectures/lecture18_robust_panel_regression.html#time-fixed-effects",
    "title": "L18: Robust panel regression",
    "section": "Time fixed effects",
    "text": "Time fixed effects\n\nresults_timefe = PanelOLS(dependent = comp[yvar], \n                          exog = comp[xvars], \n                          time_effects = True\n                         ).fit();\nprint(results_timefe.summary)\n\n                          PanelOLS Estimation Summary                           \n================================================================================\nDep. Variable:           w_future_roa   R-squared:                        0.0944\nEstimator:                   PanelOLS   R-squared (Between):              0.1033\nNo. Observations:              185315   R-squared (Within):              -0.0645\nDate:                Fri, Feb 25 2022   R-squared (Overall):              0.0955\nTime:                        14:36:54   Log-likelihood                    6386.9\nCov. Estimator:            Unadjusted                                           \n                                        F-statistic:                      6436.1\nEntities:                       19139   P-value                           0.0000\nAvg Obs:                       9.6826   Distribution:                F(3,185271)\nMin Obs:                       1.0000                                           \nMax Obs:                       80.000   F-statistic (robust):             6436.1\n                                        P-value                           0.0000\nTime periods:                      41   Distribution:                F(3,185271)\nAvg Obs:                       4519.9                                           \nMin Obs:                       4.0000                                           \nMax Obs:                       6276.0                                           \n                                                                                \n                              Parameter Estimates                               \n================================================================================\n              Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n--------------------------------------------------------------------------------\nconst            0.0308     0.0010     31.563     0.0000      0.0289      0.0327\nw_cash          -0.3710     0.0028    -133.37     0.0000     -0.3765     -0.3656\nw_leverage      -0.0694     0.0030    -22.855     0.0000     -0.0754     -0.0634\nw_investment     0.1677     0.0064     26.382     0.0000      0.1553      0.1802\n================================================================================\n\nF-test for Poolability: 44.613\nP-value: 0.0000\nDistribution: F(40,185271)\n\nIncluded effects: Time\n\n\nOnce again, the P-value for the F-test for Poolability is very small, which means we should also keep the time fixed effects in our regression. Combined with the previous result, this means we should be including both firm and time fixed effects, which is what we do below.\nNote also how the coefficient on w_cash has changed sign again."
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture18_robust_panel_regression.html#both-time-and-year-fixed-effects",
    "href": "teaching/FIN 525/lectures/lecture18_robust_panel_regression.html#both-time-and-year-fixed-effects",
    "title": "L18: Robust panel regression",
    "section": "Both time and year fixed effects:",
    "text": "Both time and year fixed effects:\n\nresults_bothfe = PanelOLS(dependent = comp[yvar], \n                          exog = comp[xvars], \n                          entity_effects = True, time_effects = True,\n                         ).fit();\nprint(results_bothfe.summary)\n\n                          PanelOLS Estimation Summary                           \n================================================================================\nDep. Variable:           w_future_roa   R-squared:                        0.0029\nEstimator:                   PanelOLS   R-squared (Between):             -0.0734\nNo. Observations:              185315   R-squared (Within):               0.0035\nDate:                Fri, Feb 25 2022   R-squared (Overall):             -0.0097\nTime:                        14:36:55   Log-likelihood                 8.184e+04\nCov. Estimator:            Unadjusted                                           \n                                        F-statistic:                      160.89\nEntities:                       19139   P-value                           0.0000\nAvg Obs:                       9.6826   Distribution:                F(3,166133)\nMin Obs:                       1.0000                                           \nMax Obs:                       80.000   F-statistic (robust):             160.89\n                                        P-value                           0.0000\nTime periods:                      41   Distribution:                F(3,166133)\nAvg Obs:                       4519.9                                           \nMin Obs:                       4.0000                                           \nMax Obs:                       6276.0                                           \n                                                                                \n                              Parameter Estimates                               \n================================================================================\n              Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n--------------------------------------------------------------------------------\nconst           -0.0358     0.0010    -34.326     0.0000     -0.0379     -0.0338\nw_cash           0.0162     0.0038     4.2602     0.0000      0.0087      0.0236\nw_leverage      -0.0499     0.0036    -13.775     0.0000     -0.0570     -0.0428\nw_investment     0.0813     0.0050     16.210     0.0000      0.0715      0.0912\n================================================================================\n\nF-test for Poolability: 11.083\nP-value: 0.0000\nDistribution: F(19178,166133)\n\nIncluded effects: Entity, Time\n\n\nIn this final specification, it seems like cash holdings are positively associated with future profitability."
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture18_robust_panel_regression.html#sector-fixed-effects",
    "href": "teaching/FIN 525/lectures/lecture18_robust_panel_regression.html#sector-fixed-effects",
    "title": "L18: Robust panel regression",
    "section": "Sector fixed effects",
    "text": "Sector fixed effects\n\ncomp['sic2d'] = comp['sich'].astype('string').str[0:2]\ncomp['sic2d'].value_counts()\n\n73    18965\n28    16678\n36    13361\n60    11046\n38    10881\n      ...  \n76       86\n81       34\n86       11\n90       11\n89        8\nName: sic2d, Length: 69, dtype: Int64\n\n\nNote that ‘sic2d’ contains missing values, which will give us an error if we try to use them as fixed-effects. So we get rid of all missing values in our regression data, and store this in a new dataframe first:\n\ndf = comp[main_vars + ['sic2d']].dropna()\n\nNow we can run our industry fixed-effects regression:\n\nresults_indfe = PanelOLS(dependent = df[yvar], \n                          exog = df[xvars], \n                          other_effects = df['sic2d']\n                         ).fit();\nprint(results_indfe.summary)\n\n                          PanelOLS Estimation Summary                           \n================================================================================\nDep. Variable:           w_future_roa   R-squared:                        0.0571\nEstimator:                   PanelOLS   R-squared (Between):              0.0906\nNo. Observations:              153676   R-squared (Within):              -0.0440\nDate:                Fri, Feb 25 2022   R-squared (Overall):              0.1012\nTime:                        14:36:56   Log-likelihood                    1745.7\nCov. Estimator:            Unadjusted                                           \n                                        F-statistic:                      3102.9\nEntities:                       23812   P-value                           0.0000\nAvg Obs:                       6.4537   Distribution:                F(3,153604)\nMin Obs:                       0.0000                                           \nMax Obs:                       67.000   F-statistic (robust):             3102.9\n                                        P-value                           0.0000\nTime periods:                      41   Distribution:                F(3,153604)\nAvg Obs:                       3748.2                                           \nMin Obs:                       0.0000                                           \nMax Obs:                       5811.0                                           \n                                                                                \n                              Parameter Estimates                               \n================================================================================\n              Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n--------------------------------------------------------------------------------\nconst            0.0158     0.0012     13.445     0.0000      0.0135      0.0181\nw_cash          -0.3090     0.0034    -91.633     0.0000     -0.3156     -0.3024\nw_leverage      -0.0573     0.0036    -15.832     0.0000     -0.0644     -0.0502\nw_investment     0.1727     0.0074     23.452     0.0000      0.1583      0.1871\n================================================================================\n\nF-test for Poolability: 85.603\nP-value: 0.0000\nDistribution: F(68,153604)\n\nIncluded effects: Other Effect (sic2d)\nModel includes 5 other effects\nOther Effect Observations per group (sic2d):\nAvg Obs: 2227.2, Min Obs: 4.0000, Max Obs: 1.44e+04, Groups: 69"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture18_robust_panel_regression.html#white-standard-errors",
    "href": "teaching/FIN 525/lectures/lecture18_robust_panel_regression.html#white-standard-errors",
    "title": "L18: Robust panel regression",
    "section": "White standard errors",
    "text": "White standard errors\n\nresults_white = model.fit(cov_type = 'robust');\nprint(results_white.summary)\n\n                          PanelOLS Estimation Summary                           \n================================================================================\nDep. Variable:           w_future_roa   R-squared:                        0.0029\nEstimator:                   PanelOLS   R-squared (Between):             -0.0734\nNo. Observations:              185315   R-squared (Within):               0.0035\nDate:                Fri, Feb 25 2022   R-squared (Overall):             -0.0097\nTime:                        14:36:58   Log-likelihood                 8.184e+04\nCov. Estimator:                Robust                                           \n                                        F-statistic:                      160.89\nEntities:                       19139   P-value                           0.0000\nAvg Obs:                       9.6826   Distribution:                F(3,166133)\nMin Obs:                       1.0000                                           \nMax Obs:                       80.000   F-statistic (robust):             73.502\n                                        P-value                           0.0000\nTime periods:                      41   Distribution:                F(3,166133)\nAvg Obs:                       4519.9                                           \nMin Obs:                       4.0000                                           \nMax Obs:                       6276.0                                           \n                                                                                \n                              Parameter Estimates                               \n================================================================================\n              Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n--------------------------------------------------------------------------------\nconst           -0.0358     0.0016    -22.864     0.0000     -0.0389     -0.0328\nw_cash           0.0162     0.0062     2.5922     0.0095      0.0039      0.0284\nw_leverage      -0.0499     0.0051    -9.7033     0.0000     -0.0599     -0.0398\nw_investment     0.0813     0.0074     11.032     0.0000      0.0669      0.0958\n================================================================================\n\nF-test for Poolability: 11.083\nP-value: 0.0000\nDistribution: F(19178,166133)\n\nIncluded effects: Entity, Time"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture18_robust_panel_regression.html#clustering-standard-errors-at-the-firm-level",
    "href": "teaching/FIN 525/lectures/lecture18_robust_panel_regression.html#clustering-standard-errors-at-the-firm-level",
    "title": "L18: Robust panel regression",
    "section": "Clustering standard errors at the firm level",
    "text": "Clustering standard errors at the firm level\n\nresults_firm_cluster = model.fit(cov_type = 'clustered', cluster_entity = True);\nprint(results_firm_cluster.summary)\n\n                          PanelOLS Estimation Summary                           \n================================================================================\nDep. Variable:           w_future_roa   R-squared:                        0.0029\nEstimator:                   PanelOLS   R-squared (Between):             -0.0734\nNo. Observations:              185315   R-squared (Within):               0.0035\nDate:                Fri, Feb 25 2022   R-squared (Overall):             -0.0097\nTime:                        14:37:00   Log-likelihood                 8.184e+04\nCov. Estimator:             Clustered                                           \n                                        F-statistic:                      160.89\nEntities:                       19139   P-value                           0.0000\nAvg Obs:                       9.6826   Distribution:                F(3,166133)\nMin Obs:                       1.0000                                           \nMax Obs:                       80.000   F-statistic (robust):             47.063\n                                        P-value                           0.0000\nTime periods:                      41   Distribution:                F(3,166133)\nAvg Obs:                       4519.9                                           \nMin Obs:                       4.0000                                           \nMax Obs:                       6276.0                                           \n                                                                                \n                              Parameter Estimates                               \n================================================================================\n              Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n--------------------------------------------------------------------------------\nconst           -0.0358     0.0021    -17.288     0.0000     -0.0399     -0.0318\nw_cash           0.0162     0.0089     1.8152     0.0695     -0.0013      0.0336\nw_leverage      -0.0499     0.0070    -7.1564     0.0000     -0.0635     -0.0362\nw_investment     0.0813     0.0083     9.8217     0.0000      0.0651      0.0976\n================================================================================\n\nF-test for Poolability: 11.083\nP-value: 0.0000\nDistribution: F(19178,166133)\n\nIncluded effects: Entity, Time\n\n\nNote how the cash holding variable (which, last lecture, we thought has the highest predictive power over future profitability), is no longer statistically significant at the 95% confidence level."
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture18_robust_panel_regression.html#clustering-standard-errors-at-the-year-level",
    "href": "teaching/FIN 525/lectures/lecture18_robust_panel_regression.html#clustering-standard-errors-at-the-year-level",
    "title": "L18: Robust panel regression",
    "section": "Clustering standard errors at the year level",
    "text": "Clustering standard errors at the year level\n\nresults_time_cluster = model.fit(cov_type = 'clustered', cluster_time = True);\nprint(results_time_cluster.summary)\n\n                          PanelOLS Estimation Summary                           \n================================================================================\nDep. Variable:           w_future_roa   R-squared:                        0.0029\nEstimator:                   PanelOLS   R-squared (Between):             -0.0734\nNo. Observations:              185315   R-squared (Within):               0.0035\nDate:                Fri, Feb 25 2022   R-squared (Overall):             -0.0097\nTime:                        14:37:01   Log-likelihood                 8.184e+04\nCov. Estimator:             Clustered                                           \n                                        F-statistic:                      160.89\nEntities:                       19139   P-value                           0.0000\nAvg Obs:                       9.6826   Distribution:                F(3,166133)\nMin Obs:                       1.0000                                           \nMax Obs:                       80.000   F-statistic (robust):             33.385\n                                        P-value                           0.0000\nTime periods:                      41   Distribution:                F(3,166133)\nAvg Obs:                       4519.9                                           \nMin Obs:                       4.0000                                           \nMax Obs:                       6276.0                                           \n                                                                                \n                              Parameter Estimates                               \n================================================================================\n              Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n--------------------------------------------------------------------------------\nconst           -0.0358     0.0023    -15.705     0.0000     -0.0403     -0.0314\nw_cash           0.0162     0.0093     1.7413     0.0816     -0.0020      0.0343\nw_leverage      -0.0499     0.0070    -7.1542     0.0000     -0.0635     -0.0362\nw_investment     0.0813     0.0112     7.2616     0.0000      0.0594      0.1033\n================================================================================\n\nF-test for Poolability: 11.083\nP-value: 0.0000\nDistribution: F(19178,166133)\n\nIncluded effects: Entity, Time"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture18_robust_panel_regression.html#cluster-at-both-the-firm-and-year-level",
    "href": "teaching/FIN 525/lectures/lecture18_robust_panel_regression.html#cluster-at-both-the-firm-and-year-level",
    "title": "L18: Robust panel regression",
    "section": "Cluster at both the firm and year level",
    "text": "Cluster at both the firm and year level\n\nresults_both_cluster = model.fit(cov_type = 'clustered', cluster_entity = True, cluster_time = True);\nprint(results_both_cluster.summary)\n\n                          PanelOLS Estimation Summary                           \n================================================================================\nDep. Variable:           w_future_roa   R-squared:                        0.0029\nEstimator:                   PanelOLS   R-squared (Between):             -0.0734\nNo. Observations:              185315   R-squared (Within):               0.0035\nDate:                Fri, Feb 25 2022   R-squared (Overall):             -0.0097\nTime:                        14:37:03   Log-likelihood                 8.184e+04\nCov. Estimator:             Clustered                                           \n                                        F-statistic:                      160.89\nEntities:                       19139   P-value                           0.0000\nAvg Obs:                       9.6826   Distribution:                F(3,166133)\nMin Obs:                       1.0000                                           \nMax Obs:                       80.000   F-statistic (robust):             25.269\n                                        P-value                           0.0000\nTime periods:                      41   Distribution:                F(3,166133)\nAvg Obs:                       4519.9                                           \nMin Obs:                       4.0000                                           \nMax Obs:                       6276.0                                           \n                                                                                \n                              Parameter Estimates                               \n================================================================================\n              Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n--------------------------------------------------------------------------------\nconst           -0.0358     0.0027    -13.498     0.0000     -0.0410     -0.0306\nw_cash           0.0162     0.0112     1.4367     0.1508     -0.0059      0.0382\nw_leverage      -0.0499     0.0084    -5.9289     0.0000     -0.0663     -0.0334\nw_investment     0.0813     0.0118     6.8815     0.0000      0.0582      0.1045\n================================================================================\n\nF-test for Poolability: 11.083\nP-value: 0.0000\nDistribution: F(19178,166133)\n\nIncluded effects: Entity, Time\n\n\nThis is probably the specification that I would choose going forward since it accounts for both firm and time fixed effects and then adjusts for any remaining correlation in residuals along both of those dimensions."
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture18_robust_panel_regression.html#clustering-standard-errors-at-the-sector-level",
    "href": "teaching/FIN 525/lectures/lecture18_robust_panel_regression.html#clustering-standard-errors-at-the-sector-level",
    "title": "L18: Robust panel regression",
    "section": "Clustering standard errors at the sector level",
    "text": "Clustering standard errors at the sector level\nWe can cluster standard errors along other dimensions of correlation as well. Below we cluster at the industry level.\n\nresults_ind_cluster = model.fit(cov_type = 'clustered', clusters = comp['sic2d']);\nprint(results_ind_cluster.summary)\n\n                          PanelOLS Estimation Summary                           \n================================================================================\nDep. Variable:           w_future_roa   R-squared:                        0.0029\nEstimator:                   PanelOLS   R-squared (Between):             -0.0734\nNo. Observations:              185315   R-squared (Within):               0.0035\nDate:                Fri, Feb 25 2022   R-squared (Overall):             -0.0097\nTime:                        14:37:06   Log-likelihood                 8.184e+04\nCov. Estimator:             Clustered                                           \n                                        F-statistic:                      160.89\nEntities:                       19139   P-value                           0.0000\nAvg Obs:                       9.6826   Distribution:                F(3,166133)\nMin Obs:                       1.0000                                           \nMax Obs:                       80.000   F-statistic (robust):             41.229\n                                        P-value                           0.0000\nTime periods:                      41   Distribution:                F(3,166133)\nAvg Obs:                       4519.9                                           \nMin Obs:                       4.0000                                           \nMax Obs:                       6276.0                                           \n                                                                                \n                              Parameter Estimates                               \n================================================================================\n              Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n--------------------------------------------------------------------------------\nconst           -0.0358     0.0065    -5.4738     0.0000     -0.0487     -0.0230\nw_cash           0.0162     0.0277     0.5834     0.5596     -0.0381      0.0704\nw_leverage      -0.0499     0.0114    -4.3621     0.0000     -0.0723     -0.0275\nw_investment     0.0813     0.0089     9.1502     0.0000      0.0639      0.0987\n================================================================================\n\nF-test for Poolability: 11.083\nP-value: 0.0000\nDistribution: F(19178,166133)\n\nIncluded effects: Entity, Time\n\n\nNow the cash holdings variable is not even significant at the 45% confidence level. The different steps we went through in this lecture to make sure our results are trustworthy, show that the results of our analysis can change quite drastically once we take those steps: using a simple regression specification in the last lecture, it seemed like the cash holdings variable was the strongest predictor of future profitability (with a negative coefficient). Now, we see that in reality, the cash holdings variable is the only one that is NOT statistically significant in our regression."
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture23_riskadjusted_ret.html",
    "href": "teaching/FIN 525/lectures/lecture23_riskadjusted_ret.html",
    "title": "L23: Backtesting - risk adjustment",
    "section": "",
    "text": "# Import packages\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport pandas_datareader as pdr"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture23_riskadjusted_ret.html#equal-weighted-ew-portfolios",
    "href": "teaching/FIN 525/lectures/lecture23_riskadjusted_ret.html#equal-weighted-ew-portfolios",
    "title": "L23: Backtesting - risk adjustment",
    "section": "Equal-weighted (EW) portfolios",
    "text": "Equal-weighted (EW) portfolios\n\n# Load EW return data data\new_ret = pd.read_pickle('../data/AG_ew_returns.zip')\new_ret = ew_ret[:'2003-06'].copy()\new_ret\n\n\n\n\n\n\n\nportf_nr\n1.0\n2.0\n3.0\n4.0\n5.0\n6.0\n7.0\n8.0\n9.0\n10.0\nSpread\n\n\nmdate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1982-07\n-0.003948\n-0.011054\n0.002117\n-0.009741\n-0.009275\n-0.003169\n-0.004816\n-0.006250\n-0.034018\n-0.071587\n0.067639\n\n\n1982-08\n0.043106\n0.058108\n0.067768\n0.071293\n0.070272\n0.076378\n0.080404\n0.078438\n0.076014\n0.069051\n-0.025945\n\n\n1982-09\n0.037267\n0.034962\n0.034323\n0.037597\n0.034146\n0.047353\n0.034485\n0.030422\n0.009593\n-0.020953\n0.058220\n\n\n1982-10\n0.154922\n0.122708\n0.106485\n0.107380\n0.119641\n0.122316\n0.112235\n0.135088\n0.142025\n0.161400\n-0.006478\n\n\n1982-11\n0.116354\n0.115989\n0.096393\n0.089470\n0.069214\n0.090190\n0.091515\n0.082948\n0.097205\n0.050597\n0.065757\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2003-02\n-0.019734\n0.001625\n-0.026126\n-0.034833\n-0.045058\n-0.039061\n-0.032196\n-0.026197\n-0.034444\n-0.034851\n0.015116\n\n\n2003-03\n0.055259\n0.031098\n0.013882\n0.006659\n0.009049\n0.012072\n0.008408\n0.014281\n0.005784\n0.009986\n0.045273\n\n\n2003-04\n0.148508\n0.140660\n0.123251\n0.112512\n0.095413\n0.092639\n0.085307\n0.109632\n0.085932\n0.127671\n0.020836\n\n\n2003-05\n0.315904\n0.202694\n0.196376\n0.132280\n0.140624\n0.109417\n0.114861\n0.129849\n0.140532\n0.171611\n0.144293\n\n\n2003-06\n0.135771\n0.099213\n0.051757\n0.031807\n0.047800\n0.054163\n0.039503\n0.040638\n0.038876\n0.056958\n0.078813\n\n\n\n\n252 rows × 11 columns\n\n\n\n\n# Calculate average EW returns\new_means = ew_ret.mean()\new_means\n\nportf_nr\n1.0       0.022429\n2.0       0.019655\n3.0       0.017927\n4.0       0.015500\n5.0       0.014881\n6.0       0.014966\n7.0       0.013576\n8.0       0.012631\n9.0       0.008677\n10.0      0.001652\nSpread    0.020777\ndtype: float64\n\n\n\n# Plot returns of Spread portfolio\new_ret['Spread'].plot();\n\n\n\n\n\n# Calculate t-statistics for each portfolio returns (H0: average portfolio return is 0)\new_tstats = ew_means / (ew_ret.std() / ew_ret.count()**0.5)\new_tstats\n\nportf_nr\n1.0       3.760509\n2.0       4.443625\n3.0       5.114522\n4.0       5.082020\n5.0       4.975837\n6.0       4.765637\n7.0       4.130154\n8.0       3.400974\n9.0       2.047683\n10.0      0.325883\nSpread    7.035854\ndtype: float64\n\n\n\n# Plot cummulative return of spread portfolio\n(1 + ew_ret)['Spread'].cumprod().plot();\n\n\n\n\n\n# Plot cummulative return of portfolios 1 and 10\n(1+ew_ret)[1].cumprod().plot();\n(1+ew_ret)[10].cumprod().plot();"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture23_riskadjusted_ret.html#value-weighted-vw-portfolios",
    "href": "teaching/FIN 525/lectures/lecture23_riskadjusted_ret.html#value-weighted-vw-portfolios",
    "title": "L23: Backtesting - risk adjustment",
    "section": "Value-weighted (VW) portfolios",
    "text": "Value-weighted (VW) portfolios\n\n# Load VW return data data\nvw_ret = pd.read_pickle('../data/AG_vw_returns.zip')\nvw_ret = vw_ret[:'2003-06'].copy()\nvw_ret\n\n\n\n\n\n\n\nportf_nr\n1.0\n2.0\n3.0\n4.0\n5.0\n6.0\n7.0\n8.0\n9.0\n10.0\nSpread\n\n\nmdate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1982-07\n-0.016496\n-0.032506\n-0.008927\n-0.020168\n-0.008148\n-0.014216\n-0.016144\n-0.026691\n-0.052065\n-0.063027\n0.046531\n\n\n1982-08\n0.112879\n0.126619\n0.133671\n0.111961\n0.122147\n0.108101\n0.125463\n0.129065\n0.111937\n0.149069\n-0.036190\n\n\n1982-09\n0.035342\n-0.011639\n0.009310\n0.021978\n0.013155\n0.008232\n0.015515\n0.021661\n0.006081\n-0.006955\n0.042297\n\n\n1982-10\n0.180624\n0.147793\n0.118260\n0.096087\n0.082554\n0.109715\n0.099294\n0.108276\n0.163611\n0.183572\n-0.002948\n\n\n1982-11\n0.075535\n0.093501\n0.050342\n0.041856\n0.039193\n0.049904\n0.040065\n0.032300\n0.074103\n0.070121\n0.005414\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2003-02\n0.013984\n-0.000944\n-0.012868\n-0.007327\n-0.035330\n-0.029244\n-0.015190\n-0.005070\n-0.002535\n-0.009106\n0.023090\n\n\n2003-03\n0.039739\n0.007937\n-0.010142\n-0.014830\n0.008708\n0.016905\n-0.013148\n0.030607\n0.045923\n0.003247\n0.036492\n\n\n2003-04\n0.136213\n0.078133\n0.103938\n0.091120\n0.065227\n0.060584\n0.080776\n0.078677\n0.054610\n0.089784\n0.046430\n\n\n2003-05\n0.202974\n0.166204\n0.109430\n0.086531\n0.074955\n0.047940\n0.108823\n0.013190\n0.051810\n0.101467\n0.101507\n\n\n2003-06\n0.024044\n0.023553\n-0.024031\n0.005133\n0.010631\n0.001055\n0.025350\n0.038687\n0.004150\n0.023575\n0.000469\n\n\n\n\n252 rows × 11 columns\n\n\n\n\n# Calculate average EW returns\nvw_means = vw_ret.mean()\nvw_means\n\nportf_nr\n1.0       0.014354\n2.0       0.013263\n3.0       0.013213\n4.0       0.013390\n5.0       0.011904\n6.0       0.012645\n7.0       0.013343\n8.0       0.012511\n9.0       0.010194\n10.0      0.006706\nSpread    0.007648\ndtype: float64\n\n\n\n# Plot returns of Spread portfolio\nvw_ret['Spread'].plot();\n\n\n\n\n\n# Calculate t-statistics for each portfolio returns (H0: average portfolio return is 0)\nvw_tstats = vw_means / (vw_ret.std() / vw_ret.count()**0.5)\nvw_tstats\n\nportf_nr\n1.0       3.473671\n2.0       3.927155\n3.0       4.702159\n4.0       5.177343\n5.0       4.574888\n6.0       4.439000\n7.0       4.407662\n8.0       3.497476\n9.0       2.501369\n10.0      1.531934\nSpread    2.654032\ndtype: float64\n\n\n\n# Plot cummulative return of spread portfolio\n(1 + vw_ret)['Spread'].cumprod().plot();"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture23_riskadjusted_ret.html#equal-weighted-portfolios",
    "href": "teaching/FIN 525/lectures/lecture23_riskadjusted_ret.html#equal-weighted-portfolios",
    "title": "L23: Backtesting - risk adjustment",
    "section": "Equal-weighted portfolios",
    "text": "Equal-weighted portfolios\n\n# Merge EW monthly portfolio returns with the risk factors\nalldata = ew_ret.join(ff3f)\nalldata\n\n\n\n\n\n\n\n\n1.0\n2.0\n3.0\n4.0\n5.0\n6.0\n7.0\n8.0\n9.0\n10.0\nSpread\nMkt-RF\nSMB\nHML\nRF\n\n\nmdate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1982-07\n-0.003948\n-0.011054\n0.002117\n-0.009741\n-0.009275\n-0.003169\n-0.004816\n-0.006250\n-0.034018\n-0.071587\n0.067639\n-0.0319\n0.0083\n0.0009\n0.0105\n\n\n1982-08\n0.043106\n0.058108\n0.067768\n0.071293\n0.070272\n0.076378\n0.080404\n0.078438\n0.076014\n0.069051\n-0.025945\n0.1114\n-0.0414\n0.0095\n0.0076\n\n\n1982-09\n0.037267\n0.034962\n0.034323\n0.037597\n0.034146\n0.047353\n0.034485\n0.030422\n0.009593\n-0.020953\n0.058220\n0.0129\n0.0295\n0.0028\n0.0051\n\n\n1982-10\n0.154922\n0.122708\n0.106485\n0.107380\n0.119641\n0.122316\n0.112235\n0.135088\n0.142025\n0.161400\n-0.006478\n0.1130\n0.0234\n-0.0366\n0.0059\n\n\n1982-11\n0.116354\n0.115989\n0.096393\n0.089470\n0.069214\n0.090190\n0.091515\n0.082948\n0.097205\n0.050597\n0.065757\n0.0467\n0.0467\n-0.0187\n0.0063\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2003-02\n-0.019734\n0.001625\n-0.026126\n-0.034833\n-0.045058\n-0.039061\n-0.032196\n-0.026197\n-0.034444\n-0.034851\n0.015116\n-0.0188\n-0.0045\n-0.0138\n0.0009\n\n\n2003-03\n0.055259\n0.031098\n0.013882\n0.006659\n0.009049\n0.012072\n0.008408\n0.014281\n0.005784\n0.009986\n0.045273\n0.0109\n0.0103\n-0.0191\n0.0010\n\n\n2003-04\n0.148508\n0.140660\n0.123251\n0.112512\n0.095413\n0.092639\n0.085307\n0.109632\n0.085932\n0.127671\n0.020836\n0.0822\n0.0066\n0.0117\n0.0010\n\n\n2003-05\n0.315904\n0.202694\n0.196376\n0.132280\n0.140624\n0.109417\n0.114861\n0.129849\n0.140532\n0.171611\n0.144293\n0.0605\n0.0474\n0.0050\n0.0009\n\n\n2003-06\n0.135771\n0.099213\n0.051757\n0.031807\n0.047800\n0.054163\n0.039503\n0.040638\n0.038876\n0.056958\n0.078813\n0.0142\n0.0177\n0.0015\n0.0010\n\n\n\n\n252 rows × 15 columns\n\n\n\nCycle through all portfolios and regress excess returns on risk factors\n\n# First, create empty tables to store portfolio alphas and their tstats\new_portf_coeff = pd.DataFrame(np.nan, index = ['const', 'Mkt-RF','SMB', 'HML'], columns = ew_ret.columns)\new_portf_coeff\n\n\n\n\n\n\n\nportf_nr\n1.0\n2.0\n3.0\n4.0\n5.0\n6.0\n7.0\n8.0\n9.0\n10.0\nSpread\n\n\n\n\nconst\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nMkt-RF\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nSMB\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nHML\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\new_portf_tstats = ew_portf_coeff.copy()\new_portf_tstats\n\n\n\n\n\n\n\nportf_nr\n1.0\n2.0\n3.0\n4.0\n5.0\n6.0\n7.0\n8.0\n9.0\n10.0\nSpread\n\n\n\n\nconst\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nMkt-RF\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nSMB\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nHML\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n# Regressions for each portfolio\nfor p in ew_ret.columns:\n    #Set up the data\n        # Dependent variable is excess return on the portfolio\n    y = alldata[p] - alldata['RF']\n        # Except for the spread portfolio (which is alread an excess return)\n    if p == 'Spread':\n        y = alldata[p] \n        \n        # Independent variables are the risk factors\n    X = alldata[['Mkt-RF','SMB','HML']].copy()\n    X['const'] = 1\n    \n    # Run the regression\n    res = sm.OLS(y, X[['const','Mkt-RF','SMB','HML']], missing = 'drop').fit()\n    # Adjust for autocorrelation in residuals\n    res_robust = res.get_robustcov_results(cov_type = 'HAC', maxlags = 6)\n    \n    # Store the results\n    ew_portf_coeff.loc[:,p] = res_robust.params \n    ew_portf_tstats.loc[:,p] = res_robust.tvalues \n\nprint(res_robust.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 Spread   R-squared:                       0.197\nModel:                            OLS   Adj. R-squared:                  0.187\nMethod:                 Least Squares   F-statistic:                     7.715\nDate:                Wed, 26 Apr 2023   Prob (F-statistic):           6.01e-05\nTime:                        07:21:58   Log-Likelihood:                 441.74\nNo. Observations:                 252   AIC:                            -875.5\nDf Residuals:                     248   BIC:                            -861.4\nDf Model:                           3                                         \nCovariance Type:                  HAC                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0214      0.004      6.008      0.000       0.014       0.028\nMkt-RF        -0.1747      0.062     -2.812      0.005      -0.297      -0.052\nSMB            0.6059      0.174      3.474      0.001       0.262       0.949\nHML            0.1271      0.154      0.825      0.410      -0.176       0.431\n==============================================================================\nOmnibus:                       53.275   Durbin-Watson:                   1.728\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              143.375\nSkew:                           0.936   Prob(JB):                     7.35e-32\nKurtosis:                       6.185   Cond. No.                         45.2\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity and autocorrelation robust (HAC) using 6 lags and without small sample correction\n\n\nTake a look at the results\n\nprint(\"\\n Portfolio alphas and factor loadings:\\n\")\new_portf_coeff\n\n\n Portfolio alphas and factor loadings:\n\n\n\n\n\n\n\n\n\nportf_nr\n1.0\n2.0\n3.0\n4.0\n5.0\n6.0\n7.0\n8.0\n9.0\n10.0\nSpread\n\n\n\n\nconst\n0.009559\n0.006966\n0.004510\n0.002879\n0.002097\n0.002345\n0.000996\n-0.000359\n-0.004288\n-0.011835\n0.021394\n\n\nMkt-RF\n1.062260\n0.961886\n0.958992\n0.891349\n0.905635\n0.927476\n0.953022\n1.034324\n1.105890\n1.236997\n-0.174737\n\n\nSMB\n1.563695\n1.179390\n0.836426\n0.688619\n0.666368\n0.702386\n0.720628\n0.828599\n0.876110\n0.957753\n0.605943\n\n\nHML\n0.073249\n0.223328\n0.419966\n0.353247\n0.368794\n0.289732\n0.234368\n0.185963\n0.053055\n-0.053822\n0.127071\n\n\n\n\n\n\n\n\nprint(\"\\n T-statistics:\\n\")\new_portf_tstats\n\n\n T-statistics:\n\n\n\n\n\n\n\n\n\nportf_nr\n1.0\n2.0\n3.0\n4.0\n5.0\n6.0\n7.0\n8.0\n9.0\n10.0\nSpread\n\n\n\n\nconst\n2.619291\n3.055189\n3.110097\n2.709521\n2.296160\n2.428906\n1.200616\n-0.338180\n-2.958751\n-5.159747\n6.007953\n\n\nMkt-RF\n11.486691\n15.931878\n22.489307\n26.761871\n40.593130\n39.773473\n43.670124\n37.855299\n30.527413\n20.069321\n-2.812017\n\n\nSMB\n8.961525\n10.707320\n7.599961\n8.404802\n9.502792\n12.176715\n8.882976\n9.840857\n6.720156\n5.728980\n3.473969\n\n\nHML\n0.318961\n1.472370\n4.116269\n5.334134\n6.354676\n5.332194\n4.049948\n3.267815\n0.562896\n-0.352292\n0.824591"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture23_riskadjusted_ret.html#value-weighted-portfolios",
    "href": "teaching/FIN 525/lectures/lecture23_riskadjusted_ret.html#value-weighted-portfolios",
    "title": "L23: Backtesting - risk adjustment",
    "section": "Value-weighted portfolios",
    "text": "Value-weighted portfolios\n\n# Merge VW monthly portfolio returns with the risk factors\nalldata = vw_ret.join(ff3f)\nalldata\n\n\n\n\n\n\n\n\n1.0\n2.0\n3.0\n4.0\n5.0\n6.0\n7.0\n8.0\n9.0\n10.0\nSpread\nMkt-RF\nSMB\nHML\nRF\n\n\nmdate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1982-07\n-0.016496\n-0.032506\n-0.008927\n-0.020168\n-0.008148\n-0.014216\n-0.016144\n-0.026691\n-0.052065\n-0.063027\n0.046531\n-0.0319\n0.0083\n0.0009\n0.0105\n\n\n1982-08\n0.112879\n0.126619\n0.133671\n0.111961\n0.122147\n0.108101\n0.125463\n0.129065\n0.111937\n0.149069\n-0.036190\n0.1114\n-0.0414\n0.0095\n0.0076\n\n\n1982-09\n0.035342\n-0.011639\n0.009310\n0.021978\n0.013155\n0.008232\n0.015515\n0.021661\n0.006081\n-0.006955\n0.042297\n0.0129\n0.0295\n0.0028\n0.0051\n\n\n1982-10\n0.180624\n0.147793\n0.118260\n0.096087\n0.082554\n0.109715\n0.099294\n0.108276\n0.163611\n0.183572\n-0.002948\n0.1130\n0.0234\n-0.0366\n0.0059\n\n\n1982-11\n0.075535\n0.093501\n0.050342\n0.041856\n0.039193\n0.049904\n0.040065\n0.032300\n0.074103\n0.070121\n0.005414\n0.0467\n0.0467\n-0.0187\n0.0063\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2003-02\n0.013984\n-0.000944\n-0.012868\n-0.007327\n-0.035330\n-0.029244\n-0.015190\n-0.005070\n-0.002535\n-0.009106\n0.023090\n-0.0188\n-0.0045\n-0.0138\n0.0009\n\n\n2003-03\n0.039739\n0.007937\n-0.010142\n-0.014830\n0.008708\n0.016905\n-0.013148\n0.030607\n0.045923\n0.003247\n0.036492\n0.0109\n0.0103\n-0.0191\n0.0010\n\n\n2003-04\n0.136213\n0.078133\n0.103938\n0.091120\n0.065227\n0.060584\n0.080776\n0.078677\n0.054610\n0.089784\n0.046430\n0.0822\n0.0066\n0.0117\n0.0010\n\n\n2003-05\n0.202974\n0.166204\n0.109430\n0.086531\n0.074955\n0.047940\n0.108823\n0.013190\n0.051810\n0.101467\n0.101507\n0.0605\n0.0474\n0.0050\n0.0009\n\n\n2003-06\n0.024044\n0.023553\n-0.024031\n0.005133\n0.010631\n0.001055\n0.025350\n0.038687\n0.004150\n0.023575\n0.000469\n0.0142\n0.0177\n0.0015\n0.0010\n\n\n\n\n252 rows × 15 columns\n\n\n\nCycle through all portfolios and regress excess returns on risk factors\n\n# First, create empty tables to store portfolio alphas and their tstats\nvw_portf_coeff = pd.DataFrame(np.nan, index = ['const', 'Mkt-RF','SMB', 'HML'], columns = vw_ret.columns)\nvw_portf_coeff\n\n\n\n\n\n\n\nportf_nr\n1.0\n2.0\n3.0\n4.0\n5.0\n6.0\n7.0\n8.0\n9.0\n10.0\nSpread\n\n\n\n\nconst\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nMkt-RF\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nSMB\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nHML\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\nvw_portf_tstats = vw_portf_coeff.copy()\nvw_portf_tstats\n\n\n\n\n\n\n\nportf_nr\n1.0\n2.0\n3.0\n4.0\n5.0\n6.0\n7.0\n8.0\n9.0\n10.0\nSpread\n\n\n\n\nconst\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nMkt-RF\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nSMB\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nHML\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n# Regressions for each portfolio\nfor p in vw_ret.columns:\n    #Set up the data\n        # Dependent variable is excess return on the portfolio\n    y = alldata[p] - alldata['RF']\n        # Except for the spread portfolio (which is alread an excess return)\n    if p == 'Spread':\n        y = alldata[p] \n        \n        # Independent variables are the risk factors\n    X = alldata[['Mkt-RF','SMB','HML']].copy()\n    X['const'] = 1\n    \n    # Run the regression\n    res = sm.OLS(y, X[['const','Mkt-RF','SMB','HML']], missing='drop').fit()\n    res_robust = res.get_robustcov_results(cov_type = 'HAC', maxlags = 6)\n    \n    # Store the results\n    vw_portf_coeff.loc[:,p] = res_robust.params \n    vw_portf_tstats.loc[:,p] = res_robust.tvalues \n\nprint(res_robust.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 Spread   R-squared:                       0.227\nModel:                            OLS   Adj. R-squared:                  0.217\nMethod:                 Least Squares   F-statistic:                     15.12\nDate:                Wed, 26 Apr 2023   Prob (F-statistic):           4.53e-09\nTime:                        07:23:08   Log-Likelihood:                 452.67\nNo. Observations:                 252   AIC:                            -897.3\nDf Residuals:                     248   BIC:                            -883.2\nDf Model:                           3                                         \nCovariance Type:                  HAC                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0054      0.003      1.817      0.070      -0.000       0.011\nMkt-RF        -0.0781      0.080     -0.974      0.331      -0.236       0.080\nSMB            0.4752      0.149      3.181      0.002       0.181       0.769\nHML            0.6611      0.122      5.439      0.000       0.422       0.901\n==============================================================================\nOmnibus:                       40.358   Durbin-Watson:                   1.891\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               95.899\nSkew:                           0.749   Prob(JB):                     1.50e-21\nKurtosis:                       5.625   Cond. No.                         45.2\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity and autocorrelation robust (HAC) using 6 lags and without small sample correction\n\n\nTake a look at the results\n\nprint(\"\\n Portfolio alphas and factor loadings:\\n\")\nvw_portf_coeff\n\n\n Portfolio alphas and factor loadings:\n\n\n\n\n\n\n\n\n\nportf_nr\n1.0\n2.0\n3.0\n4.0\n5.0\n6.0\n7.0\n8.0\n9.0\n10.0\nSpread\n\n\n\n\nconst\n0.000290\n-0.000428\n0.000185\n0.001408\n0.000363\n0.000519\n0.001905\n0.002063\n0.000197\n-0.005083\n0.005374\n\n\nMkt-RF\n1.137942\n1.079517\n0.961564\n0.883103\n0.880913\n0.950440\n0.976145\n1.006447\n1.066098\n1.216012\n-0.078070\n\n\nSMB\n0.737353\n0.169837\n0.042593\n-0.075015\n-0.049715\n-0.136981\n-0.102068\n0.076246\n0.065937\n0.262185\n0.475168\n\n\nHML\n0.268259\n0.307791\n0.359662\n0.250077\n0.146542\n0.170523\n-0.041658\n-0.341530\n-0.553634\n-0.392884\n0.661143\n\n\n\n\n\n\n\n\nprint(\"\\n T-statistics:\\n\")\nvw_portf_tstats\n\n\n T-statistics:\n\n\n\n\n\n\n\n\n\nportf_nr\n1.0\n2.0\n3.0\n4.0\n5.0\n6.0\n7.0\n8.0\n9.0\n10.0\nSpread\n\n\n\n\nconst\n0.138785\n-0.247346\n0.181093\n1.489228\n0.424545\n0.551499\n2.117366\n1.610279\n0.185540\n-2.927644\n1.816907\n\n\nMkt-RF\n18.169942\n24.861527\n26.683369\n26.409845\n30.260815\n28.649934\n36.383952\n31.229472\n22.818687\n26.260559\n-0.974375\n\n\nSMB\n7.450553\n1.837578\n0.807182\n-2.127936\n-1.136439\n-4.092208\n-2.521685\n1.509005\n1.186412\n3.443714\n3.180851\n\n\nHML\n3.423134\n2.726291\n5.445035\n4.055598\n3.168255\n1.787967\n-0.805742\n-7.028478\n-7.056546\n-5.343526\n5.439341"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture01_Introduction_and_Setup.html",
    "href": "teaching/FIN 525/lectures/lecture01_Introduction_and_Setup.html",
    "title": "L01: Introduction",
    "section": "",
    "text": "Topics covered in this course\n\nThe fundamentals of programming in Python\n\nVariables and basic data types (int, float, str, bool)\nData structures (lists, tuples, dictionaries)\nConditional statements (“if/else” else)\nIteration (“for” loops)\nFunctions (“def” statement)\nThe “dot” notation (everything in Python is an “object”)\nInterpreting error messages (almost everything in an error message is useless)\nFinding answers online (documentation files, Google is your friend, stackoverflow)\n\nData processing\n\nData Input/Output (IO)\nData cleaning\nDates and lags\nMerging datasets\n\nDescriptive statistics and data visualization\n\nUnconditional (full-sample) statistics and visualization\nConditional (subsample) statistics and visualization\n\nLinear Regression\n\nFundamentals of linear regression\nRobust linear regression in panel and time-series datasets\n\nBacktesting investment strategies (academic paper replication)\n\nCollecting and cleaning the data\nCalculating portfolio returns\nAnalyzing portfolio performance\n\n\nNote that these topics are subject to change, based on how the tempo of the class is going. You will be notified when changes are made.\n\n\nAnaconda, Jupyter, and VS Code installation\nTo install Python, we will actually install a different program, called Anaconda, which comes with Python and many other tools frequently used in Data Science. Anaconda takes a lot more space than Python itself, but the benefit is that it comes with everything you need for this course (and likely any other course you will ever take on Machine Learning or Data Science). Once you install Anaconda, install Jupyter Notebook and VS Code following the instructions below.\n\nInstalling Anaconda\n\nGo to https://www.anaconda.com/products/individual\nClick “Download” and then select the “64-Bit Graphical Installer” for your operating system\n\nI am assuming you all have 64-bit systems\nIf you have a 32-bit Windows system, you also have the option to download a 32-bit Anaconda installer\n\nFollow the instructions on the screen\n\nFor additional instructions on installation, see the “Resources” section below\nWhen in doubt, use the default settings suggested by the Anaconda installer\n\n\nTo open Anaconda:\n\nWindows\n\nFrom the Start menu, click the Anaconda Navigator desktop app\n\nmacOS\n\nOpen Launchpad, then click the Anaconda Navigator icon\n\n\nFor all of our classes, we will use Jupyter Notebooks, which you can install from inside Anaconda like this:\n\n\nInstalling Jupyter Notebook\n\nOn Anaconda Navigator’s Home tab, in the Applications pane on the right, scroll to the Jupyter Notebook (not JupyterLab) tile and click the Install button to install Jupyter Notebook (it might be already installed, in which case, the button will say “Launch” instead of “Install”).\n\n\n\nInstalling VS Code\nThis is an IDE (integrated development environment) that can run Jupyter notebooks too. I will be using it during class, but you do not have to (as explained in the following section). I strongly recommend you install it though, since it comes with some very useful functionality when writing code (e.g. autocompletion, integrated terminal etc).\nTo install VS Code:\n\nGo to https://code.visualstudio.com/download\n\nOn Windows, click on the “User Installer” “x64” tile under “Windows”\nOn Mac, click on “.zip” “Apple Silicon” if you have an M1 or M2 Mac, or “Intel Chip” if you have an older Mac.\n\nFollow the installer’s instructions after the download is complete\n\n\n\n\nHow to open the lecture notes\nIn D2L, click on the “Lectures and Data” tab. You will see a set of subfolders inside that tab. Please replicate that exact subfolder structure somewhere on your computer (say inside a folder called FIN525). Save all the contents of the “data” folder in your “data” folder. Before each class, save the contents of the “lectureXX” folder for that class into the “lectureXX” folder on your computer where XX stands the lecture number corresponding to that class.\nThere are (at least) three ways you can open a particular lecture (a Jupyter notebook) on your computer:\n\nMETHOD 1 (recommended):\n\nOpen VS Code\nClick “Open Folder” one the welcome page or under the “File” menu\nNavigate to the folder that contains your lecture notes and hit “Enter”\nThis folder should show up on the right side of the screen. Find the lecture notes you want to open and double click them.\n\nMETHOD 2:\n\nOpen Anaconda Navigator, find the Jupyter Notebook tile in the pane on the right, and hit “Launch”. This will open a web browser showing a directory structure on your computer. Use that to navigate to the lecture you want to open and click on it.\n\nMETHOD 3:\n\nNavigate to the folder containing the lecture you want to open on your computer. Copy the full path to that folder:\n\non Windows: double-click your folder and then click the navigation bar at the top of the window\non macOS: see “Resources” below\n\nOpen a terminal like this:\n\non Windows: from the Start menu, search for and open “Anaconda Prompt”\non macOS: open Launchpad, then click the Terminal icon\n\nIn the terminal, type “cd” (without the quotes) and then paste the path to your lectures folder you copied above. Hit enter.\nIn the same terminal, type “jupyter notebook” (without the quotes) and hit enter.\nThis process should have opened a browser showing all the file in your lectures folder. Open a lecture by just clicking on it in this web browser.\n\n\n\n\nInstalling Python packages\nAnaconda will allow you to easily install new Python packages (pieces of code written by someone else). There are two methods to do this (though depending on the package, only one of them might work):\n\nMETHOD 1: using “conda”\n\nOpen a new terminal as described above\nType “conda install” (without quotes) followed by the name(s) of the Python package(s) you want to install. Hit enter.\n\nMETHOD 2: using “pip”\n\nOpen a new terminal as described above\nType “pip install” (without quotes) followed by the name(s) of the Python package(s) you want to install. Hit enter. I recommend using “pip” only if “conda” gives you an error.\n\n\nExample:\nA package that we will use almost daily in class, is called “yfinance” and it allows us to download data from Yahoo Finance with just one Python command.\nTo install “yfinance”, open a new terminal and type:\npip install yfinance\nThe reason why we use “pip install” here, rather than “conda install” is because the latter results in some errors for some users and it seems like the pip installation is more stable.\nOther packages we will use frequently in class:\n\npandas\nnumpy\nmatplotlib\n\nThey all come pre-installed with Anaconda.\n\n\nDownloading data from WRDS (OPTIONAL)\nWharton Research Data Services (WRDS) is one of the the most widely used database service in academic finance research. It provides access to many different kinds of data, but for this class, we will only use CRSP and Compustat data:\n\nCRSP contains stock market data (stock prices, returns, volume, etc.). This is the “crspm.zip” file in the “data” folder on D2L.\nCompustat contains accounting data (all data from the three main financial statements). This is the “compa.zip” file in the “data” folder on D2L.\n\nI have created a class account for you with WRDS. Please use the information in the Syllabus to log in.\nThe download instructions below are only here for your reference in case you need to download these datasets later on (e.g. for your summer projects). However, for the purpose of this class, I wanted to make sure everyone will work with the same data (and get the same results) so I downloaded the two datasets mentioned above and uploaded them on D2L. They are under the “Lectures and Data” tab, in the “data” folder. The CRSP monthly file is called “crspm” and the Compustat annual file is called “compa”. I also downloaded the Fama-French risk factor file and called it “ff” on D2L. Please download that as well.\n\nDownloading monthly stock-market data from CRSP:\n\nHome → CRSP → Stock/Security File → Monthly Stock File\nSTEP 1: Select start & end dates for your dataset\nSTEP 2: Select PERMNO and “Search the entire database”\nSTEP 3: Click “Select all” to request all variables, or click on just the ones you want\nSTEP 4: Leave everything as is, though, I would choose “gzip” for compression to speed things along (you can extract gzip files on Windows with the “7zip” software available for free on the internet)\n\n\n\nDownloading annual accounting data from Compustat:\n\nHome → CRSP → CRSP/Compustat Merged → Fundamentals Annual\nSTEP 1: Select start & end dates for your dataset\nSTEP 2: Select PERMNO and “Search the entire database”. Leave “Screening Variables” unchanged\nSTEP 3: Leave unchanged\nSTEP 4: Click “Select all” to request all variables, or click on just the ones you want. Leave “Conditional Statements” unchanged.\nSTEP 5: Again, leave unchanged if you don’t mind waiting. Use gzip if you know how to extract compressed files.\n\n\n\n\nFor next class\n\nMake sure you have Anaconda, Jupyter Notebook, and VS Code installed and running on your computer before class.\nCreate a folder on your computer which contains the same subfolder structure you see in D2L under “Lectures and Data” (as explained in the “How to open the lecture notes” section above.)\nOpen a Terminal, type the following command and hit enter:\n\n    pip install yfinance pandas-datareader statsmodels linearmodels\n\nOpen a Terminal, type the following command and hit enter:\n\n    conda install -y openpyxl xlrd\n\n\nResources\n\nAnaconda installation instructions:\n\nhttps://docs.anaconda.com/anaconda/install/\n\nGetting started with Anaconda:\n\nhttps://docs.anaconda.com/anaconda/user-guide/getting-started/\n\nInstalling packages:\n\nhttps://docs.anaconda.com/anaconda/user-guide/tasks/install-packages/\n\nCopy folder path on macOS:\n\nhttps://www.switchingtomac.com/tutorials/osx/5-ways-to-reveal-the-path-of-a-file-on-macos/\n\nWRDS login page\n\nhttps://wrds-www.wharton.upenn.edu/"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture15_regression_intro1.html",
    "href": "teaching/FIN 525/lectures/lecture15_regression_intro1.html",
    "title": "L15: Linear regression intro",
    "section": "",
    "text": "# Import packages\nimport pandas as pd\nimport yfinance as yf\nimport pandas_datareader as pdr\n\nLoad Fama-French factor data:\n\nff3f = pdr.DataReader('F-F_Research_Data_Factors', 'famafrench', '2012-01-01')[0]/100\nff3f.head(2)\n\n\n\n\n\n\n\n\nMkt-RF\nSMB\nHML\nRF\n\n\nDate\n\n\n\n\n\n\n\n\n2012-01\n0.0505\n0.0203\n-0.0097\n0.0\n\n\n2012-02\n0.0442\n-0.0185\n0.0043\n0.0\n\n\n\n\n\n\n\nDownload monthly prices (keep only Adjusted Close prices):\n\nfirm_prices = yf.download('TSLA', '2012-12-01', '2020-12-31', interval = '1mo')['Adj Close'].dropna().to_frame()\nfirm_prices.head(2)\n\n[*********************100%***********************]  1 of 1 completed\n\n\n\n\n\n\n\n\n\nAdj Close\n\n\nDate\n\n\n\n\n\n2012-12-01\n2.258000\n\n\n2013-01-01\n2.500667\n\n\n\n\n\n\n\nCalculate monthly returns, drop missing and rename “Adj Close” to “TSLA”:\n\nfirm_ret = firm_prices.pct_change().dropna().rename(columns = {'Adj Close': 'TSLA'})\nfirm_ret.head(2)\n\n\n\n\n\n\n\n\nTSLA\n\n\nDate\n\n\n\n\n\n2013-01-01\n0.107470\n\n\n2013-02-01\n-0.071448\n\n\n\n\n\n\n\nWe need to merge firm_ret with ff3f but note that their dates look different. Check their format first:\n\nfirm_ret.index.dtype\n\ndtype('&lt;M8[ns]')\n\n\n\nff3f.index.dtype\n\nperiod[M]\n\n\nConvert index of firm_ret to monthly period, to match the date format un ff3f:\n\nfirm_ret.index = firm_ret.index.to_period('M')\nfirm_ret.head(2)\n\n\n\n\n\n\n\n\nTSLA\n\n\nDate\n\n\n\n\n\n2013-01\n0.107470\n\n\n2013-02\n-0.071448\n\n\n\n\n\n\n\nMerge the two datasets:\n\ndata = firm_ret.join(ff3f)\ndata.head(2)\n\n\n\n\n\n\n\n\nTSLA\nMkt-RF\nSMB\nHML\nRF\n\n\nDate\n\n\n\n\n\n\n\n\n\n2013-01\n0.107470\n0.0557\n0.0033\n0.0096\n0.0\n\n\n2013-02\n-0.071448\n0.0129\n-0.0028\n0.0011\n0.0\n\n\n\n\n\n\n\n\ndata['const'] = 1\ndata\n\n\n\n\n\n\n\n\nTSLA\nMkt-RF\nSMB\nHML\nRF\nconst\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n2013-01\n0.107470\n0.0557\n0.0033\n0.0096\n0.0000\n1\n\n\n2013-02\n-0.071448\n0.0129\n-0.0028\n0.0011\n0.0000\n1\n\n\n2013-03\n0.087855\n0.0403\n0.0081\n-0.0019\n0.0000\n1\n\n\n2013-04\n0.424914\n0.0155\n-0.0236\n0.0045\n0.0000\n1\n\n\n2013-05\n0.810706\n0.0280\n0.0173\n0.0263\n0.0000\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n2020-08\n0.741452\n0.0763\n-0.0022\n-0.0296\n0.0001\n1\n\n\n2020-09\n-0.139087\n-0.0363\n0.0004\n-0.0268\n0.0001\n1\n\n\n2020-10\n-0.095499\n-0.0210\n0.0437\n0.0422\n0.0001\n1\n\n\n2020-11\n0.462736\n0.1247\n0.0581\n0.0213\n0.0001\n1\n\n\n2020-12\n0.243252\n0.0463\n0.0489\n-0.0150\n0.0001\n1\n\n\n\n\n96 rows × 6 columns"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture15_regression_intro1.html#a1.-linearity",
    "href": "teaching/FIN 525/lectures/lecture15_regression_intro1.html#a1.-linearity",
    "title": "L15: Linear regression intro",
    "section": "A1. Linearity",
    "text": "A1. Linearity\nThe relation between the variables is assumed to be linear in parameters:\n\\[Y_t = \\alpha + \\beta \\cdot X_t + \\epsilon_t \\]\nNote that “linear in parameters” means the function that describes the relation between X and Y (the equation above) is linear with respect to \\(\\alpha\\) and \\(\\beta\\) (e.g. \\(Y = \\alpha \\cdot X^{\\beta} + \\epsilon\\) is not linear in parameters). It does not mean that the relation needs to be linear with respect to X (e.g. \\(Y = \\alpha + \\beta \\cdot X^2 + \\epsilon\\) is still linear in parameters).\nBefore we cover the remaining assumptions, a bit of terminology:\n\nY is commonly referred to as the “dependent”, or “explained” or “endogenous” variable\nX is commonly referred to as the “independent”, or “explanatory”, or “exogenous” variable (though, remember, that X can stand for more than one variables)\n\\(\\epsilon\\) is commonly referred to as the “residual” of the regression, or the “error” term, or the “disturbance” term\n\\(\\alpha\\) (alpha) and \\(\\beta\\) (beta) are the “coefficients” or “parameters” of the regression. The outcome of “running a regression” is to calculate estimates for these alpha and beta coefficients.\nthe t subscript is meant to represent the fact that we observe multiple realizations of the X and Y variables, and the linear relation is assumed to hold for each set of realizations (different t’s can represent different points in time, or different firms, different countries, etc). Going forward in this section, we will assume that t stands for time, to make the interpretation clearer."
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture15_regression_intro1.html#a2.-mean-independence",
    "href": "teaching/FIN 525/lectures/lecture15_regression_intro1.html#a2.-mean-independence",
    "title": "L15: Linear regression intro",
    "section": "A2. Mean independence",
    "text": "A2. Mean independence\nThis assumption states that the independent variable(s) X convey no information about the disturbance terms (\\(\\epsilon\\)’s). Technically, we write this assumption as:\n\\[E[\\epsilon_t | X] = 0\\]\nThis is also called the “strict exogeneity” assumption. When this condition is not satisfied, we say that our regression model has an endogeneity problem."
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture15_regression_intro1.html#a3.-homoskedastic-and-uncorrelated-disturbances",
    "href": "teaching/FIN 525/lectures/lecture15_regression_intro1.html#a3.-homoskedastic-and-uncorrelated-disturbances",
    "title": "L15: Linear regression intro",
    "section": "A3. Homoskedastic and uncorrelated disturbances",
    "text": "A3. Homoskedastic and uncorrelated disturbances\nThis assumption states that all disturbance terms have the same variance (i.e. they are “homoskedastic”):\n\\[ Var[\\epsilon_t | X] = \\sigma^2                 \\text{  , for all observations $t$} \\]\nWhen this assumption is not satisfied, we say we have a heteroskedasticity problem.\nThe standard linear regression model also assumes that any two disturbance terms are uncorrelated with each other:\n\\[ Cov [\\epsilon_{t_1}, \\epsilon_{t_2}] = 0 \\text{ , for all $t_1 \\neq t_2$}\\]"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture15_regression_intro1.html#a4.-full-rank",
    "href": "teaching/FIN 525/lectures/lecture15_regression_intro1.html#a4.-full-rank",
    "title": "L15: Linear regression intro",
    "section": "A4. Full rank",
    "text": "A4. Full rank\nThis assumption states that there are no exact linear relationships between the explanatory variables X (when there are two or more such variables). When this assumption is not satisfied, we say we have a multicollinearity problem.\nIn the next few lectures, we will cover strategies that we can use when some of the above assumptions are not satisfied."
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture15_regression_intro1.html#example-1-estimating-a-stocks-alpha-and-beta-using-the-market-model",
    "href": "teaching/FIN 525/lectures/lecture15_regression_intro1.html#example-1-estimating-a-stocks-alpha-and-beta-using-the-market-model",
    "title": "L15: Linear regression intro",
    "section": "Example 1: estimating a stock’s alpha and beta using the market model",
    "text": "Example 1: estimating a stock’s alpha and beta using the market model\nThe market model (aka the “single-factor model” or the “single-index model”) is a linear regression model that relates the excess return on a stock to the excess returns on the market portfolio:\n\\[R_{i,t} - R_{f,t} = \\alpha_i + \\beta_i (R_{m,t} - R_{f,t}) + \\epsilon_{i,t}\\]\nwhere: - \\(R_{i,t}\\) is the return of firm \\(i\\) at time \\(t\\) - \\(R_{m,t}\\) is the return of the market at time \\(t\\) (we generally use the S&P500 index as the market portfolio) - \\(R_{f,t}\\) is the risk-free rate at time \\(t\\) (most commonly the yield on the 1-month Tbill)\nBelow, we estimate this model for TSLA, using the data we gathered at the top of these lecture notes:\nTo “run” (i.e. “fit” or “estimate”) the regression, we use the .fit() function which can be applied after the sm.OLS() function. We store results in “res”:\n\nres = sm.OLS(endog = data['TSLA']-data['RF'], \n             exog = data[['const','Mkt-RF']], \n             missing='drop'\n            ).fit()\nres\n\n&lt;statsmodels.regression.linear_model.RegressionResultsWrapper at 0x7fb7bb37f310&gt;\n\n\nThe above shows that res is a “RegressionResultsWrapper”. We have not seen this kind of an object before. Check all the attributes of the results (res) object:\n\nprint(dir(res))\n\n['HC0_se', 'HC1_se', 'HC2_se', 'HC3_se', '_HCCM', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_abat_diagonal', '_cache', '_data_attr', '_data_in_cache', '_get_robustcov_results', '_is_nested', '_use_t', '_wexog_singular_values', 'aic', 'bic', 'bse', 'centered_tss', 'compare_f_test', 'compare_lm_test', 'compare_lr_test', 'condition_number', 'conf_int', 'conf_int_el', 'cov_HC0', 'cov_HC1', 'cov_HC2', 'cov_HC3', 'cov_kwds', 'cov_params', 'cov_type', 'df_model', 'df_resid', 'eigenvals', 'el_test', 'ess', 'f_pvalue', 'f_test', 'fittedvalues', 'fvalue', 'get_influence', 'get_prediction', 'get_robustcov_results', 'info_criteria', 'initialize', 'k_constant', 'llf', 'load', 'model', 'mse_model', 'mse_resid', 'mse_total', 'nobs', 'normalized_cov_params', 'outlier_test', 'params', 'predict', 'pvalues', 'remove_data', 'resid', 'resid_pearson', 'rsquared', 'rsquared_adj', 'save', 'scale', 'ssr', 'summary', 'summary2', 't_test', 't_test_pairwise', 'tvalues', 'uncentered_tss', 'use_t', 'wald_test', 'wald_test_terms', 'wresid']\n\n\nParticularly important results are stored in summary(), params, pvalues, tvalues, rsquared. We’ll cover all of these below. As the name suggests, the summary() attribute contains a summary of the regression results:\n\nprint(res.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.155\nModel:                            OLS   Adj. R-squared:                  0.146\nMethod:                 Least Squares   F-statistic:                     17.26\nDate:                Wed, 22 Mar 2023   Prob (F-statistic):           7.18e-05\nTime:                        07:32:31   Log-Likelihood:                 30.144\nNo. Observations:                  96   AIC:                            -56.29\nDf Residuals:                      94   BIC:                            -51.16\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0416      0.019      2.186      0.031       0.004       0.079\nMkt-RF         1.8297      0.440      4.154      0.000       0.955       2.704\n==============================================================================\nOmnibus:                       29.750   Durbin-Watson:                   1.561\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               56.488\nSkew:                           1.227   Prob(JB):                     5.42e-13\nKurtosis:                       5.846   Cond. No.                         24.2\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture15_regression_intro1.html#example-2-estimating-a-stocks-alpha-and-betas-using-the-fama-french-three-factor-model",
    "href": "teaching/FIN 525/lectures/lecture15_regression_intro1.html#example-2-estimating-a-stocks-alpha-and-betas-using-the-fama-french-three-factor-model",
    "title": "L15: Linear regression intro",
    "section": "Example 2: estimating a stock’s alpha and beta(s) using the Fama-French three-factor model",
    "text": "Example 2: estimating a stock’s alpha and beta(s) using the Fama-French three-factor model\nThe Fama-French three factor model is a linear regression model that relates the excess return on a stock to the excess returns on the market portfolio and the returns on the SMB (small minus big) and HML (high minus low book-to-market) factors:\n\\[R_{i,t} - R_{f,t} = \\alpha_i + \\beta_{i,m} (R_{m,t} - R_{f,t}) + \\beta_{i,smb}  R_{smb} + \\beta_{i,hml} R_{hml} + \\epsilon_{i,t}\\]\nChallenge:\nEstimate this regression for TSLA using the data we gathered at the top of these lecture notes.\n\n# Run the regression and print the results\nres3 = sm.OLS(endog = data['TSLA']-data['RF'], \n             exog = data[['const','Mkt-RF','SMB','HML']], \n             missing='drop'\n            ).fit()\nres3\nprint(res3.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.164\nModel:                            OLS   Adj. R-squared:                  0.137\nMethod:                 Least Squares   F-statistic:                     6.036\nDate:                Wed, 22 Mar 2023   Prob (F-statistic):           0.000847\nTime:                        07:32:31   Log-Likelihood:                 30.677\nNo. Observations:                  96   AIC:                            -53.35\nDf Residuals:                      92   BIC:                            -43.10\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0371      0.020      1.872      0.064      -0.002       0.076\nMkt-RF         1.8810      0.480      3.915      0.000       0.927       2.835\nSMB            0.1931      0.794      0.243      0.808      -1.384       1.770\nHML           -0.6717      0.667     -1.007      0.317      -1.997       0.653\n==============================================================================\nOmnibus:                       31.346   Durbin-Watson:                   1.587\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               62.467\nSkew:                           1.268   Prob(JB):                     2.73e-14\nKurtosis:                       6.030   Cond. No.                         45.0\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture15_regression_intro1.html#coefficients",
    "href": "teaching/FIN 525/lectures/lecture15_regression_intro1.html#coefficients",
    "title": "L15: Linear regression intro",
    "section": "Coefficients",
    "text": "Coefficients\nThe “const” row contains information about the firm’s \\(\\alpha\\) and the “Mkt-RF” row contains information about the firm’s \\(\\beta\\). The \\(\\alpha\\) and \\(\\beta\\) coefficient estimates themselves are in the “coef” column (\\(\\alpha = 0.0416\\), and \\(\\beta = 1.83\\) in the single-factor model).\nThe “const” coefficient tells us what we should expect the return on TSLA to be on a day with no systematic shocks (i.e. a day with market return of 0).\nThe ‘Mkt-RF’ coefficient tells us how we should expect the return on TSLA to react to a given shock to the market portfolio (e.g. the 1.83 coefficient tells us that, on average, if the market goes up by 1%, TSLA goes up by 1.83%, and when the market goes down by 1%, TSLA goes down by 1.83%)\nThe results object (res) stores the regression coefficients in its params attribute:\n\nres.params\n\nconst     0.041642\nMkt-RF    1.829738\ndtype: float64\n\n\nNote that res.params is a Pandas Series, so we can access its individual elements using the index labels:\n\nprint('Alpha = ', res.params['const'])\nprint('Market Beta = ', res.params['Mkt-RF'])\n\nAlpha =  0.041642456171143115\nMarket Beta =  1.8297380496913744\n\n\nChallenge:\nPrint out (separately) the alpha and each of the betas from the three-factor model\n\nprint(\"three-factor alpha:\", res3.params['const'])\nprint(\"three-factor market beta:\", res3.params['Mkt-RF'])\nprint(\"SMB beta:\", res3.params['SMB'])\nprint(\"HML beta:\", res3.params['HML'])\n\nthree-factor alpha: 0.037089953287553816\nthree-factor market beta: 1.881000087482544\nSMB beta: 0.19308158602997738\nHML beta: -0.6717493189696122"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture15_regression_intro1.html#statistical-significance",
    "href": "teaching/FIN 525/lectures/lecture15_regression_intro1.html#statistical-significance",
    "title": "L15: Linear regression intro",
    "section": "Statistical significance",
    "text": "Statistical significance\nThe p-values are in the “P &gt; |t|” column. P-values lower than 0.05 allow us to conclude that the corresponding coefficient is statistically different from 0 at the 95% confidence level (i.e. reject the null hypothesis that the coefficient is 0). At the 99% confidence level, we would need the p-value to be smaller than 1% (1 minus the confidence level) to reject the null hypothesis that alpha = 0.\nThe t-statistics for the two coefficients are in the “t” column. Loosely speaking a t-statistic that larger than 2 or smaller than -2 allows us to conclude that the corresponding coefficient is statistically different from 0 at the 95% confidence level (i.e. reject the null hypothesis that the coefficient is 0). In terms of statistical significance, the t-statistic does not provide any new information over the p-value.\nThe last two columns give us the 95% confidence interval for each coefficient.\nFor the market model, TSLA’s alpha has a p-value of 0.031 so we can conclude that it’s alpha is statistically significantly different from 0 at the 95% confidence level (but not at the 99% confidence level).\nThe fact that the alpha is positive and statistically different from 0 (at 95% level) means that, based on the single-factor model, TSLA seems to be undervalued. A negative alpha would mean the stock in overvalued.\nIf we can not reject the null hypothesis that alpha is 0, the conclusion is NOT that alpha = 0 and therefore the stock is correctly valued (since we can never “accept” a null hypothesis, we can only fail to reject). The conclusion is that we do not have enough evidence to claim that the stock is either undervalued or overvalued (which is not the same thing as saying that we have enough evidence to claim that the stock is correctly valued).\nThe results object (res) stores the regression p-values in its pvalues attribute:\n\nres.pvalues\n\nconst     0.031312\nMkt-RF    0.000072\ndtype: float64\n\n\nThe p-values can be accessed individually:\n\nprint(\"Alpha p-value = \", res.pvalues['const'])\nprint(\"Beta p-value = \", res.pvalues['Mkt-RF'])\n\nAlpha p-value =  0.031311923663591416\nBeta p-value =  7.182372372221883e-05\n\n\nT-statistics are stored in the tvalues attribute:\n\nres.tvalues\n\nconst     2.185832\nMkt-RF    4.154374\ndtype: float64\n\n\nT-statistics of individual coefficients:\n\nprint(\"Alpha t-stat = \", res.tvalues['const'])\nprint(\"Beta t-stat = \", res.tvalues['Mkt-RF'])\n\nAlpha t-stat =  2.1858319462947398\nBeta t-stat =  4.154374466130978\n\n\nChallenge:\nIs TSLA mispriced (undervalued OR overvalued) at the 5% significance level with respect to the Fama-French 3-factor model?\n\nprint(\"TSLA mispriced? \\n\", res3.pvalues['const'] &lt; 0.05)\n\nTSLA mispriced? \n False\n\n\nChallenge:\nDoes TSLA have a significant exposure (at 5% level) to either of the 3 factors in the Fama-French model?\n\nprint('Significant market exposure?\\n', res3.pvalues['Mkt-RF'] &lt; 0.05)\nprint('Significant exposure to SMB?\\n', res3.pvalues['SMB'] &lt; 0.05)\nprint('Significant exposure to HML?\\n', res3.pvalues['HML'] &lt; 0.05)\n\nSignificant market exposure?\n True\nSignificant exposure to SMB?\n False\nSignificant exposure to HML?\n False"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture15_regression_intro1.html#the-r-squared-coefficient",
    "href": "teaching/FIN 525/lectures/lecture15_regression_intro1.html#the-r-squared-coefficient",
    "title": "L15: Linear regression intro",
    "section": "The R-squared coefficient",
    "text": "The R-squared coefficient\nThe R-squared coefficient (top-right of the table, also referred to as the “coefficient of determination”) estimates the percentage of the total variance in the dependent variable (Y) that can be explained by the variance in the explanatory variable(s) (X).\nIn the context of our market-model example, the R-squared tells us the percentage of the firm’s total variance that is systematic in nature (i.e. non-diversifiable). The percentage of total variance that is idiosyncratic (diversifiable) equals 1 minus the R-squared.\nThe R-squared is stored in the rsquared attribute of the regression results object:\n\nres.rsquared\n\n0.1551232170825012\n\n\nUsing the market model, our estimates of the percentage or total TSLA variance that is systematic vs idiosyncratic are:\n\nprint(\"Percent of total variance that is systematic: \", res.rsquared)\nprint(\"Percent of total variance that is idiosyncratic: \", 1 - res.rsquared)\n\nPercent of total variance that is systematic:  0.1551232170825012\nPercent of total variance that is idiosyncratic:  0.8448767829174988\n\n\nChallenge:\nWhat percentage of TSLA total variance can be diversified away under the Fama-French 3-factor model?\n\nprint(\"Percent of total variance that is systematic: \", res3.rsquared)\nprint(\"Percent of total variance that is idiosyncratic: \", 1 - res3.rsquared)\n\nPercent of total variance that is systematic:  0.16445635788368973\nPercent of total variance that is idiosyncratic:  0.8355436421163103"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture15_regression_intro1.html#diagnostics-bottom-of-the-table",
    "href": "teaching/FIN 525/lectures/lecture15_regression_intro1.html#diagnostics-bottom-of-the-table",
    "title": "L15: Linear regression intro",
    "section": "Diagnostics (bottom of the table):",
    "text": "Diagnostics (bottom of the table):\nThe regression table reports (at the bottom) a few statistics that help us understand if some of the assumption of the linear regression model are not satisfied.\n\nDurbin-Watson: tests for residual autocorrelation. Takes values in [0,4]. Below 2 means positive autocorr. Above 2 means negative aoutocorr. A value of 2 is perfect (means no autocorrelation).\nOmnibus: tests normality of residuals. Prob(Omnibus) close to 0 means reject normality\nJB: another normality test (null is skew=0, kurt=3). Prob(JB) close to 0 means rejection of normality\nCond. No: tests for multicollinearity. Over 100 is worrisome, but we still have to look at correlations between variables to determine if any of them need to be dropped."
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture11_Dates_Lags_Sorting.html",
    "href": "teaching/FIN 525/lectures/lecture11_Dates_Lags_Sorting.html",
    "title": "L11: Dates, lags, sorting",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\ndf = pd.read_excel('./rawdata12.xlsx')\ndf\n\n\n\n\n\n\n\n\nfirmid\ndate\nreturn\n\n\n\n\n0\n1\n09/30/2010\n0.050\n\n\n1\n2\n10/31/2010\n0.450\n\n\n2\n3\n11/30/2010\n23.000\n\n\n3\n1\n11/30/2010\n0.870\n\n\n4\n2\n11/30/2010\n0.200\n\n\n5\n3\n12/31/2010\n0.340\n\n\n6\n1\n12/31/2010\n0.060\n\n\n7\n2\n12/31/2010\n0.001\n\n\n8\n3\n12/31/2010\n-0.120"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture11_Dates_Lags_Sorting.html#sort_values",
    "href": "teaching/FIN 525/lectures/lecture11_Dates_Lags_Sorting.html#sort_values",
    "title": "L11: Dates, lags, sorting",
    "section": ".sort_values()",
    "text": ".sort_values()\nSyntax:\nDataFrame.sort_values(by, axis=0, ascending=True, inplace=False)\n\ndf.sort_values('firmid')\n\n\n\n\n\n\n\n\nfirmid\ndate\nreturn\n\n\n\n\n0\n1\n09/30/2010\n0.050\n\n\n3\n1\n11/30/2010\n0.870\n\n\n6\n1\n12/31/2010\n0.060\n\n\n1\n2\n10/31/2010\n0.450\n\n\n4\n2\n11/30/2010\n0.200\n\n\n7\n2\n12/31/2010\n0.001\n\n\n2\n3\n11/30/2010\n23.000\n\n\n5\n3\n12/31/2010\n0.340\n\n\n8\n3\n12/31/2010\n-0.120\n\n\n\n\n\n\n\n\ndf.sort_values('firmid', ascending = False)\n\n\n\n\n\n\n\n\nfirmid\ndate\nreturn\n\n\n\n\n2\n3\n11/30/2010\n23.000\n\n\n5\n3\n12/31/2010\n0.340\n\n\n8\n3\n12/31/2010\n-0.120\n\n\n1\n2\n10/31/2010\n0.450\n\n\n4\n2\n11/30/2010\n0.200\n\n\n7\n2\n12/31/2010\n0.001\n\n\n0\n1\n09/30/2010\n0.050\n\n\n3\n1\n11/30/2010\n0.870\n\n\n6\n1\n12/31/2010\n0.060\n\n\n\n\n\n\n\nRemember, functions that have an inplace parameter do not actually change the original dataset unless we set that parameter to True:\n\ndf\n\n\n\n\n\n\n\n\nfirmid\ndate\nreturn\n\n\n\n\n0\n1\n09/30/2010\n0.050\n\n\n1\n2\n10/31/2010\n0.450\n\n\n2\n3\n11/30/2010\n23.000\n\n\n3\n1\n11/30/2010\n0.870\n\n\n4\n2\n11/30/2010\n0.200\n\n\n5\n3\n12/31/2010\n0.340\n\n\n6\n1\n12/31/2010\n0.060\n\n\n7\n2\n12/31/2010\n0.001\n\n\n8\n3\n12/31/2010\n-0.120\n\n\n\n\n\n\n\n\ndf.sort_values('firmid', inplace = True)\ndf\n\n\n\n\n\n\n\n\nfirmid\ndate\nreturn\n\n\n\n\n0\n1\n09/30/2010\n0.050\n\n\n3\n1\n11/30/2010\n0.870\n\n\n6\n1\n12/31/2010\n0.060\n\n\n1\n2\n10/31/2010\n0.450\n\n\n4\n2\n11/30/2010\n0.200\n\n\n7\n2\n12/31/2010\n0.001\n\n\n2\n3\n11/30/2010\n23.000\n\n\n5\n3\n12/31/2010\n0.340\n\n\n8\n3\n12/31/2010\n-0.120"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture11_Dates_Lags_Sorting.html#sort_index",
    "href": "teaching/FIN 525/lectures/lecture11_Dates_Lags_Sorting.html#sort_index",
    "title": "L11: Dates, lags, sorting",
    "section": ".sort_index()",
    "text": ".sort_index()\nAbbreviated syntax:\nDataFrame.sort_index(axis=0, level=None, ascending=True, inplace=False)\n\ndf.sort_index()\n\n\n\n\n\n\n\n\nfirmid\ndate\nreturn\n\n\n\n\n0\n1\n09/30/2010\n0.050\n\n\n1\n2\n10/31/2010\n0.450\n\n\n2\n3\n11/30/2010\n23.000\n\n\n3\n1\n11/30/2010\n0.870\n\n\n4\n2\n11/30/2010\n0.200\n\n\n5\n3\n12/31/2010\n0.340\n\n\n6\n1\n12/31/2010\n0.060\n\n\n7\n2\n12/31/2010\n0.001\n\n\n8\n3\n12/31/2010\n-0.120"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture11_Dates_Lags_Sorting.html#lagging-and-leading-using-.shift",
    "href": "teaching/FIN 525/lectures/lecture11_Dates_Lags_Sorting.html#lagging-and-leading-using-.shift",
    "title": "L11: Dates, lags, sorting",
    "section": "Lagging and leading using .shift()",
    "text": "Lagging and leading using .shift()\nSyntax:\nDataFrame.shift(periods=1, freq=None, axis=0, fill_value=NoDefault.no_default)\nSuppose we want to create a new column lag_return which tells us, for each firm, its returns from the prior month. The general advice you’ll see is that you should first sort your dataframe by firm identifier and by date:\n\ndf2 = df.sort_values(['firmid','date']).copy()\ndf2\n\n\n\n\n\n\n\n\nfirmid\ndate\nreturn\n\n\n\n\n0\n1\n09/30/2010\n0.050\n\n\n3\n1\n11/30/2010\n0.870\n\n\n6\n1\n12/31/2010\n0.060\n\n\n1\n2\n10/31/2010\n0.450\n\n\n4\n2\n11/30/2010\n0.200\n\n\n7\n2\n12/31/2010\n0.001\n\n\n2\n3\n11/30/2010\n23.000\n\n\n5\n3\n12/31/2010\n0.340\n\n\n8\n3\n12/31/2010\n-0.120\n\n\n\n\n\n\n\nAnd then use the .shift() function, after you tell Python that your dates are grouped at the firm level (each firm identifier has its own set of dates):\n\ndf2['lag_ret'] = df2.groupby('firmid')['return'].shift(1)\ndf2\n\n\n\n\n\n\n\n\nfirmid\ndate\nreturn\nlag_ret\n\n\n\n\n0\n1\n09/30/2010\n0.050\nNaN\n\n\n3\n1\n11/30/2010\n0.870\n0.05\n\n\n6\n1\n12/31/2010\n0.060\n0.87\n\n\n1\n2\n10/31/2010\n0.450\nNaN\n\n\n4\n2\n11/30/2010\n0.200\n0.45\n\n\n7\n2\n12/31/2010\n0.001\n0.20\n\n\n2\n3\n11/30/2010\n23.000\nNaN\n\n\n5\n3\n12/31/2010\n0.340\n23.00\n\n\n8\n3\n12/31/2010\n-0.120\n0.34\n\n\n\n\n\n\n\nNote that the entries for lag_ret on the second row and the last row are not correct: they do not tell us what the return of the firm was in the prior month. This happens because\n\nOur data has gaps in coverage (October 2010 is missing for firmid==1)\nOur data has duplicates (there are two entries for December 2010 for firmid==3)\n\nNote that both of these issues disappear if we first get rid of duplicates and if we reinterpret “lagging” to mean “the last available data point” not “data from the last calendar period” and “leading” to mean “the next available data point” and not “data from the following calendar period”. To keep things simple, this is the approach we will take in this course.\nHowever, if this reinterpretation of lagging and leading is not exactly what you need for your application and you need to lag and lead in terms of calendar periods, you should follow the approach below:"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture11_Dates_Lags_Sorting.html#leading-and-lagging-with-.merge",
    "href": "teaching/FIN 525/lectures/lecture11_Dates_Lags_Sorting.html#leading-and-lagging-with-.merge",
    "title": "L11: Dates, lags, sorting",
    "section": "Leading and lagging with .merge()",
    "text": "Leading and lagging with .merge()\nFirst, again, create a copy of the original dataset:\n\ndf2 = df.sort_values(['firmid','date']).copy()\ndf2\n\n\n\n\n\n\n\n\nfirmid\ndate\nreturn\n\n\n\n\n0\n1\n09/30/2010\n0.050\n\n\n3\n1\n11/30/2010\n0.870\n\n\n6\n1\n12/31/2010\n0.060\n\n\n1\n2\n10/31/2010\n0.450\n\n\n4\n2\n11/30/2010\n0.200\n\n\n7\n2\n12/31/2010\n0.001\n\n\n2\n3\n11/30/2010\n23.000\n\n\n5\n3\n12/31/2010\n0.340\n\n\n8\n3\n12/31/2010\n-0.120\n\n\n\n\n\n\n\nWe will first create a new date variable that tells Python the frequency of our dates:\n\ndf2['mdate'] = pd.to_datetime(df2['date']).dt.to_period('M')\ndf2\n\n\n\n\n\n\n\n\nfirmid\ndate\nreturn\nmdate\n\n\n\n\n0\n1\n09/30/2010\n0.050\n2010-09\n\n\n3\n1\n11/30/2010\n0.870\n2010-11\n\n\n6\n1\n12/31/2010\n0.060\n2010-12\n\n\n1\n2\n10/31/2010\n0.450\n2010-10\n\n\n4\n2\n11/30/2010\n0.200\n2010-11\n\n\n7\n2\n12/31/2010\n0.001\n2010-12\n\n\n2\n3\n11/30/2010\n23.000\n2010-11\n\n\n5\n3\n12/31/2010\n0.340\n2010-12\n\n\n8\n3\n12/31/2010\n-0.120\n2010-12\n\n\n\n\n\n\n\nNow create a new dataframe containing the firm identifiers (firmid), the period date (mdate) and the variable we want to lag (return):\n\nlags = df2[['firmid','mdate','return']].copy()\nlags\n\n\n\n\n\n\n\n\nfirmid\nmdate\nreturn\n\n\n\n\n0\n1\n2010-09\n0.050\n\n\n3\n1\n2010-11\n0.870\n\n\n6\n1\n2010-12\n0.060\n\n\n1\n2\n2010-10\n0.450\n\n\n4\n2\n2010-11\n0.200\n\n\n7\n2\n2010-12\n0.001\n\n\n2\n3\n2010-11\n23.000\n\n\n5\n3\n2010-12\n0.340\n\n\n8\n3\n2010-12\n-0.120\n\n\n\n\n\n\n\nNow add (subtract in case of leads) the number of periods you want to lag the return variable (1 in our example) to the period date:\n\nlags['mdate'] = lags['mdate'] + 1\nlags\n\n\n\n\n\n\n\n\nfirmid\nmdate\nreturn\n\n\n\n\n0\n1\n2010-10\n0.050\n\n\n3\n1\n2010-12\n0.870\n\n\n6\n1\n2011-01\n0.060\n\n\n1\n2\n2010-11\n0.450\n\n\n4\n2\n2010-12\n0.200\n\n\n7\n2\n2011-01\n0.001\n\n\n2\n3\n2010-12\n23.000\n\n\n5\n3\n2011-01\n0.340\n\n\n8\n3\n2011-01\n-0.120\n\n\n\n\n\n\n\nAnd rename return to lag_return:\n\nlags = lags.rename(columns={'return':'lag_return'})\nlags\n\n\n\n\n\n\n\n\nfirmid\nmdate\nlag_return\n\n\n\n\n0\n1\n2010-10\n0.050\n\n\n3\n1\n2010-12\n0.870\n\n\n6\n1\n2011-01\n0.060\n\n\n1\n2\n2010-11\n0.450\n\n\n4\n2\n2010-12\n0.200\n\n\n7\n2\n2011-01\n0.001\n\n\n2\n3\n2010-12\n23.000\n\n\n5\n3\n2011-01\n0.340\n\n\n8\n3\n2011-01\n-0.120\n\n\n\n\n\n\n\nFinally, merge this lagged data into the original dataset:\n\ndf2 = df2.merge(lags, how='left', on=['firmid','mdate'])\n\n\ndf2.sort_values(['firmid','mdate'])\n\n\n\n\n\n\n\n\nfirmid\ndate\nreturn\nmdate\nlag_return\n\n\n\n\n0\n1\n09/30/2010\n0.050\n2010-09\nNaN\n\n\n1\n1\n11/30/2010\n0.870\n2010-11\nNaN\n\n\n2\n1\n12/31/2010\n0.060\n2010-12\n0.87\n\n\n3\n2\n10/31/2010\n0.450\n2010-10\nNaN\n\n\n4\n2\n11/30/2010\n0.200\n2010-11\n0.45\n\n\n5\n2\n12/31/2010\n0.001\n2010-12\n0.20\n\n\n6\n3\n11/30/2010\n23.000\n2010-11\nNaN\n\n\n7\n3\n12/31/2010\n0.340\n2010-12\n23.00\n\n\n8\n3\n12/31/2010\n-0.120\n2010-12\n23.00\n\n\n\n\n\n\n\nNote that now the second and last entry in lag_return are correct.\nYou still have to contend with what it means that you have duplicate entries for return for December 2010 for firmid==3 but dealing with duplicates needs to be addressed on a case by case basis, depending on the particulars of the data you are using."
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture03_Data_Structures_Dot_Notation.html",
    "href": "teaching/FIN 525/lectures/lecture03_Data_Structures_Dot_Notation.html",
    "title": "L03: Data structures",
    "section": "",
    "text": "Lecture overview\nWe begin this lecture with a discussion of data structures. These are just collections of several pieces of data. Python provides a few different types of data structures, each with different properties (and hence, different use cases). We will talk about only the most commonly used ones in this course: lists, tuples, and dictionaries. Strings are also data structures (they are sequences of single characters) but we already covered them in the previous class.\nIn Python, different data structures (or basic data types like int or float) come with different “attributes” (pieces of code that compute something using the data in that data structure). These attributes and methods can be accessed using the “dot” notation (object, followed by a . followed by the attribute name or method name). We will cover these concepts in the second part of the lecture.\n\n\nLists\nLists are sequences of several pieces of data that are mutable (can be changed) and ordered.\nLists are created using square brackets, separating its elements (members) with commas:\n\nlist1 = [1, 2]\nprint(list1)\n\n[1, 2]\n\n\n\nprint(type(list1))\n\n&lt;class 'list'&gt;\n\n\nLists can contain elements of different types:\n\nlist2 = [10, 10.5, 'abc', False, list1] #an int, a float, a str, a bool, and a list\nprint(list2)\n\n[10, 10.5, 'abc', False, [1, 2]]\n\n\nElements (members) of a list can be accessed using the same slicing techniques we used for strings:\n\nprint(list2[2]) \n\nabc\n\n\n\nprint(list2[-2]) #second from the end\n\nFalse\n\n\n\nprint(list2[1:3])\n\n[10.5, 'abc']\n\n\nBecause Pyhton starts counting from 0, the above notation usually causes some confusion: “How come 1:3 gives me the second and third elements? One way to remember this is to think of 1 and 3 as endpoints”between” the elements of the list: 1 is an endpoint between the first and second elements, and 3 is between the third and fourth elements. 1:3 asks python for all the elements of the list between those end points (which means the second and third elements).\nNote that you can omit one of the endpoints in a slice:\n\nprint([10,20,30][:2]) #this is short for print([10,20,30][0:2])\n\n[10, 20]\n\n\n\nprint([10,20,30][2:]) #this is short for print([10,20,30][2:None])\n\n[30]\n\n\nAnd remember that negative positions mean “starting from the end”:\n\nprint(['a','b','c'][1:-1])\n\n['b']\n\n\n\nprint(['a','b','c'][-3:-1]) #note, when starting from the back, -1 represents the first element (not -0)\n\n['a', 'b']\n\n\nRemember, lists are ordered. This means that two lists with the same elements but in different order, are not the same:\n\na = [1,2]\nb = [2,1]\nprint(a==b)\n\nFalse\n\n\nList are mutable. That means you can change the value of their elements:\n\na = [1,2]\na[1] = 'a'\nprint(a)\n\n[1, 'a']\n\n\nCommon operators for lists (+, *, in) work as they do for strings:\n\nprint([1,2] + ['a','b'])\n\n[1, 2, 'a', 'b']\n\n\n\nprint([1,2] * 3)\n\n[1, 2, 1, 2, 1, 2]\n\n\n\nprint(2 in [1,2])\n\nTrue\n\n\n\nprint([2] in [1,2])\n\nFalse\n\n\n\nprint([2] in [1, [2]])\n\nTrue\n\n\n\n\nTuples\nTuples are also sequences of several pieces of data. Tuples are also ordered, but they are NOT mutable (i.e. they are immutable: their elements can not be changed).\nWe create tuples using parentheses, with elements separated by commas:\n\nt = (1, 'a', True, [5,6], (7,8))\nprint(t)\n\n(1, 'a', True, [5, 6], (7, 8))\n\n\nEverything we learned for lists can be performed for tuples, except for changing the value of any of its elements:\n\nprint(t[2:4])\n\n(True, [5, 6])\n\n\n\nprint(t + ('c','d')) #note: this does NOT change t\n\n(1, 'a', True, [5, 6], (7, 8), 'c', 'd')\n\n\n\nprint(t * 2)\n\n(1, 'a', True, [5, 6], (7, 8), 1, 'a', True, [5, 6], (7, 8))\n\n\n\nprint('c' in t)\n\nFalse\n\n\n\n#This will not work\nt[0] = 5\n\nTypeError: 'tuple' object does not support item assignment\n\n\n\n# But this does not give us an error\nt = t + ('c','d') #you are not trying to change any of the existing elements of t\nprint(t)\n\n(1, 'a', True, [5, 6], (7, 8), 'c', 'd')\n\n\n\n\nDictionaries (dict)\nPython dictionaries are data structures that allow us to collect key-value pairs. This is similar to how a real dictionary is structured: words are “keys” and their definitions are “values”\nWe construct dictionaries using curly brackets, with key-value pairs separated by commas, and each key separated by its value with a colon (:)\n\nd = {'k1': 1, 'k2': 'abc', 5: [6,7]}\nprint(d)\n\n{'k1': 1, 'k2': 'abc', 5: [6, 7]}\n\n\nIn the example above, the “keys” of the dictionary are ‘k1’, ‘k2’, and 5 (note that they do not have to be of the same type). The “values” of the dictionary are 1, ‘abc’, and [6,7] (values also do not have to have the same type.\nThe main difference between dictionaries and lists is in the way we access their elements. For lists, we use the position of that element in the list. For dictionaries, we use the key of the value we want to retrieve:\n\nprint(d['k1'])\n\n1\n\n\n\n#This will not work\nprint(d[0])\n\nKeyError: 0\n\n\n\n#But why does this not give me an error?\nprint(d[5]) #note: this is NOT the fifth entry in the dictionary. it's the value corresponding to the key named 5\n\n[6, 7]\n\n\nThere are no restrictions on what the values in a dictionaries can be, but keys must be unique. If not, the last entry will be the only one recorded:\n\nmk = {'a': 10, 'b': 20, 'a':30}\nprint(mk)\n\n{'a': 30, 'b': 20}\n\n\nAdding and entry to a dictionary:\n\nmk['c'] = 40\nprint(mk)\n\n{'a': 30, 'b': 20, 'c': 40}\n\n\nChanging an entry:\n\nmk['a'] = 50\nprint(mk)\n\n{'a': 50, 'b': 20, 'c': 40}\n\n\nRemoving an entry:\n\ndel mk['b']\nprint(mk)\n\n{'a': 50, 'c': 40}\n\n\nNote that if you try to run the cell above TWICE, the second time, it will give you an error, because it tries to delete the entry for key ‘b’ but it can’t find it, since you already deleted it the first time you ran the code.\n\n\nAttributes, and the dot notation\nObjects of different types (either basic data types like int and float or data structures like lists or dicts) have a number of predefined attributes, which allow you to compute something about that object. You may also see these attributes referred to as methods (for the most part, the two terms are interchangeable).\nWe can list out all the attributes of a particular object using the dir command:\n\nmylist = ['a','b','c']\nprint(dir(mylist))\n\n['__add__', '__class__', '__contains__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__imul__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rmul__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'append', 'clear', 'copy', 'count', 'extend', 'index', 'insert', 'pop', 'remove', 'reverse', 'sort']\n\n\nNote that we would get the same answer for any other list (i.e. available attributes are the same for all objects of a give type):\n\nanotherlist = [1,2]\nprint(dir(anotherlist))\n\n['__add__', '__class__', '__contains__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__imul__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rmul__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'append', 'clear', 'copy', 'count', 'extend', 'index', 'insert', 'pop', 'remove', 'reverse', 'sort']\n\n\nAnd the result is different for different kinds of data types:\n\n#Attributes of strings\nprint(dir(\"abc\"))\n\n['__add__', '__class__', '__contains__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getnewargs__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mod__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__rmod__', '__rmul__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', 'capitalize', 'casefold', 'center', 'count', 'encode', 'endswith', 'expandtabs', 'find', 'format', 'format_map', 'index', 'isalnum', 'isalpha', 'isascii', 'isdecimal', 'isdigit', 'isidentifier', 'islower', 'isnumeric', 'isprintable', 'isspace', 'istitle', 'isupper', 'join', 'ljust', 'lower', 'lstrip', 'maketrans', 'partition', 'replace', 'rfind', 'rindex', 'rjust', 'rpartition', 'rsplit', 'rstrip', 'split', 'splitlines', 'startswith', 'strip', 'swapcase', 'title', 'translate', 'upper', 'zfill']\n\n\n\n#Attributes of dictionaries\nprint(dir({'a':1}))\n\n['__class__', '__contains__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'clear', 'copy', 'fromkeys', 'get', 'items', 'keys', 'pop', 'popitem', 'setdefault', 'update', 'values']\n\n\nEach of these attributes does something else. For example, in the list of dictionary attributes above, ‘keys’ gives us a list of all the keys of that dictionary. To use the ‘keys’ attribute, we write the name of that attribute after the name of the dictionary we want to apply it to, separated by a dot:\n\nmydict = {'a': 1, \n          'b': 2}\n\nk = mydict.keys() \nprint(k)\n\ndict_keys(['a', 'b'])\n\n\nThere are different rules that you need to follow for each attribute, in terms of what you need to write AFTER the name of the attribute (e.g. putting parentheses after keys in the example above, is mandatory, otherwise you will get an error message). These rules are referred to as the syntax of that attribute.\nThere is a very lager number of attributes in python and you are not expected to remember any of the rules associated with those attributes. There will be some that we will use so often that you will just remember how to use them naturally, but you will not be required to. This is the reason why programmers always have a Google tab open: you will constantly have to search how you’re supposed to use a particular attribute.\nWe will introduce more attributes as we need them throughout the rest of the semester. This section was just meant to introduce you to the concept and make you familiar with the dot notation. This notation is also used to access subpackages of Python packages. We will talk more about this when we cover packages in more detail later on.\n\n\nPython built-in functions\nPython has a set of built-in functions that are not specific to any given data type (like the attributes we mentioned above). We have already used several such functions: “print”, “type”, “dir”. A full list of built-in functions, as well as the syntax for all of them can be found here: https://docs.python.org/3/library/functions.html\nNote that, to use these built-in functions, we do not use the dot notation mentioned above. Instead, we pass the data on which we want to operate as a “parameter” to the function (i.e. inside parentheses, after the function name).\nFor example, we write:\n\nprint(\"abc\")\n\nabc\n\n\nNot:\n\n\"abc\".print()\n\nAttributeError: 'str' object has no attribute 'print'\n\n\nWe will talk more about specific built-in functions as we need them later on in the course. If you want to take a look ahead of time, some of the most commonly used ones are: range(), abs(), list(), dict(), len(), round(), sum(), str(), zip()."
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture00_Jupyter_Basics.html",
    "href": "teaching/FIN 525/lectures/lecture00_Jupyter_Basics.html",
    "title": "L00: Jupyter basics",
    "section": "",
    "text": "Jupyter notebooks are split into “cells”.\nIf you click anywhere inside a notebook, it will enclose the cell that you clicked on in a rectangle, to show you the exact contents of that cell (the cell may be empty). - You can then press “Enter” to edit the contents of that cell (the enclosing rectangle will change colors - from blue to green). - When you are done editing a cell, you can press “Shift+Enter” to run it.\nThe two types of cells that you will be using most often are: - Markdown cells - Contain text in “Markdown” language (more on this below) - Convert a cell to Markdown by clicking on it, and the pressing “Esc” and “M” - Code cells - Contain code in language you specified by the notebook’s “kernel” (more on this below) - Convert a cell to Markdown by clicking on it, and the pressing “Esc” and “Y”\n\n\nThese are cells where you write text. This cell is a Markdown cell. Think of Markdown as a lightweight version of Microsoft Word. It allows you to format your text (indentation, bullet points, italics, etc) using pre-specified syntax (code) rather than clicking through menu items like in Work.\n\n\n\nThe following markdown cell shows you how to do basic formatting in Markdown\nHeaders:"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture00_Jupyter_Basics.html#markdown-cells",
    "href": "teaching/FIN 525/lectures/lecture00_Jupyter_Basics.html#markdown-cells",
    "title": "L00: Jupyter basics",
    "section": "",
    "text": "These are cells where you write text. This cell is a Markdown cell. Think of Markdown as a lightweight version of Microsoft Word. It allows you to format your text (indentation, bullet points, italics, etc) using pre-specified syntax (code) rather than clicking through menu items like in Work."
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture00_Jupyter_Basics.html#markdown-formatting",
    "href": "teaching/FIN 525/lectures/lecture00_Jupyter_Basics.html#markdown-formatting",
    "title": "L00: Jupyter basics",
    "section": "",
    "text": "The following markdown cell shows you how to do basic formatting in Markdown\nHeaders:"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture00_Jupyter_Basics.html#second-largest-header",
    "href": "teaching/FIN 525/lectures/lecture00_Jupyter_Basics.html#second-largest-header",
    "title": "L00: Jupyter basics",
    "section": "Second largest header",
    "text": "Second largest header\n\nThird largers header\nLists (bullet points):\n\nitem\n\nsubitem\n\nsubsubitem\n\n\nitem\n\nsubitem\n\nsubsubitem\n\n\n\nLists (numbered):\n\nitem\nitem\n\nLists (mixed):\n\nitem\n\nsubitem\n\nsubsubitem\n\nsubsubsubitem\n\n\n\nitem\n\nsubitem\n\nsubsubitem\n\nsubsubsubitem\n\n\n\n\nThis is how you do italics and this is how you do bold.\n\n\nLaTex\nMarkdown also allows you to write mathematical formulas using LaTex syntax.\nFor example:\n\\(Y_{i,t} = \\alpha + \\beta X_{i,t} + \\epsilon_{i,t}\\)\nWhich was written as\n     $Y_{i,t} = \\alpha + \\beta X_{i,t} + \\epsilon_{i,t}$\nNote that the LaTex syntax for the symbols in your equation are different from Markdown syntax."
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture00_Jupyter_Basics.html#code-cells",
    "href": "teaching/FIN 525/lectures/lecture00_Jupyter_Basics.html#code-cells",
    "title": "L00: Jupyter basics",
    "section": "Code cells",
    "text": "Code cells\nBy default, any new cell you create in a notebook will be a code cell.\nIn a code cell, you write code written in the language of the kernel you chose for your notebook. This is the language you selected when you created your notebook. It is usually displayed in the top right corner of the screen (under the “Logout” icon. If you write code written in any other language in a code cell, you will get errors when you try to run that cell.\nFor example, this is a Python 3 notebook, so the following code will work because it is written in the Python 3 language:\n\nprint(\"This will work\")\n\nBut the following code will not work, because it is written in the Stata language:\n\ndisplay \"This will not work\"\n\nThere are some kernels that will allow you to create notebooks that support multiple programming languages, but that is a more advanced topic and we will not cover it here.\nEach code cell can contain multiple lines of code.\n\nprint(\"Line 1\")\nprint(\"Line 2\")\na = 2\n\nAnd the code in a particular code cell will recognize variables that were created in a different code cell (from above or below), as long as that cell has already been run. For example the following line of code knows what the variable “a” is (from the previous cell)\n\nprint(a)\n\nTo the left of every code cell, you will see the order in which you ran them in the current session. For example, “In [4]” means this is the fourth code cell you ran in this session. Because at any time, you can run any code cell you want, the order in which you run them may not match the order in which they appear in the notebook. I highly recommend you NEVER use in a code cell variables that you define in a cell that appears lower in the notebook. Another way of saying that is to always assume that the code cells will be run in the order in which they appear in the notebook (even though that need not be the case).\nFor example, don’t run the following cell AFTER you run the one after it:\n\nprint(b)\n\n3\n\n\nNote that code cells don’t always have an output. For example the line below does not seem to do anything when you run it, but it does assign the value 3 to variable b (we’ll talk more about variables next class).\n\nb = 3"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture14_conditional_stats_applied.html",
    "href": "teaching/FIN 525/lectures/lecture14_conditional_stats_applied.html",
    "title": "L14: Conditional stats applied",
    "section": "",
    "text": "Loosely speaking, “conditional descriptive statistics” are statistics calculated for subsamples (subsets) of your data. The information you use to create these subsamples is referred to as the “conditioning information”. In this lecture, we showcase these types of descriptive statistics using the tools learned in the previous lecture, and a panel dataset: the “compa” file, which contains accounting information for multiple firms over multiple years.\nIn the previous lecture we calculated conditional statistics in situations where the variable which dictated what observations are in what subsample already exists in the dataset. For example, we calculated average returns for each industry separately. In that example, the returns of each individual industry constitute a separate subsample of our data. The “conditioning information” which allowed us to specify which observation was in what sample was the “Industry” variable (which already existed in the dataset).\nIn this lecture, we will focus on examples where we have to create ourselves the variable which specifies which observation is in what sample."
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture14_conditional_stats_applied.html#multi-dimensional-bins",
    "href": "teaching/FIN 525/lectures/lecture14_conditional_stats_applied.html#multi-dimensional-bins",
    "title": "L14: Conditional stats applied",
    "section": "Multi-dimensional bins",
    "text": "Multi-dimensional bins\nIn the example below, we redo this analysis, but this time, to judge which firm goes into which ROA bin, we compare profitability levels only amongst firms in a given year (and we do this for all years).\nTo do this, we need to use the .transform() function we introduced in the last lecture. We supply pd.qcut as a parameter to .transform().\n\ndef myq(x, myq=5, mylabels=range(1,6)):\n    return pd.qcut(x,q=myq,labels=mylabels)\n\n\ncomp['roa_q2'] = comp.groupby('year')['roa'].transform(myq)\n\n\ncomp['roa_q'] = comp.groupby('year')['roa'].transform(lambda x: pd.qcut(x, q=5, labels = range(1,6)))\n\nNote that the lambda x tells Python that, what follows after it (i.e. pd.qcut) should be seen as a function of x. So the line of code above splits the “roa” data in years, then, it takes the roa data each year, calls it “x” and then supplies it as an input to the pd.qcut() function. That function uses that roa information to split firms into quintiles (q=5) based on how their roa ranks amongst all other firms that year. These quintiles are given names 1 through 5 (labels = range(1,6)), and stored in a new column called “roa_q” inside the “comp” dataframe.\nLet’s take a look at these quintile, as well as the ones we created in the prior section, and the roa levels themselves:\n\ncomp[['roa','roa_quintile','roa_q','roa_q2']]\n\n\n\n\n\n\n\n\nroa\nroa_quintile\nroa_q\nroa_q2\n\n\n\n\n188730\n-0.345154\n1\n1\n1\n\n\n188550\n0.054648\n4\n4\n4\n\n\n188566\n0.026506\n3\n3\n3\n\n\n188567\n0.046187\n4\n4\n4\n\n\n188568\n0.065069\n4\n4\n4\n\n\n...\n...\n...\n...\n...\n\n\n493016\n-0.029779\n2\n2\n2\n\n\n493017\n-0.068448\n2\n2\n2\n\n\n493020\n-0.032821\n2\n2\n2\n\n\n493021\n-0.025125\n2\n2\n2\n\n\n493024\n0.013826\n3\n4\n4\n\n\n\n\n237017 rows × 4 columns\n\n\n\n\ncomp['roa_q'].value_counts()\n\n1    47310\n5    47301\n2    47289\n4    47289\n3    47283\nName: roa_q, dtype: int64\n\n\nNow we recalculate cash holding trends separately for each ROA bin, using these new bins:\n\ncomp.groupby(['year','roa_q'])['w_cash'].mean().unstack('roa_q').plot();\n\n\n\n\nThis looks very similar to what we found in the prior section: the result that “firms seem to be holding a lot more cash now” holds only for firms with the lowest profitability. What do you think could account for these findings?"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture07_data_input_output.html",
    "href": "teaching/FIN 525/lectures/lecture07_data_input_output.html",
    "title": "L07: Pandas I/O",
    "section": "",
    "text": "If you have not done so already (you were asked to do this in lecture01):\n\nOpen a Terminal, type the following command and hit enter:\n\npip install yfinance pandas-datareader\n\nOpen a Terminal, type the following command and hit enter:\n\nconda install -y openpyxl xlrd\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport requests\nimport pandas_datareader as pdr\nimport yfinance as yf"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture07_data_input_output.html#to_pickle",
    "href": "teaching/FIN 525/lectures/lecture07_data_input_output.html#to_pickle",
    "title": "L07: Pandas I/O",
    "section": ".to_pickle()",
    "text": ".to_pickle()\nTo store this data in a .pkl file, we use the “.to_pickle” function, applied right after the name of the dataframe which contains the data we want to store.\nSyntax:\nDataFrame.to_pickle(path, compression='infer', protocol=5, storage_options=None)\nNote that the first argument (path) is mandatory (it has no default value). This argument is where you specify the name of the .pkl file you want to create (mydata.pkl below) and the location (directory) where this file should be stored (. below) all in a single string, separated by /.\n\ndf.to_pickle('./mydata.pkl')\n\nNote that we can also compress the file:\n\ndf.to_pickle('./mydata.zip')"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture07_data_input_output.html#read_pickle",
    "href": "teaching/FIN 525/lectures/lecture07_data_input_output.html#read_pickle",
    "title": "L07: Pandas I/O",
    "section": ".read_pickle()",
    "text": ".read_pickle()\nTo read data from an existing .pkl file, we use the “.read_pickle” function, specifying as an argument the path to the file we want to read (including its name):\nSyntax:\npandas.read_pickle(filepath_or_buffer, compression='infer', storage_options=None)\nFor example, if we want to read the contents of the .pkl file we just created above, and store those contents into a new variable df2, we would use:\n\ndf2 = pd.read_pickle('./mydata.pkl')\n\n\ndf2\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n0.377738\n0.380756\n0.822901\n0.685419\n\n\n1\n0.134739\n0.608980\n0.171573\n0.064065\n\n\n2\n0.208184\n0.155670\n0.083572\n0.922087\n\n\n3\n0.796854\n0.193076\n0.262668\n0.639699\n\n\n4\n0.927714\n0.290597\n0.763463\n0.390035\n\n\n\n\n\n\n\nAnd we can read compressed .pkl files too:\n\ndf2 = pd.read_pickle('./mydata.zip')\n\n\ndf2\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n0.377738\n0.380756\n0.822901\n0.685419\n\n\n1\n0.134739\n0.608980\n0.171573\n0.064065\n\n\n2\n0.208184\n0.155670\n0.083572\n0.922087\n\n\n3\n0.796854\n0.193076\n0.262668\n0.639699\n\n\n4\n0.927714\n0.290597\n0.763463\n0.390035\n\n\n\n\n\n\n\nNote a very important difference in how we use the two functions above. The syntax for .to_pickle() starts with DataFrame.to_pickle which tells us that the function must be applied to and existing DataFrame. On the other hand, the syntax for .read_pickle() starts with pandas.read_pickle, which we converted to pd.read_pickle, because we imported pandas as pd in the first cell code in this notebook (at the top).\nThis pattern is the same for all the read-write functions we discuss in this lecture: the write functions (.to_pickle(), .to_csv(), .to_excel()) are written after the name of the dataframe we want to write to a file, while the read functions (.read_pickle(), .read_csv(), .read_excel()) follow the name of the pandas package (which we renamed to pd above)."
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture07_data_input_output.html#to_csv",
    "href": "teaching/FIN 525/lectures/lecture07_data_input_output.html#to_csv",
    "title": "L07: Pandas I/O",
    "section": ".to_csv()",
    "text": ".to_csv()\nHere is the abbreviated version of the syntax for .to_csv() excluding parameters that are not used as often:\nDataFrame.to_csv(path_or_buf=None, sep=',', columns=None, header=True, index=True, index_label=None)\nNote that the default separator is a comma (sep=',') which means we can omit that parameter when we write .csv files. The columns parameter allows you to specify which columns of the dataframe you want to write in the .csv file.\n\ndf.to_csv('./mydata.csv', columns = ['B','C'])\n\nTo write tab-delimited .txt files, we use sep='\\t' and change the file extension to .txt:\n\ndf.to_csv('./mydata.txt', sep='\\t')\n\nTo write space-delimited .txt files, we use sep=' ' (though I always recommend using tabs for .txt files):\n\ndf.to_csv('./mydata_space.txt', sep=' ')"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture07_data_input_output.html#read_csv",
    "href": "teaching/FIN 525/lectures/lecture07_data_input_output.html#read_csv",
    "title": "L07: Pandas I/O",
    "section": ".read_csv()",
    "text": ".read_csv()\nHere is the abbreviated version of the syntax for .read_csv() excluding parameters that are not used as often:\npandas.read_csv(filepath_or_buffer, sep=',',  header='infer', names=None, index_col=None, usecols=None, nrows=None, \n                skiprows = None)\nNote that the default separator is a comma (sep=',') which means we can omit that parameter when we read .csv files:\n\ndf3 = pd.read_csv('./mydata.csv')\ndf3\n\n\n\n\n\n\n\n\nUnnamed: 0\nB\nC\n\n\n\n\n0\n0\n0.380756\n0.822901\n\n\n1\n1\n0.608980\n0.171573\n\n\n2\n2\n0.155670\n0.083572\n\n\n3\n3\n0.193076\n0.262668\n\n\n4\n4\n0.290597\n0.763463\n\n\n\n\n\n\n\nNote however, that we did not specify that the first column is just an index for the table, so that first column was just included as data in the table itself. Note also that .read_csv() guessed that the column names are on the first row, because the default value of the header parameter is infer. To be save, I always recommend being explicit about where the row names and column names are (remember, Python starts counting from 0:\n\ndf3 = pd.read_csv('./mydata.csv', header = 0, index_col = 0)\ndf3\n\n\n\n\n\n\n\n\nB\nC\n\n\n\n\n0\n0.380756\n0.822901\n\n\n1\n0.608980\n0.171573\n\n\n2\n0.155670\n0.083572\n\n\n3\n0.193076\n0.262668\n\n\n4\n0.290597\n0.763463\n\n\n\n\n\n\n\nTo read tab-delimited .txt files, we use sep='\\t':\n\ndf4 = pd.read_csv('./mydata.txt', sep='\\t', header = 0, index_col = 0)\ndf4\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n0.377738\n0.380756\n0.822901\n0.685419\n\n\n1\n0.134739\n0.608980\n0.171573\n0.064065\n\n\n2\n0.208184\n0.155670\n0.083572\n0.922087\n\n\n3\n0.796854\n0.193076\n0.262668\n0.639699\n\n\n4\n0.927714\n0.290597\n0.763463\n0.390035\n\n\n\n\n\n\n\nTo read space-delimited .txt files, we use sep=' ':\n\ndf5 = pd.read_csv('./mydata_space.txt', sep=' ', header = 0, index_col = 0)\ndf5\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n0.377738\n0.380756\n0.822901\n0.685419\n\n\n1\n0.134739\n0.608980\n0.171573\n0.064065\n\n\n2\n0.208184\n0.155670\n0.083572\n0.922087\n\n\n3\n0.796854\n0.193076\n0.262668\n0.639699\n\n\n4\n0.927714\n0.290597\n0.763463\n0.390035\n\n\n\n\n\n\n\nThe .to_csv() and .read_csv() functions have a lot more useful parameters. In the practice problems for this lecture, you will be asked to investigate some of them on your own by reading the official documentation:\n\n.to_csv(): https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html\n.read_csv(): https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture07_data_input_output.html#to_excel",
    "href": "teaching/FIN 525/lectures/lecture07_data_input_output.html#to_excel",
    "title": "L07: Pandas I/O",
    "section": ".to_excel()",
    "text": ".to_excel()\nHere is an abbreviate version of the syntax for .to_excel():\nDataFrame.to_excel(excel_writer, sheet_name='Sheet1', columns=None)\n\ndf.to_excel('./mydata_excel.xlsx', sheet_name = 's1', columns = ['A', 'C'])"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture07_data_input_output.html#read_excel",
    "href": "teaching/FIN 525/lectures/lecture07_data_input_output.html#read_excel",
    "title": "L07: Pandas I/O",
    "section": ".read_excel()",
    "text": ".read_excel()\nHere is an abbreviated version of the syntax for .read_excel():\npandas.read_excel(io, sheet_name=0, header=0, names=None, index_col=None, usecols=None, skiprows=None, nrows=None)\n\ndf6 = pd.read_excel('./mydata_excel.xlsx', sheet_name = 's1', header = 0, index_col=0, engine = 'openpyxl')\ndf6\n\n\n\n\n\n\n\n\nA\nC\n\n\n\n\n0\n0.377738\n0.822901\n\n\n1\n0.134739\n0.171573\n\n\n2\n0.208184\n0.083572\n\n\n3\n0.796854\n0.262668\n\n\n4\n0.927714\n0.763463\n\n\n\n\n\n\n\nThe .to_excel() and .read_excel() functions have a lot more useful parameters. In the practice problems for this lecture, you will be asked to investigate some of them on your own by reading the official documentation:\n\n.to_excel(): https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_excel.html\n.read_excel(): https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture07_data_input_output.html#the-requests-package",
    "href": "teaching/FIN 525/lectures/lecture07_data_input_output.html#the-requests-package",
    "title": "L07: Pandas I/O",
    "section": "The “requests” package",
    "text": "The “requests” package\nThe requests package allows us to retrieve data from websites. If you want a more detailed discussion of the full functionality of this package, see the documentation at https://docs.python-requests.org/en/latest/.\nHere, we’ll just see how we can use the package to download data from files hosted on websites. For this, we need to URL to the file we want to download. In the example below, I use data on economic policy uncertainty in the US from this website:\nhttps://www.policyuncertainty.com/us_monthly.html\nIf you right-click on the “Download Data” link and select “Copy Link Address”, you should see the link below when you paste it in your code:\n\nurl = \"https://www.policyuncertainty.com/media/US_Policy_Uncertainty_Data.xlsx\"\n\nWe use the .get() function to retrieve the (binary) data from the URL above:\n\nr = requests.get(url)\n\nWe can check if the request was successful using the status_code attribute. 200 means the request was successful, 404 means there was an error.\n\nr.status_code\n\n200\n\n\nTo write the data we retrieved into an Excel file on our computer, we use the Python built-in open() function, specifying the path to the file we want to write the data to ('./policy_uncertainty.xlsx below), specifying that we are writing binary data in it (wb below):\n\noutfile = open('./policy_uncertainty.xlsx', 'wb')\n\n\ntype(outfile)\n\n_io.BufferedWriter\n\n\nNow we can write the data into that file using the Python built-in write() function and then closing that file with the close() function. Note that the actual data from the URL above is found under the content attribute of the request r that we created above.\n\noutfile.write(r.content)\noutfile.close()\n\nWe can check if this process worked by either manually opening the policy_uncertainty.xlsx in our working directory, or by using pd.read_excel() to just read the data into a dataframe and take a look at it:\n\ndf = pd.read_excel('./policy_uncertainty.xlsx').dropna() #.dropna() means drop rows with missing values\ndf\n\n\n\n\n\n\n\n\nYear\nMonth\nThree_Component_Index\nNews_Based_Policy_Uncert_Index\n\n\n\n\n0\n1985\n1.0\n125.224740\n103.748803\n\n\n1\n1985\n2.0\n99.020809\n78.313193\n\n\n2\n1985\n3.0\n112.190509\n100.761475\n\n\n3\n1985\n4.0\n102.811319\n84.778863\n\n\n4\n1985\n5.0\n120.082716\n98.053653\n\n\n...\n...\n...\n...\n...\n\n\n452\n2022\n9.0\n174.183475\n201.738489\n\n\n453\n2022\n10.0\n177.423127\n207.275335\n\n\n454\n2022\n11.0\n171.737257\n210.374894\n\n\n455\n2022\n12.0\n136.502767\n150.156098\n\n\n456\n2023\n1.0\n143.894044\n162.788431\n\n\n\n\n457 rows × 4 columns"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture07_data_input_output.html#the-pandas_datareader-package",
    "href": "teaching/FIN 525/lectures/lecture07_data_input_output.html#the-pandas_datareader-package",
    "title": "L07: Pandas I/O",
    "section": "The “pandas_datareader” package",
    "text": "The “pandas_datareader” package\nThe pandas_datareader package allows us to download data from many different sources on the internet. Here is a list of all these sources:\nhttps://pandas-datareader.readthedocs.io/en/latest/readers/index.html\nThe general syntax to download data from a particular source is as follows:\nSyntax:\npandas_datareader.DataReader(name,data_source=None,start=None,end=None)\nThe two sources I will cover here are the St. Louis Federal Reserve Economic Data (FRED) (data_source = 'fred') which contains a lot of useful macroeconomic data, and the Fama-French Data (Ken French’s Data Library) (data_source = 'famafrench') which contains returns on many portfolios commonly used in asset pricing (e.g. the market portfolio, SMB, HML, etc).\nFor both of these sources, we use the name parameter to specify what exactly we want to download from these data sources.\nFor example, to download data on the the CPI from FRED, we need to use name = 'CPIAUCSL' which is the internal name that FRED uses for the CPI data:\nhttps://fred.stlouisfed.org/series/CPIAUCSL\n\ncpi = pdr.DataReader(name = 'CPIAUCSL', data_source = 'fred', start = '2020-09-01', end = '2020-12-31')\ncpi\n\n\n\n\n\n\n\n\nCPIAUCSL\n\n\nDATE\n\n\n\n\n\n2020-09-01\n260.190\n\n\n2020-10-01\n260.352\n\n\n2020-11-01\n260.721\n\n\n2020-12-01\n261.564\n\n\n\n\n\n\n\nTo download data on the Fama-French three risk factors (market, SMB, and HML) we use name = 'F_Research_Data_Factors'\" which is the name of the text file containing these factors on Ken French’s website:\nhttps://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html\n\nff3f = pdr.DataReader(name ='F-F_Research_Data_Factors', data_source='famafrench', start='2020-09-01', end='2020-12-31')\nff3f\n\n{0:          Mkt-RF   SMB   HML    RF\n Date                             \n 2020-09   -3.63  0.04 -2.68  0.01\n 2020-10   -2.10  4.36  4.21  0.01\n 2020-11   12.47  5.82  2.14  0.01\n 2020-12    4.63  4.89 -1.51  0.01,\n 1:       Mkt-RF    SMB    HML    RF\n Date                            \n 2020   23.66  13.17 -46.57  0.45,\n 'DESCR': 'F-F Research Data Factors\\n-------------------------\\n\\nThis file was created by CMPT_ME_BEME_RETS using the 202212 CRSP database. The 1-month TBill return is from Ibbotson and Associates, Inc. Copyright 2022 Kenneth R. French\\n\\n  0 : (4 rows x 4 cols)\\n  1 : Annual Factors: January-December (1 rows x 4 cols)'}\n\n\nNote that for the ‘famafrench’ data source, the ‘DataReader’ function return a dictionary of dataframes, not a single pandas dataframe. That’s because the ‘F-F_Research_Data_Factors’ contains multiple tables. The monthly returns on the Fama-French risk factors are in the first entry in that dictionary (the 0 key), so we can retrieve it like this:\n\nff3f[0]\n\n\n\n\n\n\n\n\nMkt-RF\nSMB\nHML\nRF\n\n\nDate\n\n\n\n\n\n\n\n\n2020-09\n-3.63\n0.04\n-2.68\n0.01\n\n\n2020-10\n-2.10\n4.36\n4.21\n0.01\n\n\n2020-11\n12.47\n5.82\n2.14\n0.01\n\n\n2020-12\n4.63\n4.89\n-1.51\n0.01\n\n\n\n\n\n\n\nThere is no easy way to know under what name you can find the data you need. You have to look at the FRED and Fama-French websites first, to see what names those websites use for the data you need and then type those names into your code, like we did above."
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture07_data_input_output.html#the-yfinance-package",
    "href": "teaching/FIN 525/lectures/lecture07_data_input_output.html#the-yfinance-package",
    "title": "L07: Pandas I/O",
    "section": "The “yfinance” package",
    "text": "The “yfinance” package\nThe yfinance package allows us to retrieve stock price data from Yahoo Finance. The full documentation for the package can be found here: https://pypi.org/project/yfinance/ (especially, look under “Fetching data for multiple tickers” on the main page).\nSyntax:\nyfinance.download(tickers, start = None, end = None, interval = '1d')\nFor example, to retrieve monthly stock prices for Microsoft and Apple, we need to supply their tickers in a single string (separated by a space) as the first parameter to the download function and change the interval parameter to 1mo (otherwise it will give us daily data):\n\nprc = yf.download(tickers = \"MSFT AAPL\", start = '2020-09-01', end = '2020-12-31', interval = '1mo')\nprc\n\n[*********************100%***********************]  2 of 2 completed\n\n\n\n\n\n\n\n\n\nAdj Close\nClose\nHigh\nLow\nOpen\nVolume\n\n\n\nAAPL\nMSFT\nAAPL\nMSFT\nAAPL\nMSFT\nAAPL\nMSFT\nAAPL\nMSFT\nAAPL\nMSFT\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2020-09-01\n114.239166\n206.105713\n115.809998\n210.330002\n137.979996\n232.860001\n103.099998\n196.250000\n132.759995\n225.509995\n3.885245e+09\n768176300.0\n\n\n2020-10-01\n107.383453\n198.403580\n108.860001\n202.470001\n125.389999\n225.210007\n107.720001\n199.619995\n117.639999\n213.490005\n2.894666e+09\n631618000.0\n\n\n2020-11-01\n117.435226\n209.770584\n119.050003\n214.070007\n121.989998\n228.119995\n107.320000\n200.119995\n109.110001\n204.289993\n2.123077e+09\n573443000.0\n\n\n2020-11-06\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2020-11-18\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2020-12-01\n131.116043\n218.523499\n132.690002\n222.419998\n138.789993\n227.179993\n120.010002\n209.110001\n121.010002\n214.509995\n2.322190e+09\n594761700.0\n\n\n\n\n\n\n\nWe will always drop missing values (with .dropna()) and use Adj Close prices (prices adjusted for dividends and splits):\n\naprc = prc['Adj Close'].dropna()\naprc\n\n\n\n\n\n\n\n\nAAPL\nMSFT\n\n\nDate\n\n\n\n\n\n\n2020-09-01\n114.239166\n206.105713\n\n\n2020-10-01\n107.383453\n198.403580\n\n\n2020-11-01\n117.435226\n209.770584\n\n\n2020-12-01\n131.116043\n218.523499\n\n\n\n\n\n\n\nNote that, if we download data for a single stock, this will return a pandas Series, not a DataFrame:\n\nprc2 = yf.download(tickers = \"MSFT\", start = '2020-09-01', end = '2020-12-31', interval = '1mo')['Adj Close'].dropna()\nprc2\n\n[*********************100%***********************]  1 of 1 completed\n\n\nDate\n2020-09-01    206.105698\n2020-10-01    198.403580\n2020-11-01    209.770599\n2020-12-01    218.523514\nName: Adj Close, dtype: float64\n\n\n\ntype(prc2)\n\npandas.core.series.Series\n\n\nAs mentioned before, we will usually turn Series into dataframes before continuing to work with them further:\n\naprc2 = prc2.to_frame()\naprc2\n\n\n\n\n\n\n\n\nAdj Close\n\n\nDate\n\n\n\n\n\n2020-09-01\n206.105698\n\n\n2020-10-01\n198.403580\n\n\n2020-11-01\n209.770599\n\n\n2020-12-01\n218.523514"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture07_data_input_output.html#the-wrds-package",
    "href": "teaching/FIN 525/lectures/lecture07_data_input_output.html#the-wrds-package",
    "title": "L07: Pandas I/O",
    "section": "The “wrds” package",
    "text": "The “wrds” package\nThe wrds package allows us to download data directly from the WRDS database. Unfortunately, this functionality is not available for class accounts like the ones I created for this course. I only mention this package for PhD students, who should be able to use this package with their own individual WRDS credentials. The documentation for this package is found here:\nhttps://pypi.org/project/wrds/"
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture19_regression_application_timeseries.html",
    "href": "teaching/FIN 525/lectures/lecture19_regression_application_timeseries.html",
    "title": "L19: Robust timeseries regression",
    "section": "",
    "text": "Coverage\n\nWe only cover “static” models (of the type \\(y_{t+k} = \\alpha + \\beta \\cdot X_t + \\epsilon_{t+k}\\))\nWe do not cover dynamic models (e.g. ARIMA models or VAR models which include lags of the dependent variable as explanatory variables)\nWe do not cover conditional heteroskedasticity models (e.g. ARCH and GARCH models of the variance of the error term)\n\nDealing with autocorrelated errors (failure of assumption A3)\n\nNewey-West correction\n\nDealing with non-stationary variables (failure of assumption A2)\n\nTest for stationarity\nCommon ways to address non-stationarity\n\nFirst-differencing\nDetrending\n\n\n\n\n\nThe showcase the tools in this lecture, we will develop a (somewhat crude) test of the Expectations Hypothesis of the term structure of interest rates. In a nutshell, this hypothesis claims long-term rates should equal compounded future expected short-term rates:\n\\[ (1 + r_{t,t+N})^N = (1 + E_t(r_{t,t+1}))(1 + E_t(r_{t+1,t+2}))...(1 + E_t(r_{t+N-1,t+N})) \\]\nAssuming rational expectations, future realized short-term rates should on average match current expectations of those rates. If this is the case, one way we can test the Expectations Hypothesis by testing if current long-term rates can predict future short-term rates.\nTo implement this test, we use the yield on 10-year Treasury bonds as our long-term rate, and the yield on the 3-month Treasury bill as our short-term rate. We then regress the 3-month rate from 5 years in the future on the current 10-year rate\n\\[r^{3m}_{t+5} = \\alpha + \\beta \\cdot r^{10yr}_t + \\epsilon_{t+5} \\]\nYes, the 5-year horizon is quite arbitrary (we should be testing all horizons up to 10 years at the same time), hence my calling it a “somewhat crude” test. The purpose of this application is to showcase the common statistical issues one often encounters in time-series regressions. See this paper https://core.ac.uk/download/pdf/6956537.pdf for more thorough tests of the hypothesis.\nWe start by downloading data on the two rates (monthly frequency, not seasonally adjusted) and running the regression mentioned above. The rest of the lecture describes two main issues with this regression (non-stationarity and autocorrelated errors) and describes common tools used to address these issues."
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture19_regression_application_timeseries.html#application",
    "href": "teaching/FIN 525/lectures/lecture19_regression_application_timeseries.html#application",
    "title": "L19: Robust timeseries regression",
    "section": "",
    "text": "The showcase the tools in this lecture, we will develop a (somewhat crude) test of the Expectations Hypothesis of the term structure of interest rates. In a nutshell, this hypothesis claims long-term rates should equal compounded future expected short-term rates:\n\\[ (1 + r_{t,t+N})^N = (1 + E_t(r_{t,t+1}))(1 + E_t(r_{t+1,t+2}))...(1 + E_t(r_{t+N-1,t+N})) \\]\nAssuming rational expectations, future realized short-term rates should on average match current expectations of those rates. If this is the case, one way we can test the Expectations Hypothesis by testing if current long-term rates can predict future short-term rates.\nTo implement this test, we use the yield on 10-year Treasury bonds as our long-term rate, and the yield on the 3-month Treasury bill as our short-term rate. We then regress the 3-month rate from 5 years in the future on the current 10-year rate\n\\[r^{3m}_{t+5} = \\alpha + \\beta \\cdot r^{10yr}_t + \\epsilon_{t+5} \\]\nYes, the 5-year horizon is quite arbitrary (we should be testing all horizons up to 10 years at the same time), hence my calling it a “somewhat crude” test. The purpose of this application is to showcase the common statistical issues one often encounters in time-series regressions. See this paper https://core.ac.uk/download/pdf/6956537.pdf for more thorough tests of the hypothesis.\nWe start by downloading data on the two rates (monthly frequency, not seasonally adjusted) and running the regression mentioned above. The rest of the lecture describes two main issues with this regression (non-stationarity and autocorrelated errors) and describes common tools used to address these issues."
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture19_regression_application_timeseries.html#testing-for-stationarity-unit-root-tests",
    "href": "teaching/FIN 525/lectures/lecture19_regression_application_timeseries.html#testing-for-stationarity-unit-root-tests",
    "title": "L19: Robust timeseries regression",
    "section": "Testing for stationarity (unit root tests)",
    "text": "Testing for stationarity (unit root tests)\nBefore you run ANY time series regression, you need to first test if the variables in your regression are stationary (such tests are commonly referred to as “unit root tests”). There are several test you could use for this purpose, but one of the most common ones is the Augmented Dickey-Fuller test (ADF). You can implement this test using the “adfuller” function from the “statsmodels.tsa.stattools” package (“st” below).\nThe null hypothesis in the ADF test is that the series is non-stationary. So if the test returns a small p-value (e.g smaller that 1%), we can conclude that the series does not suffer from non-stationarity.\n\n# Test if short rate is not stationary\nadf_m3 = st.adfuller(rates['r_3m_lead5'].dropna())\nprint(\"P-value = \", adf_m3[1])\nprint(\"\\nAll results:\")\nadf_m3\n\nP-value =  0.3021083944344449\n\nAll results:\n\n\n(-1.9650413351510605,\n 0.3021083944344449,\n 20,\n 736,\n {'1%': -3.4392661055744767,\n  '5%': -2.86547495466493,\n  '10%': -2.56886540295664},\n 583.7846147439693)\n\n\nThe p-value above is larger than 1% so we can not conclude that the short rate is non-stationary.\n\n# Test if long rate is not stationary\nadf_y10 = st.adfuller(rates['r_10yr'].dropna())\nprint(\"P-value = \", adf_y10[1])\n\nP-value =  0.7806902727097635\n\n\nThe p-value above is larger than 1% so we can not conclude that the long rate is non-stationary."
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture19_regression_application_timeseries.html#first-differencing",
    "href": "teaching/FIN 525/lectures/lecture19_regression_application_timeseries.html#first-differencing",
    "title": "L19: Robust timeseries regression",
    "section": "First-differencing",
    "text": "First-differencing\nThe most commonly used method to convert a non-stationary series into a stationary series it to first-difference it (i.e. current level minus the previous level of the series). Technically, this assumes that the non-stationary series has “order of integration 1” which is the case for most economic series of interest. You don’t need to understand what that means for this class. I am only mentioning it so you understand that sometimes, first-differencing may NOT produce a stationary series. In such cases, a second-difference may help: take a first-difference of the first-difference. Alternatively, use the detrending methods in sections 3.3. and 3.4. below.\n\n# Calculate first diferences\nrates['r_3m_change'] = rates['r_3m_lead5'].diff()\nrates['r_10yr_change'] = rates['r_10yr'].diff()\n\n\n# Take a look at the differenced series\nrates[['r_3m_change','r_10yr_change']].plot();\nrates.dropna(inplace = True)\n\n\n\n\n\n# Look at autocorrelation of differenced short rate\nrates['r_3m_change'].autocorr()\n\n0.3301913374751896\n\n\nIt looks like the 1-month autocorrelation is still quite high. But, if we look at the autocorrelation plot (the gray lines are 95% confidence intervals), most of these autocorrelations are statistically insignificant.\n\npd.plotting.autocorrelation_plot(rates['r_3m_change']);\n\n\n\n\nAgain, to formally test if we still have a non-stationary problem, we run a ADF test:\n\n# Test if differenced short rate is non-stationary\nadf_m3 = st.adfuller(rates['r_3m_change'].dropna())\nprint(\"P-value = \", adf_m3[1])\n\nP-value =  4.8474710656084215e-08\n\n\nThe p-value is virtually 0, so we can reject the null of non-stationarity for the first-differenced short rate.\n\n# Test if differenced long rate is non-stationary\nadf_y10 = st.adfuller(rates['r_10yr_change'].dropna())\nprint(\"P-value = \", adf_y10[1])\n\nP-value =  1.617988023356004e-11\n\n\nAgain, the p-value is virtually 0, so we can reject the null of non-stationarity for the first-differenced long rate too.\n\n# Re-run regression using differenced variables \nresults = sm.OLS(rates['r_3m_change'], rates[['const','r_10yr_change']], missing='drop').fit()\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            r_3m_change   R-squared:                       0.001\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     1.099\nDate:                Fri, 25 Feb 2022   Prob (F-statistic):              0.295\nTime:                        14:39:54   Log-Likelihood:                -408.69\nNo. Observations:                 756   AIC:                             821.4\nDf Residuals:                     754   BIC:                             830.6\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nconst            -0.0035      0.015     -0.233      0.816      -0.033       0.026\nr_10yr_change     0.0586      0.056      1.048      0.295      -0.051       0.168\n==============================================================================\nOmnibus:                      450.788   Durbin-Watson:                   1.341\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            27983.939\nSkew:                          -1.902   Prob(JB):                         0.00\nKurtosis:                      32.562   Cond. No.                         3.69\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n# Obtain HAC standard errors\nresults_hac = results.get_robustcov_results(cov_type = 'HAC', maxlags = 5)\nprint(results_hac.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            r_3m_change   R-squared:                       0.001\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                    0.9896\nDate:                Fri, 25 Feb 2022   Prob (F-statistic):              0.320\nTime:                        14:39:54   Log-Likelihood:                -408.69\nNo. Observations:                 756   AIC:                             821.4\nDf Residuals:                     754   BIC:                             830.6\nDf Model:                           1                                         \nCovariance Type:                  HAC                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nconst            -0.0035      0.018     -0.199      0.842      -0.038       0.031\nr_10yr_change     0.0586      0.059      0.995      0.320      -0.057       0.174\n==============================================================================\nOmnibus:                      450.788   Durbin-Watson:                   1.341\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            27983.939\nSkew:                          -1.902   Prob(JB):                         0.00\nKurtosis:                      32.562   Cond. No.                         3.69\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity and autocorrelation robust (HAC) using 5 lags and without small sample correction\n\n\nThese results differ from the non-differenced regression in a very crucial way: the p-value of the r_10yr_change variable is not lower than 1% anymore, so we can not reject the null that the long rate has no predictive power over the short rate. This contradicts the prediction of the Expectations Hypothesis that long rates should have statistically significant predictive power over short rates."
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture19_regression_application_timeseries.html#controlling-for-deterministic-trends",
    "href": "teaching/FIN 525/lectures/lecture19_regression_application_timeseries.html#controlling-for-deterministic-trends",
    "title": "L19: Robust timeseries regression",
    "section": "Controlling for deterministic trends",
    "text": "Controlling for deterministic trends\nIf we believe that our variables are non-stationary because of a deterministic trend (like a linear trend or a quadratic trend), then we can adjust for this by simply including these trends in our regression. To do this, we first create the trend variables (we will restrict ourselves to a linear trend and a quadratic trend), and then we simply add them to our regression.\n\n# Create linear and quadratic trends\nrates['linear_trend'] = range(rates['r_3m_lead5'].count())\nrates['linear_trend'].plot();\n\n\n\n\n\nrates['quadratic_trend'] = rates['linear_trend']**2\nrates['quadratic_trend'].plot();\n\n\n\n\n\n# Control for trends in our main regression directly\nresults = sm.OLS(rates['r_3m_lead5'], \n                 rates[['const','r_10yr','linear_trend','quadratic_trend']], missing='drop'\n                ).fit().get_robustcov_results(cov_type = 'HAC', maxlags = 5)\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             r_3m_lead5   R-squared:                       0.575\nModel:                            OLS   Adj. R-squared:                  0.573\nMethod:                 Least Squares   F-statistic:                     58.47\nDate:                Fri, 25 Feb 2022   Prob (F-statistic):           5.57e-34\nTime:                        14:39:54   Log-Likelihood:                -1623.3\nNo. Observations:                 756   AIC:                             3255.\nDf Residuals:                     752   BIC:                             3273.\nDf Model:                           3                                         \nCovariance Type:                  HAC                                         \n===================================================================================\n                      coef    std err          t      P&gt;|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------\nconst               2.9522      0.348      8.484      0.000       2.269       3.635\nr_10yr              0.1978      0.096      2.057      0.040       0.009       0.387\nlinear_trend        0.0160      0.005      3.094      0.002       0.006       0.026\nquadratic_trend  -3.05e-05   7.29e-06     -4.182      0.000   -4.48e-05   -1.62e-05\n==============================================================================\nOmnibus:                      214.109   Durbin-Watson:                   0.041\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              593.941\nSkew:                           1.417   Prob(JB):                    1.07e-129\nKurtosis:                       6.290   Cond. No.                     8.00e+05\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity and autocorrelation robust (HAC) using 5 lags and without small sample correction\n[2] The condition number is large,  8e+05. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\nNote that even controlling for trends renders the long-rate statistically insignificant."
  },
  {
    "objectID": "teaching/FIN 525/lectures/lecture19_regression_application_timeseries.html#eliminating-stochastic-changing-trends-optional",
    "href": "teaching/FIN 525/lectures/lecture19_regression_application_timeseries.html#eliminating-stochastic-changing-trends-optional",
    "title": "L19: Robust timeseries regression",
    "section": "Eliminating stochastic (changing) trends (OPTIONAL)",
    "text": "Eliminating stochastic (changing) trends (OPTIONAL)\nIn some cases, non-stationarity could be caused by trends that change over time (e.g. a linear trend in the first part of the sample, no trend in the middle, and a quadratic trend towards the end). In this case, the deterministic-trends approach from above may not accurately control for these trends and hence may not solve our non-stationarity problem.\nIn this circumstance, it is more appropriate to estimate these stochastic trends first (for each series). Then subtract these trends from the series and use these de-trended variables in our regression instead. The Hodrick-Prescott method is very commonly used for this purpose and it outputs the detrended series directly (as well as the estimated trend). This method can be implemented using the .tsa.filters.hpfilter() function in the statsmodel package, as below.\n\n# Estimate stochastic trend in short rate using Hodrick-Prescott method\nrates['r_3m_detrended'],rates['r_3m_trend'] = sm.tsa.filters.hpfilter(rates['r_3m_lead5'],lamb = 129600)\n        #IMPORTANT: we use this \"lamb\" value because we have monthly data. \n        #use lamb = 1600 for quarterly data and lamb = 6.25 for annual data\n        \n# Take a look at the short rate and its stochastic trend\nrates[['r_3m_lead5','r_3m_trend']].plot(); \n\n\n\n\n\n# Now take a look at the detrended short rate\nrates['r_3m_detrended'].plot(); \n\n\n\n\n\n# Estimate stochastic trend in long rate using Hodrick-Prescott method\nrates['r_10yr_detrended'],rates['r_10yr_trend'] = sm.tsa.filters.hpfilter(rates['r_10yr'],lamb = 129600)\n\n# Take a look at the results \nrates[['r_10yr','r_10yr_trend']].plot(); \n\n\n\n\n\n# Now take a look at the detrended long rate\nrates['r_10yr_detrended'].plot(); \n\n\n\n\n\n# Re-run regression using detrended series\nresults = sm.OLS(rates['r_3m_detrended'], \n                 rates[['const','r_10yr_detrended']], missing='drop'\n                ).fit().get_robustcov_results(cov_type = 'HAC', maxlags = 5)\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:         r_3m_detrended   R-squared:                       0.006\nModel:                            OLS   Adj. R-squared:                  0.004\nMethod:                 Least Squares   F-statistic:                    0.8779\nDate:                Fri, 25 Feb 2022   Prob (F-statistic):              0.349\nTime:                        14:39:55   Log-Likelihood:                -1201.4\nNo. Observations:                 756   AIC:                             2407.\nDf Residuals:                     754   BIC:                             2416.\nDf Model:                           1                                         \nCovariance Type:                  HAC                                         \n====================================================================================\n                       coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------------\nconst              5.36e-12      0.097    5.5e-11      1.000      -0.191       0.191\nr_10yr_detrended     0.1270      0.136      0.937      0.349      -0.139       0.393\n==============================================================================\nOmnibus:                       83.839   Durbin-Watson:                   0.120\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              133.609\nSkew:                           0.751   Prob(JB):                     9.71e-30\nKurtosis:                       4.410   Cond. No.                         1.41\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity and autocorrelation robust (HAC) using 5 lags and without small sample correction\n\n\nOnce again, detrending the data using stochastic trends also renders the long rate insignificant."
  },
  {
    "objectID": "teaching/FIN 421/lectures/L09_Review.html",
    "href": "teaching/FIN 421/lectures/L09_Review.html",
    "title": "L09: Review",
    "section": "",
    "text": "Lecture Overview\nIn this lecture we will work on an application that tries to incorporate most of the tools we learned so far.\n\n\nReal-World Application: performance of main asset classes\nFrom Yahoo Finance, download monthly data on the SPDR S&P 500 ETF, the SPRD Gold Shares ETF, and BlackRock’s long-term (20+ years) treasury ETF (tickers: SPY, GLD, TLT respectively).\n\nDownload monthly data from 2005 to 2022 and calculate monthly returns (use Adj Close)\nPlot cumulative returns on these assets over time\nCalculate Sharpe ratio for each asset\nCalculate optimal tangency portfolio using these 3 assets\nCalculate Sharpe ratio of tangency portfolio\nCalculate optimal capital allocation if risk aversion is A = 4\n\n\n\nCumulative returns\n\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n\n\n\nSample statistics of individual assets\n\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n\n\n\nTangency portfolio weights\n\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n\n\n\nSample statistics of tangency portfolio\n\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n\n\n\nOptimal asset allocation weights\n\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#"
  },
  {
    "objectID": "teaching/FIN 421/lectures/Lecture 18 and 19 on Multiples Valuation.html",
    "href": "teaching/FIN 421/lectures/Lecture 18 and 19 on Multiples Valuation.html",
    "title": "L18_19: Multiples valuation",
    "section": "",
    "text": "Valuation using multiples: The general approach\nThe general idea behind using multiples to estimate the intrinsic value of a firm is that intrinsic value should have a stable relationship with some fundamental aspects of the firm (e.g. its profits or its book value). If this is the case, and we have a way to estimate this relation, then we can use it to back out the intrinsic value of the firm.\nAs an example, suppose we have a way to estimate the firm’s true value to earnings ratio. We can then multiply this estimate by the firm’s earnings, and that would give us an estimate for the firm’s value.\nThe most common ways of estimating the true value of any given multiple for a firm: - Take an average of this multiple for a handful of close competitors of the firm you are valuing - Take an average of this multiple for the entire industry of the firm you are valuing - Take an average over the past values of this multiple for the firm you are valuing\nThis is the reason why we say that multiples valuation is a form of comparables valuation, where comparables are firms that are similar in some fundamental way to the firm we are valuing.\nWe will demonstrate this technique below, using three common multiples: - Price to earnings ratios (P/E) - Price to book ratios (P/B, aka market to book ratios or M/B) - (Total) Enterprise Value to EBITDA ratios (TEV/EBITDA)\n\n\nThe price to earnings (P/E) ratio\nThe P/E ratio is calculated as the current price of the stock divided by its earnings per share (EPS). This is equivalent to dividing total market capitalization by net income.\n\\[\\frac{P}{E} = \\frac{Price}{EPS} = \\frac{Market Cap}{Net Income}\\]\nThe P/E ratio has different names depending on which EPS (or Net Income) number we use: - Trailing P/E ratios use EPS from the most recent 10-K filing - Current P/E ratios use the most recent EPS available (sometimes called the TTM (trailing twelve month) EPS) - Forward P/E ratios use a forecasted EPS\nExample 1:\nAssume in the most recent fiscal year, MSFT had EPS of $5.7, and its main competitors had trailing P/E ratios of 35, 40, and 45 respectively. Estimate the intrinsic value of a share of MSFT using this data.\n\n#\n#\n#\n#\n#\n#\n#\n#\n\nExample 2:\nAssume you estimated that MSFT will have EPS of $5.7 in the following fiscal year, and its main competitors have forward P/E ratios of 25, 30, and 35 respectively. Estimate the intrinsic value of a share of MSFT using this data.\n\n#\n#\n#\n#\n#\n#\n#\n#\n\n\n\nThe price to book (P/B, or M/B) ratio\nThe P/B ratio is calculated as the market value of equity divided by the book value of equity:\n\\[\\frac{P}{B} = \\frac{Market Cap}{Book Equity} = \\frac{Market Cap}{Assets - Liabilities}\\]\nHigh P/B firms are often called growth firms.\nLow P/B firms are often called value firms.\nExample 3:\nAssume in the most recent fiscal year, MSFT had book value of equity of $118 (bil) and 7.6 bil shares outstanding. Its main competitors had P/B ratios of 15, 20, and 25 respectively. Estimate the intrinsic value of a share of MSFT using this data.\n\n#\n#\n#\n#\n#\n#\n#\n#\n\n\n\n(Total) Enterprise value to EBITDA (EV / EBITDA) ratio\nThe EV/EBITDA (or TEV/EBITDA) ratio is calculated at the enterprise value of the firm divided by the firm’s EBITDA, where enterprise value is defined as the market value of the firm’s equity plus the market value of the firm’s debt minus cash. Often, the firm’s book value of total debt is used instead of the market value of debt:\n\\[\\frac{EV}{EBITDA} = \\frac{Market Cap + Preferred Stock+ Long Term Debt - Cash}{EBITDA}\\]\nFor long term debt, that should include the portion due within a year.\nExample 4:\nAssume in the most recent fiscal year, MSFT had EBITDA of $118 (bil) and 7.6 bil shares outstanding. It also had $98 bil in long term debt and $113 (bil) in cash holdings. Its main competitors had EV/EBITDA ratios of 15, 20, and 25 respectively. Estimate the intrinsic value of a share of MSFT using this data.\n\n#\n#\n#\n#\n#\n#\n#\n#\n\n\n\nUsing real-world data\nWe estimate the true P/E, P/B, and EV/EBITDA of MSFT in three different ways, using: 1. Averages of MSFT’s own ratios 2. Averages of these ratios at the industry level (from Damodaran’s files) 3. Averages of these ratios for MSFT’s competitors\nWe then multiply each of these estimates with MSFT’s most recent value for the denominator in each ratio.\n\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#"
  },
  {
    "objectID": "teaching/FIN 421/lectures/Lecture 12.html",
    "href": "teaching/FIN 421/lectures/Lecture 12.html",
    "title": "L12: CAPM",
    "section": "",
    "text": "Markets are perfectly competitive\n\nNo investor is wealthy enough to individually affect prices\nAll information is publicly available; all securities are public\nNo taxes on returns, no transaction costs\nUnlimited borrowing/lending at risk-free rate\n\nInvestors are all alike except for initial wealth and risk aversion\n\nInvestors plan for a single-period horizon; they are rational, mean-variance optimizers\nInvestors use the same inputs, consider identical portfolio opportunity sets\n\n\n\n\n\n\nThe risk premium on individual assets is proportional to the risk premium on the market:\n\n\\[E[R_{i}] = R_{f} +  \\beta_i (E[R_{m}] - R_{f}) \\]\n\nThe market portfolio is the optimal risky portfolio (the tangency portfolio) in the economy\nAll investors choose to hold a combination of the risk-free asset and the market portfolio (in different proportions, given by the investor’s risk aversion)\n\n\n\n\nIf the CAPM assumptions hold, plotting all securities’ risk premia (\\(E[R_{i}] - R_{f}\\)) against their betas (\\(\\beta_i\\)) results in a single line called the Security Market Line (SML).\n\n\n\nThe “data.xlsx” file from D2L contains adjusted close prices for the past 10 years for the following tickers: ‘AAPL’, ‘GE’, ‘F’, ‘GM’, ‘NFLX’, ‘MSFT’, ‘GS’, ‘JPM’, ‘AMD’, ‘NVDA’.\nFor all stocks:\n\nCalculate monthly returns\nCalculate market betas (use the SLOPE function in Excel)\nCalculate average returns\nPlot average returns (y axis) against betas (x axis)\n\n\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#"
  },
  {
    "objectID": "teaching/FIN 421/lectures/Lecture 12.html#capm-assumptions",
    "href": "teaching/FIN 421/lectures/Lecture 12.html#capm-assumptions",
    "title": "L12: CAPM",
    "section": "",
    "text": "Markets are perfectly competitive\n\nNo investor is wealthy enough to individually affect prices\nAll information is publicly available; all securities are public\nNo taxes on returns, no transaction costs\nUnlimited borrowing/lending at risk-free rate\n\nInvestors are all alike except for initial wealth and risk aversion\n\nInvestors plan for a single-period horizon; they are rational, mean-variance optimizers\nInvestors use the same inputs, consider identical portfolio opportunity sets"
  },
  {
    "objectID": "teaching/FIN 421/lectures/Lecture 12.html#capm-predictions",
    "href": "teaching/FIN 421/lectures/Lecture 12.html#capm-predictions",
    "title": "L12: CAPM",
    "section": "",
    "text": "The risk premium on individual assets is proportional to the risk premium on the market:\n\n\\[E[R_{i}] = R_{f} +  \\beta_i (E[R_{m}] - R_{f}) \\]\n\nThe market portfolio is the optimal risky portfolio (the tangency portfolio) in the economy\nAll investors choose to hold a combination of the risk-free asset and the market portfolio (in different proportions, given by the investor’s risk aversion)"
  },
  {
    "objectID": "teaching/FIN 421/lectures/Lecture 12.html#the-security-market-line-sml",
    "href": "teaching/FIN 421/lectures/Lecture 12.html#the-security-market-line-sml",
    "title": "L12: CAPM",
    "section": "",
    "text": "If the CAPM assumptions hold, plotting all securities’ risk premia (\\(E[R_{i}] - R_{f}\\)) against their betas (\\(\\beta_i\\)) results in a single line called the Security Market Line (SML)."
  },
  {
    "objectID": "teaching/FIN 421/lectures/Lecture 12.html#application",
    "href": "teaching/FIN 421/lectures/Lecture 12.html#application",
    "title": "L12: CAPM",
    "section": "",
    "text": "The “data.xlsx” file from D2L contains adjusted close prices for the past 10 years for the following tickers: ‘AAPL’, ‘GE’, ‘F’, ‘GM’, ‘NFLX’, ‘MSFT’, ‘GS’, ‘JPM’, ‘AMD’, ‘NVDA’.\nFor all stocks:\n\nCalculate monthly returns\nCalculate market betas (use the SLOPE function in Excel)\nCalculate average returns\nPlot average returns (y axis) against betas (x axis)\n\n\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#"
  },
  {
    "objectID": "teaching/FIN 421/lectures/L07_08_Portfolio_Theory_4_Optimal_Asset_Allocation.html",
    "href": "teaching/FIN 421/lectures/L07_08_Portfolio_Theory_4_Optimal_Asset_Allocation.html",
    "title": "L07_08: Optimal asset allocation",
    "section": "",
    "text": "Optimal asset allocation has three parts:\n\nFinding the tangency portfolio (combining risky assets optimally)\n\nCovered in the previous lecture (Lecture 6).\nAlso called the optimal risky portfolio.\nDoes not depend on risk aversion. Only on the expected returns, standard deviations and covariances of the risky assets you want to invest in.\nAmounts to finding the weights in those risky assets that give us the portfolio with the highest Sharpe ratio (best expected return per unit of risk)\n\nFinding the optimal capital allocation\n\nCovered two lectures ago (Lecture 5)\nInvolves finding the proportion of our funds we should invest in the tangency portfolio found above (with the remaining funds to be invested in a risk-free asset)\nDepends on the level of risk aversion of the investor\nThe resulting combination of the risky portfolio and the risk-free asset is called the complete portfolio\n\nCombining the results in the previous two steps\n\nCovered in this class\nAmounts to multiplying the weight of each risky asset inside the tangency portfolio with the weight of the tangency portfolio inside the complete portfolio"
  },
  {
    "objectID": "teaching/FIN 421/lectures/L07_08_Portfolio_Theory_4_Optimal_Asset_Allocation.html#step-1-find-the-tangency-portfolio",
    "href": "teaching/FIN 421/lectures/L07_08_Portfolio_Theory_4_Optimal_Asset_Allocation.html#step-1-find-the-tangency-portfolio",
    "title": "L07_08: Optimal asset allocation",
    "section": "Step 1: Find the tangency portfolio",
    "text": "Step 1: Find the tangency portfolio\nWe saw in the previous lecture that, given your N risky assets, the weights that give us the portfolio with the highest Sharpe ratio are given by:\n\\[W_{tan} = \\frac{ \\Sigma^{-1}\\mu^e }{ \\mathbf{1^{T}}\\Sigma^{-1}\\mu^e }\\]\nwhere \\(\\Sigma^{-1}\\) is the inverse of the variance-covariance matrix of the N assets, and \\(\\mu^e\\) is the vector of risk premia on your N assets.\nRemember that \\(W_{tan}\\) is a vector of numbers, not a single number:\n\\[W_{tan} = \\begin{pmatrix} w_{1,tan} \\\\ w_{2,tan} \\\\ ... \\\\ w_{N,tan} \\end{pmatrix}\\]\nwhere each element \\(w_{i,tan}\\) gives us the weight of risky asset \\(i\\) inside the tangency portfolio."
  },
  {
    "objectID": "teaching/FIN 421/lectures/L07_08_Portfolio_Theory_4_Optimal_Asset_Allocation.html#step-2-find-the-optimal-capital-allocation",
    "href": "teaching/FIN 421/lectures/L07_08_Portfolio_Theory_4_Optimal_Asset_Allocation.html#step-2-find-the-optimal-capital-allocation",
    "title": "L07_08: Optimal asset allocation",
    "section": "Step 2: Find the optimal capital allocation",
    "text": "Step 2: Find the optimal capital allocation\nWe saw two lectures ago that, if we have decided to invest in some risky portfolio P and the risk-free asset, the optimal weight in the risky asset is given by:\n\\[w_{oca} = \\frac{E[R_P] - R_f}{A \\sigma_{P}^2}\\]\nwhere \\(A\\) is the investor’s risk aversion coefficient."
  },
  {
    "objectID": "teaching/FIN 421/lectures/L07_08_Portfolio_Theory_4_Optimal_Asset_Allocation.html#step-3-combine-the-first-two-steps-to-obtain-the-optimal-asset-allocation",
    "href": "teaching/FIN 421/lectures/L07_08_Portfolio_Theory_4_Optimal_Asset_Allocation.html#step-3-combine-the-first-two-steps-to-obtain-the-optimal-asset-allocation",
    "title": "L07_08: Optimal asset allocation",
    "section": "Step 3: Combine the first two steps to obtain the optimal asset allocation",
    "text": "Step 3: Combine the first two steps to obtain the optimal asset allocation\nThe risky portfolio \\(P\\) in the above formula is our tangency portfolio from Step 1. So, in order to apply the above formula, we need to know not only the risk aversion of the investor (\\(A\\)), but also the expected return and variance of the tangency portfolio (\\(E[R_P]\\) and \\(\\sigma_{P}^2\\)). This is straightforward, since we already know the weights of each asset inside the tangency portfolio from Step 1 (\\(W_{tan}\\)). Using the matrix notation above, we have:\n\\[E[R_P] - R_f = W_{tan}^{T} \\cdot \\mu^e\\]\nand\n\\[\\sigma_{P}^2 = W_{tan}^{T} \\cdot  \\Sigma \\cdot  W_{tan}\\]\nRemember the \\(T\\) superscript in \\(W_{tan}^{T}\\) stands for “transpose” which means reshaping the N x 1 vector \\(W_{tan}\\) into a 1 x N vector:\n\\[W_{tan}^{T} = \\begin{pmatrix} w_{1,tan} & w_{2,tan} & ...& w_{N,tan} \\end{pmatrix}\\]\nGiven the optimal capital allocation weight \\(w_{oca}\\) and the tangency portfolio weights \\(W_{tan}\\), the optimal asset allocation involves:\n\nInvesting \\(1 - w_{oca}\\) of your funds in the risk-free asset\nInvesting \\(w_{oca} \\cdot w_{i,tan}\\) of your finds in each risky asset \\(i\\)\n\nWhere, as mentioned above, \\(w_{i,tan}\\) is the weight of asset \\(i\\) inside the tangency portfolio (the \\(i^{th}\\) element of the \\(W_{tan}\\) vector)"
  },
  {
    "objectID": "teaching/FIN 421/lectures/L07_08_Portfolio_Theory_4_Optimal_Asset_Allocation.html#n-2-step-1-tangency-portfolio",
    "href": "teaching/FIN 421/lectures/L07_08_Portfolio_Theory_4_Optimal_Asset_Allocation.html#n-2-step-1-tangency-portfolio",
    "title": "L07_08: Optimal asset allocation",
    "section": "N = 2: Step 1 (tangency portfolio)",
    "text": "N = 2: Step 1 (tangency portfolio)\nThe tangency portfolio has weights:\n\\[W_{tan} = \\begin{pmatrix} w_{A,tan} \\\\ w_{B,tan} \\end{pmatrix}\\]\nwhere\n\\[w_{A,tan} = \\frac{(E[R_A] - R_f) \\sigma_{B}^2 - (E[R_B] - R_f)Cov[R_A, R_B]}{(E[R_A] - R_f) \\sigma_{B}^2 + (E[R_B] - R_f) \\sigma_{A}^2 - (E[R_A] - R_f + E[R_B] - R_f)Cov[R_A, R_B]}\\]\nand \\(w_{B,tan} = 1 - w_{A,tan}\\)."
  },
  {
    "objectID": "teaching/FIN 421/lectures/L07_08_Portfolio_Theory_4_Optimal_Asset_Allocation.html#n-2-step-2-capital-allocation",
    "href": "teaching/FIN 421/lectures/L07_08_Portfolio_Theory_4_Optimal_Asset_Allocation.html#n-2-step-2-capital-allocation",
    "title": "L07_08: Optimal asset allocation",
    "section": "N = 2: Step 2 (capital allocation)",
    "text": "N = 2: Step 2 (capital allocation)\nHere the formula looks the same as for the general case. The optimal capital allocation involves placing a weight\n\\[w_{oca} = \\frac{E[R_P] - R_f}{A \\sigma_{P}^2}\\]\nof your total funds in the tangency portfolio P. As calculated at Step 1, this tangency portfolio has weights \\((w_{A,tan}, w_{B,tan})\\) invested A and B respectively. This means that, to use the formula above, we must first calculate:\n\\[E[R_P] = w_{A,tan} E[R_A] + w_{B,tan} E[R_B]\\]\nand\n\\[\\sigma_{P}^2 = Var[R_P] =  w_{A,tan}^2 Var[R_A] + w_{B,tan}^2 Var[R_B] + 2 w_{A,tan} w_{B,tan} Cov[R_A, R_B]\\]"
  },
  {
    "objectID": "teaching/FIN 421/lectures/L07_08_Portfolio_Theory_4_Optimal_Asset_Allocation.html#n-3-step-3-asset-allocation",
    "href": "teaching/FIN 421/lectures/L07_08_Portfolio_Theory_4_Optimal_Asset_Allocation.html#n-3-step-3-asset-allocation",
    "title": "L07_08: Optimal asset allocation",
    "section": "N = 3: Step 3 (asset allocation)",
    "text": "N = 3: Step 3 (asset allocation)\nStep 2 says that we should put \\(w_{oca}\\) of our funds in the tangency portfolio and \\(1 - w_{oca}\\) in the risk-free asset. Step 1, says that, of the \\(w_{oca}\\) proportion we put in the tangency portfolio, \\(w_{A,tan}\\) should be invested in A and \\(w_{B,tan}\\) should be invested in B.\nSo the final, optimal asset allocation says we should put:\n\nA proportion \\(1 - w_{oca}\\) of the funds in the risk-free asset\nA proportion \\(w_{oca} \\cdot w_{A,tan}\\) of the funds in the risky asset A\nA proportion \\(w_{oca} \\cdot w_{B,tan}\\) of the funds in the risky asset B"
  },
  {
    "objectID": "teaching/FIN 421/lectures/L07_08_Portfolio_Theory_4_Optimal_Asset_Allocation.html#example",
    "href": "teaching/FIN 421/lectures/L07_08_Portfolio_Theory_4_Optimal_Asset_Allocation.html#example",
    "title": "L07_08: Optimal asset allocation",
    "section": "Example",
    "text": "Example\nYou want to invest in Facebook (FB) and Netflix (NFLX) and the risk-free asset but are not sure how much to invest in each. You have estimated that the expected returns of FB and NFLX are 10% and 15% respectively, their standard deviations are 20% and 25% respectively and their covariance is 0.02. The risk-free rate is 0.1%. Find the optimal asset allocation between the three assets assuming that your risk-aversion coefficient is 5.\n\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#"
  },
  {
    "objectID": "teaching/FIN 421/lectures/Lecture 14.html",
    "href": "teaching/FIN 421/lectures/Lecture 14.html",
    "title": "L14: Bond pricing",
    "section": "",
    "text": "Bond prices are calculated using the same general approach as the prices of any financial asset: they equal the discounted value of their future cash flows.\n\\[P_t = \\frac{C}{1+YTM} + \\frac{C}{(1+YTM)^2} + ... + \\frac{C + Par}{(1+YTM)^T} \\]\nwhere - \\(P_t\\) is the price of the bond at time \\(t\\) - \\(C\\) is the coupon payment of the bond - \\(Par\\) is the par value of the bond (also known as it face value) - \\(T\\) is the number of years left until maturity of the bond - \\(YTM\\) is the yield to maturity of the bond (often simply called the yield of the bond)\nFor bonds that do not pay coupons (also known as zero-coupon bonds or “zeros”), the formula becomes\n\\[P_t = \\frac{Par}{(1+YTM)^T} \\]\n\n\nThe formula above is only correct if the bond has a single coupon payment per year. If the bond pays “F” coupons per year (e.g. F = 2 for bond with semi-annual coupon payments) then the formula above becomes:\n\\[P_t = \\frac{C/F}{1+YTM/F} + \\frac{C/F}{(1+YTM/F)^2} + ... + \\frac{C/F + Par}{(1+YTM/F)^{T \\cdot F}} \\]\n\n\n\nThe formula above is only valid on coupon-paying days (i.e. \\(t\\) is immediately after a coupon is paid) or when the bond is issued. On any other day, we need to adjust the price for something called “accrued interest”. We cover this at the end of this lecture."
  },
  {
    "objectID": "teaching/FIN 421/lectures/Lecture 14.html#when-bond-pays-coupons-more-than-once-a-year",
    "href": "teaching/FIN 421/lectures/Lecture 14.html#when-bond-pays-coupons-more-than-once-a-year",
    "title": "L14: Bond pricing",
    "section": "",
    "text": "The formula above is only correct if the bond has a single coupon payment per year. If the bond pays “F” coupons per year (e.g. F = 2 for bond with semi-annual coupon payments) then the formula above becomes:\n\\[P_t = \\frac{C/F}{1+YTM/F} + \\frac{C/F}{(1+YTM/F)^2} + ... + \\frac{C/F + Par}{(1+YTM/F)^{T \\cdot F}} \\]"
  },
  {
    "objectID": "teaching/FIN 421/lectures/Lecture 14.html#non-coupon-paying-days",
    "href": "teaching/FIN 421/lectures/Lecture 14.html#non-coupon-paying-days",
    "title": "L14: Bond pricing",
    "section": "",
    "text": "The formula above is only valid on coupon-paying days (i.e. \\(t\\) is immediately after a coupon is paid) or when the bond is issued. On any other day, we need to adjust the price for something called “accrued interest”. We cover this at the end of this lecture."
  },
  {
    "objectID": "teaching/FIN 421/lectures/Lecture 14.html#day-counting-conventions",
    "href": "teaching/FIN 421/lectures/Lecture 14.html#day-counting-conventions",
    "title": "L14: Bond pricing",
    "section": "Day-counting conventions",
    "text": "Day-counting conventions\nA crucial question is: how should \\(t_1\\), \\(t_2\\),…,\\(t_N\\) be calculated. The answer to this question can be different from bond to bond, depending on the day-counting convention used for that type of bond. The most common conventions are:\n\n30/360 (assumes all months have 30 days and all years have 360 days)\n\nmost common for corporate bonds, municipal bonds and mortgage-backed securities\n\nActual/Actual (uses the actual number of days between dates, and the actual number of days in the year)\n\nmost common for Treasury Bonds\n\nActual/360 (uses the actual number of days between dates, but assumes all years have 360 days)\n\nmost common for short-term securities like commercial paper and T-bills"
  },
  {
    "objectID": "teaching/FIN 421/lectures/Lecture 14.html#clean-flat-vs-dirty-invoice-prices",
    "href": "teaching/FIN 421/lectures/Lecture 14.html#clean-flat-vs-dirty-invoice-prices",
    "title": "L14: Bond pricing",
    "section": "Clean (flat) vs dirty (invoice) prices",
    "text": "Clean (flat) vs dirty (invoice) prices\nThe price given by the formula above is called the dirty price or invoice price of the bond. This is the price you actually have to pay if you buy the bond at \\(t_0\\).\nThe price that is quoted for the bond is called the clean price or flat price of the bond. This price differs from the dirty price by the amount of accrued interest of the bond since the last coupon payment.\n\\[Dirty Price = Clean Price + Accrued Interest\\]\nWhere \\[AccruedInterest = \\frac{\\text{Days since last coupon was paid}}{\\text{Total days in the current coupon period}} Coupon Payment\\]\nIt is also important to remember that, bond prices are quoted as percentage of par. So a bond with par value of $1000 and a price of 103, actually has a clean price of $1030. When I do not specify the par value, assume that it is $1000.\nExample 3:\nBond A was issued on May 15, 2000 and it matures in May 15, 2030. Coupons are paid semi-annualy, the coupon rate is 6%, and the yield to maturity is 2%. What was the invoice (dirty) price on July 31, 2012? Assume days are counted using the “Actual/Actual” convention.\n\n# \n#\n#\n#\n#\n#\n#\n#\n\nExample 4:\nIt is January 1st, 2000. Bond A matures on March 1st, 2000. Coupons are paid quarterly, the coupon rate is 4%, and the yield to maturity is 4%. What was the invoice (dirty) price today in dollars? Assume the days are counted using the “30/360” convention.\n\n# \n#\n#\n#\n#\n#\n#\n#"
  },
  {
    "objectID": "teaching/FIN 421/lectures/Lecture 15.html",
    "href": "teaching/FIN 421/lectures/Lecture 15.html",
    "title": "L15: Bond yields",
    "section": "",
    "text": "Yield to maturity and current yield\nA bond’s yield to maturity (YTM) is the rate that makes the present value of the bond’s future cash flows equal to its current price. In other words, if you know the price of the bond, the yield is obtained by solving for YTM in the pricing equation:\n\\[P_t = \\frac{C}{1+YTM} + \\frac{C}{(1+YTM)^2} + ... + \\frac{C + Par}{(1+YTM)^T} \\]\nThere is an inverse relationship between bond prices and their YTM: if a bond has a higher YTM, that means its cash flows are discounted at a higher rate, which means its price is lower. The implication is that bond prices go up when interest rates go down and they go down when interest rates go up.\nA bond’s current yield equals its annual coupon payment divided by the current bond price.\nTo calculate YTM using Excel, we use the YIELD function, which has the following parameters:\n\nSettlement date = Date as of which you are pricing the bond\nMaturity date\nAnnual coupon rate (decimal, e.g. 6% should be put in as 0.06)\nPrice of the bond = The clean (flat) price of the bond, expresses as a percentage of par (face) value\nRedemption value (% of face value, usually 100)\nCoupon payments per year\n\nExample 1:\nAssume today is July 31, 2012. Bond A matures on July 31, 2018, has par value of $1000, pays coupons semiannually, and has a coupon rate of 6%. The flat price of the bond is $1225.102. What is the YTM of the bond today? What is its current yield?\n\n# \n#\n#\n#\n#\n#\n#\n#\n\nExample 2:\nBond A matures in 30 years, and pays coupons annually. It has a coupon rate of 8%, and a par value of $100. The quoted clean price of the bond is 81.14617. What is the YTM of the bond today? What is its current yield?\n\n# \n#\n#\n#\n#\n#\n#\n#\n\nExample 3:\nBond A was issued on May 15, 2000 and it matures in May 15, 2030. Coupons are paid semi-annualy, the coupon rate is 6%, and the par value is $100. July 31, 2012 the flat price is 159.6281 and the invoice price is 160.8835. What is the YTM of the bond on July 31, 2012? What is its current yield.\n\n# \n#\n#\n#\n#\n#\n#\n#\n\n\n\nReinvesting coupons\nYour return from a bond investment will depend on how you (re)invest the coupons. If you are able to reinvest your coupons at a higher rate, the total return from the bond will be higher.\nBelow, we work through an example where we assume you are able to reinvest the coupons at a particular, constant, interest rate. This makes the coupons a fixed-rate annuity. The total proceeds from the coupons will therefore be the future value of that annuity.\nWe can calculate future values of annuities with the “FV” function in Excel, which has the following parameters:\n\nrate = interest rate per compounding period (YTM divided by number of coupons per year)\nnper = number of compounding periods left (years to maturity times number of coupons per year)\npmt = coupon per compounding period (par value times annual coupon rate divided by number of coupons per year). Put a negative sign in front of it to express that it is an outflow.\n\nExample 4:\nBond A matures in 10 years and it pays interest quarterly. It has a coupon rate of 5% and a YTM of 10%. Calculate the total proceeds from the coupons of the bond if you reinvest all coupons at a 10% rate and you hold the bond until maturity.\n\n# \n#\n#\n#\n#\n#\n#\n#\n\nExample 5:\nSame question as in Example 4, only now, assume you don’t reinvest the coupons at all.\n\n# \n#\n#\n#\n#\n#\n#\n#\n\nExample 6:\nSame question as in Example 4, only now, assume you reinvest the coupons at a rate of 20% per year.\n\n# \n#\n#\n#\n#\n#\n#\n#\n\n\n\nBond compounded realized yield (realized return)\nThe realized yield (or realized return) of a bond is your total return from investing in the bond. It equals the proceeds from the coupons (possibly reinvested) plus the price you obtain from selling the bond (or par value if you hold it until maturity), all divided by the price you paid for the bond initially.\nNote that this give us a total, gross, compounded return on the bond. To convert it to an APR, we use the familiar conversion formula:\n\\[APR = F \\cdot (Gross Compounded Return^{\\frac{1}{T \\cdot F}} - 1) \\]\nwhere\n\nGross Compounded Return = bond total proceeds (including reinvested coupons) divided by price paid for bond\nT is number of years the bond was held, and F is the number of coupons (or compounding periods) per year\n\nExamples 4,5,6:\nGo back to examples 4, 5, and 6, and calculate the realized return of the bond (expressed as an APR) in each example.\n\n# \n#\n#\n#\n#\n#\n#\n#\n# \n#\n#\n#\n#\n#\n#\n#\n\nThe examples above illustrate a very important point: the YTM of a bond tells you the return you will actually get from the bond ONLY if you reinvest the coupons at a rate equal to the YTM"
  },
  {
    "objectID": "teaching/FIN 421/lectures/L02_Foundations_1_Past_Returns.html",
    "href": "teaching/FIN 421/lectures/L02_Foundations_1_Past_Returns.html",
    "title": "L02: Analyzing past returns",
    "section": "",
    "text": "We discuss several key quantities we frequently use to describe a stock’s performance (usually in a past period):\n\nHolding period return\nTotal return (compounding)\nGeometric average of returns\nReal returns (adjusting for inflation)\n\nWe then work through a real world application where we use the above quantities to describe the performance of Tesla (TSLA) over the past 5 years."
  },
  {
    "objectID": "teaching/FIN 421/lectures/L02_Foundations_1_Past_Returns.html#example",
    "href": "teaching/FIN 421/lectures/L02_Foundations_1_Past_Returns.html#example",
    "title": "L02: Analyzing past returns",
    "section": "Example",
    "text": "Example\nSuppose TSLA was selling for $420 per share at the beginning of 2018 and for $500 per share at the end of 2019. Suppose TSLA also paid a dividend of $10 per share at the end of 2019. Calculate the HPR, capital gains, and dividend yield for TSLA over this two-year period.\n\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#"
  },
  {
    "objectID": "teaching/FIN 421/lectures/L02_Foundations_1_Past_Returns.html#example-1",
    "href": "teaching/FIN 421/lectures/L02_Foundations_1_Past_Returns.html#example-1",
    "title": "L02: Analyzing past returns",
    "section": "Example",
    "text": "Example\nSuppose over the last three months, TSLA had net returns of 10%, 5% and -3%. What was the total return on TSLA over this past three-month period?\n\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#"
  },
  {
    "objectID": "teaching/FIN 421/lectures/L02_Foundations_1_Past_Returns.html#example-2",
    "href": "teaching/FIN 421/lectures/L02_Foundations_1_Past_Returns.html#example-2",
    "title": "L02: Analyzing past returns",
    "section": "Example",
    "text": "Example\nUsing the same data as in Example 3.1., what was the geometric average return of TSLA over the past three-month period?\n\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#"
  },
  {
    "objectID": "teaching/FIN 421/lectures/L02_Foundations_1_Past_Returns.html#example-3",
    "href": "teaching/FIN 421/lectures/L02_Foundations_1_Past_Returns.html#example-3",
    "title": "L02: Analyzing past returns",
    "section": "Example",
    "text": "Example\nUsing the same data as in Example 3.1., assuming that the inflation rate was 0.1% in each of the past 3 months, calculate the real total return on TSLA over the past three months.\n\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#"
  },
  {
    "objectID": "teaching/FIN 421/lectures/Lecture 17 Data Processing.html",
    "href": "teaching/FIN 421/lectures/Lecture 17 Data Processing.html",
    "title": "L17: Valuation data processing",
    "section": "",
    "text": "Valuation overview\nWe will learn how to value a firm (MSFT in our examples) using three different methods:\n\nMultiples\nDividend Discount Model (DDM)\nDiscounted Cash Flow (DCF) analysis\n\nBefore we start, we need to gather some data (instructions on how to get it below). All the files we download will go into a “Data” folder that you should create in the same folder where you saved these lecture notes.\n\n\nDownloading financial statements\nWe will download financial statements from\nhttps://www.morningstar.com/\nFor this purpose, you must create a free account with Morningstar first. Then:\n\nInput the ticker of your firm in “Search Quotes and Site” at the top-left of the page.\nClick “Financials”\nClick “Expand Detail View”\nClick “Export Data” (towards the right of the screen). This will download the Income Statement.\nTo download the balance sheet, select “Balance Sheet” and then hit “Export Data”.\nFollow the same process for the Cash Flow statement.\n\nWe will create a “Valuation” excel file, and store each of these financial statements into a separate sheet in that file.\n\n\nIndustry-level data\nThese data come from Aswath Damodaran at NYU. If you want to download these data yourself, go to\nhttp://pages.stern.nyu.edu/~adamodar/\nThen, get the following files:\n\nFor “multiples” valuation, we will use data from the following files\n\nThe file that starts with “PE Ratios, …” (once downloaded, it will be called “pedata”)\nThe file that starts with “Price and Enterprise Value to Book Ratios …” (once downloaded, it will be called “pbdata”)\nThe file that starts with “Enterprise Value/EBIT …” (once downloaded, it will be called “vebitda”)\n\nFor DDM valuation, we will use data from the following files:\n\nThe file that starts with “Historical growth rate in EPS, …” (once downloaded, it will be called “histgr”)\n\nThe file that starts with “Fundamental growth rate in EPS, …” (once downloaded, it will be called “fundgr”)\n\nFor DCF valuation, we will use data from:\n\nThe “Cost of capital by industry” file (once downloaded, it will be called “wacc”)\n\n\n\n\nCompetitor data\nTo find a firm’s competitors (according to Morningstar), go to the firm’s Morningstar page (as explained above) and hit the “Quote” tab. Then hit “Competitors” (towards the middle of the page).\nFor MSFT, this should show GOOG (Alphabet), ORCL (Oracle), and CRM (Salesforce) as the competitors.\nHit “More Competitors Data” and you will see their Price/Earnings (P/E) and Price/Book (P/B) ratios.\nTo obtain more ratios (in particular, we will need Enterprise Value / EBITDA), we need to go to each competitor’s Morningstar page and hit the “Valuation” tab.\nWe will store each of these competitor ratios into a “Comps” sheet in the “Valuation” file we created above.\n\n\nReturns data\nWe will need to estimate MSTF’s cost of equity. For this purpose, we need data on MSFT returns, and the returns to the Fama-French factors. We have gathered data like this several times in class, so I will not make you do it again. The “Data” subforder found together with these lecture notes on D2L contains a file named “msft_returns” that has all the data you need to estimate MSFTs cost of equity."
  },
  {
    "objectID": "teaching/FIN 421/lectures/Lecture 26 on Option Strategies.html",
    "href": "teaching/FIN 421/lectures/Lecture 26 on Option Strategies.html",
    "title": "L26: Risk management with options",
    "section": "",
    "text": "Risk management with options\nOptions can be used to increase the risk of your portfolio (in hopes of a higher return) as well as to decrease the risk of your portfolio (at the expense of a lower return). Below, we work through a couple of examples that show how this can be achieved.\nExample 1: Using options to increase risk\nThis usually involves buying option on stocks instead of buying/shorting the stocks themselves.\nTo see the difference, consider the following two strategies:\n\nStategy A: invest $9000 in stock A by buying 100 shares at $90 each\nStategy B: Use the $9000 to buy 900 call options on stock A (trading at $10 an option). The strike price is $90 and the options expire in a year.\n\nCalculate what your (%) return would be for each strategy, if the price a year from now is:\n\n$75\n$90\n$105\n\n\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n\nExample 2: Using options to decrease risk\nYou can limit your losses from buying a particular stock by also purchasing put options on that stock. This is called the protective put strategy.\nTo see how this works, consider the following two strategies:\n\nStrategy A: buy one share of AMZN for $100\nStrategy B: you buy one share of AMZN for $100 and a put option on AMZN that costs you an additional $5. The put option has a strike price of $95 and it expires in 3 months.\n\nQ1: What is your % return for each strategy, if the price of AMZN 3 months from now is:\n\n$0\n$95\n$105\n$150\n\n\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n\nQ2: Draw the payoff and profit for this protective put, as functions of the price of AMZN\n\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n\n\n\nVolatility bets with options\nYou can use options to trade based on your opinions about volatility. Below are two examples that show you how this would work.\nExample 3: Betting on volatility\nYou buy a call option on stock X for $10 an option and you also buy a put option on stock X for $5 an option. Both options have strike price of $100. For what values of the price of the underlying asset X would you make a positive profit? Draw the payoff and profit of this strategy as a function of the underlying stock price.\n\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n\nExample 4: Betting against volatility\nYou sell a call option on stock X for $10 an option and you also sell a put option on stock X for $5 an option. Both options have strike price of $100. For what values of the price of the underlying asset X would you make a positive profit? Draw the payoff and profit of this strategy as a function of the underlying stock price.\n\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#"
  },
  {
    "objectID": "teaching/FIN 421/lectures/Lecture 13.html",
    "href": "teaching/FIN 421/lectures/Lecture 13.html",
    "title": "L13: Cost of equity",
    "section": "",
    "text": "Cost of equity using CAPM\nUnder the CAPM assumptions, we have:\n\\[E[R_{i}] = R_{f} +  \\beta_i (E[R_{m}] - R_{f}) \\]\nTherefore, to estimate the cost of equity for any firm \\(i\\), using the CAPM we need:\n\nThe current risk free rate\n\nIt is best to try to match the horizon of the risk-free rate with the horizon over which the cost of equity applies\n\nExample: if you want to calculate the cost of equity over the next year, use the yield on a 1-year T-bill as the risk-free rate\nExample: if you want to calculate the cost of equity over the next 10 years, use the yield on a 10-year T-bill as the risk-free rate\n\n\nThe firm’s market beta\n\nRegress past excess returns of the firm on past excess returns on the market\n\nAn estimate for the market risk premium\n\nTake an average of excess returns on the market (use at least 20 years of data)\n\n\n\n\nThe Fama-French three factor model\nThe Fama-French three factor model adds two more systematic risk factors in addition to the market return from the single-factor model:\n\\[R_{i,t} - R_{f,t} = \\alpha_i + \\beta_{i,m} (R_{m,t} - R_{f,t}) + \\beta_{i,smb} R_{smb} + \\beta_{i,hml} R_{hml} + \\epsilon_{i,t}\\]\nwhere: - \\(R_{smb}\\) is the return on a portfolio that is long on the smallest firms in the economy and short on the largest firms - \\(R_{hml}\\) is the return on a portfolio that is long on the firms with highest B/M ratios and short on the firms with the lowest B/M ratios - B/M = book value of equity divided by market value of equity - High B/M firms are called “value” firms - Low B/M firms are called “growth” firms\n\n\nCost of equity using the Fama-French three factor model\nUnder the Fama-French model, we have:\n\\[E[R_{i}] = R_{f} +  \\beta_{i,m} (E[R_{m}] - R_{f}) + \\beta_{i,smb} E[R_{smb}] + \\beta_{i,hml} E[R_{hml}] \\]\nTherefore, to estimate the cost of equity for any firm \\(i\\), using the Fama-French factor we also need\n\nThe firm’s market, SMB, and HML betas\n\nRegress past excess returns of the firm on past excess returns on the market and the returns on the SMB and HML factors\n\nAn estimate for the SMB and HML risk premia\n\nTake an average of the returns of the SMB and HML factors (use at least 20 years of data)\n\n\n\n\nApplication\nCalculate the cost of equity for AAPL using both the CAPM and the Fama-French three factor model. Use use monthly data since 2010 for AAPL.\n\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n\n\n\nApplication\nRecalculate the risk premia of all three risk factors using only data since 1990. Now recalculate the cost of equity of AAPL using these new risk-premia estimates.\n\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#"
  },
  {
    "objectID": "teaching/FIN 421/lectures/Lecture 24 Sensitivity Analysis.html",
    "href": "teaching/FIN 421/lectures/Lecture 24 Sensitivity Analysis.html",
    "title": "L24: Valuation sensitivity analysis",
    "section": "",
    "text": "The need for sensitivity analysis\nOur valuation results suggest that there is quite a bit of variance in our estimates of the value of MSFT. We have to remember that, even though we obtained many different estimates for this value, each of those estimates relies on some pretty strict assumptions that could easily not hold in the future.\nThe purpose of a sensitivity analysis is to obtain an idea of just how much our estimates vary when we change some of the crucial assumptions behind our valuation methods.\nThe showcase a sensitivity analysis, we use one of the valuation methods above: the DCF based on FCFE approach, and we rerun the valuation changing two crucial parameters we used for it:\n\nThe perpetual growth rate (g) and\nThe cost of capital (r_e)\n\nThese parameters: the perpetual growth rate and the discount rate, have the highest impact on any DCF and DDM valuation.\nTo perform the sensitivity analysis in Excel:\n\nWe create a table where\n\nthe columns are the different cost of capital values we want to use\nthe rows are the different growth rates we want to use\n\nIn the top left corner of the table, we link to the valuation cell from the original analysis\nSelect the entire table\nGo to Data -&gt; What-if analysis -&gt; Data Table\n\nfor “Row Cell” select the cost of equity cell in the original analysis\nfor “Column Cell” select the terminal growth rate cell in the original analysis\n\n\n\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#"
  },
  {
    "objectID": "teaching/FIN 421/lectures/Lecture 25 Option Intro.html",
    "href": "teaching/FIN 421/lectures/Lecture 25 Option Intro.html",
    "title": "L25: Options intro",
    "section": "",
    "text": "Options are a type of financial asset called derivative securities. The derivative means that the value of these assets depends on the value of a different asset (often called the “underlying” asset)."
  },
  {
    "objectID": "teaching/FIN 421/lectures/Lecture 25 Option Intro.html#call-option-payoff-and-profit",
    "href": "teaching/FIN 421/lectures/Lecture 25 Option Intro.html#call-option-payoff-and-profit",
    "title": "L25: Options intro",
    "section": "Call option payoff and profit",
    "text": "Call option payoff and profit\nThe payoff (revenue) of a call option that you own is the difference between the underlying stock price (let’s call it S) and the option’s strike price (let’s call it X) when \\(S &gt; X\\) and 0 otherwise (since you wouldn’t exercise it when \\(S &lt; X\\)).\n\\[Payoff = max(S - X, 0)\\]\nThe profit from the call option equals its payoff minus it premium (i.e. the price you paid to buy the option).\n\\[Profit = max(S - X, 0) - Premium\\]\nExample 2:\nYou buy a call option on stock A for a premium of $14. The strike price is $80 and the option expires in a month.\nQ1: What is the payoff from this call option if the price of stock A is $60? How about $70? $80? $90? $100?\nQ2: What is the profit from the call option in each of those scenarios?\n\n#\n#\n#\n#\n#\n#\n\nQ3: Draw a graph that shows your payoff (revenue) and your profit as a function of the price of stock A.\n\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#"
  },
  {
    "objectID": "teaching/FIN 421/lectures/Lecture 25 Option Intro.html#selling-writing-call-options",
    "href": "teaching/FIN 421/lectures/Lecture 25 Option Intro.html#selling-writing-call-options",
    "title": "L25: Options intro",
    "section": "Selling (writing) call options",
    "text": "Selling (writing) call options\nIf you sell (aka “write”) a call option, you are in the opposite position from the person who buys it: you give them the option (but not the obligation) to buy the underlying asset from you at a given strike price. If they decide to exercise that option, you (as the seller of the option) are obligated to sell them the underlying stock at the strike price (even though it might be trading at a higher price on the market).\nThe payoff (revenue) of a call option that you sell is the negative of the payoff to the person that bought it:\n\\[Payoff = - max(S - X, 0)\\]\nThe profit from selling a call option equals the negative of the profit to the person that bought it:\n\\[Profit = - max(S - X, 0) + Premium\\]\nExample 3:\nYou sell a call option on stock A for a premium of $10. The strike price is $50 and the option expires in a month.\nQ1: What is your payoff from this call option if the price of stock A is $40? How about $50? $60? $70?\nQ2: What is your profit from the call option in each of those scenarios?\n\n#\n#\n#\n#\n#\n#\n\nQ3: Draw a graph that shows your payoff (revenue) and your profit as a function of the price of stock A.\n\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#"
  },
  {
    "objectID": "teaching/FIN 421/lectures/Lecture 25 Option Intro.html#put-option-payoff-and-profit",
    "href": "teaching/FIN 421/lectures/Lecture 25 Option Intro.html#put-option-payoff-and-profit",
    "title": "L25: Options intro",
    "section": "Put option payoff and profit",
    "text": "Put option payoff and profit\nThe payoff (revenue) of a put option that you own is the difference between the option’s strike price (let’s put it X) and the underlying stock price (let’s put it S) when \\(X &gt; S\\) and 0 otherwise (since you wouldn’t exercise it when \\(X &lt; S\\)).\n\\[Payoff = max(X - S, 0)\\]\nThe profit from the put option equals its payoff minus it premium (i.e. the price you paid to buy the option).\n\\[Profit = max(X - S, 0) - Premium\\]\nExample 5:\nYou buy a put option on stock A for a premium of $10. The strike price is $40 and the option expires in a month.\nQ1: What is the payoff from this put option if the price of stock A is $60? How about $40? $20? $0?\nQ2: What is the profit from the put option in each of those scenarios?\n\n#\n#\n#\n#\n#\n#\n\nQ3: Draw a graph that shows your payoff (revenue) and your profit as a function of the price of stock A.\n\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#"
  },
  {
    "objectID": "teaching/FIN 421/lectures/Lecture 25 Option Intro.html#selling-writing-put-options",
    "href": "teaching/FIN 421/lectures/Lecture 25 Option Intro.html#selling-writing-put-options",
    "title": "L25: Options intro",
    "section": "Selling (writing) put options",
    "text": "Selling (writing) put options\nIf you sell (aka “write”) a put option, you are in the opposite position from the person who buys it: you give them the option (but not the obligation) to sell the underlying asset to you at a given strike price. If they decide to exercise that option, you (as the seller of the option) are obligated to buy the underlying stock from them at the strike price (even though it might be trading at a lower price on the market).\nThe payoff (revenue) of a put option that you sell is the negative of the payoff to the person that bought it:\n\\[Payoff = - max(X - S, 0)\\]\nThe profit from selling a put option equals the negative of the profit to the person that bought it:\n\\[Profit = - max(X - S, 0) + Premium\\]\nExample 6:\nYou sell a put option on stock A for a premium of $10. The strike price is $60 and the option expires in a month.\nQ1: What is your payoff from this put option if the price of stock A is $1? How about $50? $60? $70? 80?\nQ2: What is your profit from the put option in each of those scenarios?\n\n#\n#\n#\n#\n#\n#\n\nQ3: Draw a graph that shows your payoff (revenue) and your profit as a function of the price of stock A.\n\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#"
  },
  {
    "objectID": "teaching/FIN 421/lectures/Lecture 10 11.html",
    "href": "teaching/FIN 421/lectures/Lecture 10 11.html",
    "title": "L10_11: Statistical models of returns",
    "section": "",
    "text": "We have seem in previous lectures that some portion of a stock’s total risk can be diversified away when the stock is part of a larger portfolio. This portion is often referred to as the stock’s idiosyncratic risk. The remaining portion, which can not be diversified away is often called its systematic risk.\nThe single-factor model is a statistical model which assumes that all sources of systematic risk (economic factors that affect all firms to some extent) are captured by the returns on the market portfolio. If this is the case, then firm excess returns can be written as a linear function of market excess returns (for every firm \\(i\\) and time \\(t\\)):\n\\[R_{i,t} - R_{f,t} = \\alpha_i + \\beta_i (R_{m,t} - R_{f,t}) + \\epsilon_{i,t}\\]\nwhere: - \\(R_{i,t}\\) is the return of firm \\(i\\) at time \\(t\\) - \\(R_{m,t}\\) is the return of the market \\(i\\) at time \\(t\\) - \\(R_{f,t}\\) is the risk-free rate at time \\(t\\)\n\n\\(\\beta_i (R_{m,t} - R_{f,t})\\) is the portion of the firm’s return that is caused by systematic shocks\n\\(\\epsilon_{i,t}\\) is the portion of the firm’s return that is caused by idiosyncratic (firm-specific) shocks\n\\(\\beta_i\\) tells us if the firm has more or less systematic risk than the average firm in the economy (average \\(\\beta\\) = 1)\n\\(\\alpha_i\\) tells us if the firm has offered a higher or lower return (on average) than what was warranted by its exposure to the market (its \\(\\beta\\))\n\nIf markets are efficient and the market perfectly captures all systematic shocks, then \\(\\alpha\\) should be zero for all firms at all times"
  },
  {
    "objectID": "teaching/FIN 421/lectures/Lecture 10 11.html#interpreting-the-results-coefficients-t-statistics-and-pvalues",
    "href": "teaching/FIN 421/lectures/Lecture 10 11.html#interpreting-the-results-coefficients-t-statistics-and-pvalues",
    "title": "L10_11: Statistical models of returns",
    "section": "Interpreting the results: coefficients, t-statistics, and pvalues",
    "text": "Interpreting the results: coefficients, t-statistics, and pvalues\nThe “Intercept” row contains information about the firm’s \\(\\alpha\\) and the “Mkt-RF” row contains information about the firm’s \\(\\beta\\). The \\(\\alpha\\) and \\(\\beta\\) coefficient estimates themselves are in the “Coefficients” column.\nThe t-statistics for the two coefficients are in the “t Stat” column. Loosely speaking a t-statistic that larger than 2 or smaller than -2 allows us to conclude that the corresponding coefficient is statistically different from 0 (i.e. reject the null hypothesis that the coefficient is 0).\nThe p-values are in the “P-value” column. P-values lower than 0.05 allow us to conclude that the corresponding coefficient is statistically different from 0 with the 5% confidence level (i.e. reject the null hypothesis that the coefficient is 0).\nThe next two columns after the p-value give us the 95% confidence interval for each coefficient.\nTSLA’s alpha has a p-value of that is smaller than 5% so we can conclude that it’s alpha is statistically significantly different from 0 at the 95% confidence level (but not at the 99% confidence level). The 5% is a common “significance level” used in hypothesis testing (also called the Type 1 error, or, confusingly for us, the alpha of the test). One minus the significance level is called the “confidence level” of the test.\nThe fact that the alpha is positive (and statistically different from 0) means that, based on the single-index model, TSLA seems to be undervalued. A negative alpha would mean the stock in overvalued.\nIf we can not reject the null hypothesis that alpha is 0, the conclusion is NOT that alpha = 0 and therefore the stock is correctly valued (since we can never “accept” a null hypothesis, we can only fail to reject). The conclusion is that we do not have enough evidence to claim that the stock is either undervalued or overvalued (which is not the same thing as saying that we have enough evidence to claim that the stock is correctly valued)."
  },
  {
    "objectID": "teaching/FIN 421/lectures/Lecture 20 and 21 on DDM Valuation.html",
    "href": "teaching/FIN 421/lectures/Lecture 20 and 21 on DDM Valuation.html",
    "title": "L20_21: Dividend discount models",
    "section": "",
    "text": "The intrinsic value of the firm’s equity at time \\(t\\) (call it \\(V_t\\)) is the discounted value of all (expected) future dividends:\n\\[V_t = \\frac{D_{t+1}}{1+r} + \\frac{D_{t+2}}{(1+r)^2} + \\frac{D_{t+3}}{(1+r)^3} + ...\\]\nor, equivalently\n\\[V_t = \\frac{D_{t+1}}{1+r} + \\frac{D_{t+2}}{(1+r)^2} + \\frac{D_{t+3}}{(1+r)^3} + ... + \\frac{D_{t+H} + P_{t+H}}{(1+r)^H}\\]\nwhere \\(P_{t+H}\\) (the price \\(H\\) periods from now) is often called the “terminal value” of the stock.\nThe discount rate \\(r\\) (sometimes called the market capitalization rate) is the expected return on the stock (or, equivalently, the firm’s cost of equity).\nAs we have seen, this is most commonly estimated using CAPM:\n\\[r = R_f + \\beta \\cdot (E[R_m] - R_f)\\]\nbut other models may be used (e.g. the Fama-French Three-Factor model)."
  },
  {
    "objectID": "teaching/FIN 421/lectures/Lecture 20 and 21 on DDM Valuation.html#estimating-the-terminal-growth-rate",
    "href": "teaching/FIN 421/lectures/Lecture 20 and 21 on DDM Valuation.html#estimating-the-terminal-growth-rate",
    "title": "L20_21: Dividend discount models",
    "section": "Estimating the terminal growth rate",
    "text": "Estimating the terminal growth rate\nThe terminal growth rate will have a significant influence on our results, so we have to be very careful when we estimate it. A common aproach is to use a “fundamental growth rate” (aka the “sustainable growth rate”) estimate:\n\\[g = ROE \\cdot b \\]\nwhere b is the retention ratio (aka plowback ratio):\n\\[b = Retention Ratio = 1 - Dividend Payout Ratio = 1 - Dividends/Net Income\\]\nOnce again, we can use either the fundamental growth rate of the firm itself, or an average of the fundamental growth rate of close competitors, or for the industry as a whole.\nAnother common approach is to assess how you think the firm will grow relative to the economy as a whole, and base your estimate of \\(g\\) on an estimate of the perpetual growth rate in GDP (e.g. the US GDP has grown by an average of 3.1% per year in the past 70 years).\nFinally, we can use an average of past dividend growth rates, either for the firm itself or for comparable firms.\nExample 2:\nAssume the MSFT just paid a dividend of $2 per share, and that these dividends will grow at 30% for the next 5 years, and after that, at 6% per year in perpetuity. You have estimated that MSFT has a market beta of 1.1, and the market risk premium is 7% per year. The yield on a 1-year Tbill is 0.001. What is the estimated intrinsic value of on share of MSFT?\n\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#"
  },
  {
    "objectID": "teaching/FIN 421/lectures/Lecture 22 and 23 on DCF Valuation.html",
    "href": "teaching/FIN 421/lectures/Lecture 22 and 23 on DCF Valuation.html",
    "title": "L22_23: Discounted cash flow analysis",
    "section": "",
    "text": "In this approach, we estimate the value of the firm’s assets by discounting the firm’s free cash flow to the firm (FCFF) at the firm’s weighted average cost of capital (WACC):\n\\[FirmValue_t = \\frac{FCFF_{t+1}}{1+WACC} + \\frac{FCFF_{t+2}}{(1+WACC)^2} + \\frac{FCFF_{t+3}}{(1+WACC)^3} + ...\\]\nTo obtain the intrinsic value of of the firm’s equity, we subtract the book value of debt (as an approximation of its market value of debt) from the \\(FirmValue\\) estimate obtained as above.\nAs we have seen in the DDM lecture, since we can not calculate infinite sums, in practice we often assume that, after some horizon \\(H\\), the firm’s FCFF grows at a constant rate \\(g\\) for ever. That allow us to estimate the terminal value of the FCFFs at time \\(t+H\\) as:\n\\[P_{t+H} = \\frac{FCFF_{t+H}(1+g)}{(WACC - g)}\\]\nAlternatively, this terminal value can also be estimated using multiples (e.g. EV/EBITDA).\nOnce the terminal value is estimated, the valuation formula above becomes:\n\\[FirmValue_t = \\frac{FCFF_{t+1}}{1+WACC} + \\frac{FCFF_{t+2}}{(1+WACC)^2} + ... + \\frac{FCFF_{t+H}}{(1+WACC)^{t+H}} + \\frac{P_{t+H}}{(1+WACC)^{t+H}}\\]\nThe intermediate FCFFs (i.e. \\(FCFF_{t+1}\\) up to \\(FCFF_{t+H}\\)) can be estimated either using pro-forma statements (as you learned in FIN 421) or (the approach taken here) by applying particular growth rates (which we estimate below) to the latest FCFF (i.e. \\(FCFF_{t}\\)).\n\n\nThere are several definitions for FCFF, but the textbook one is:\n\\[FCFF = EBIT(1 - T_c) + Depreciation - Capital Expenditures - ChangeNWC\\]\nand \\(T_c\\) is the firm’s marginal tax rate, and \\(ChangeNWC\\) is the change in the firm’s net working capital.\nIn practice, we often don’t have data on the firm’s marginal tax rate and have to use its average tax rate instead (tax expense divided by taxable income).\nWhen we calculate net working capital, we commonly ignore cash and interest-bearing debt (i.e. NWC = (Current assets - Cash) - (Current Liabilities - Current debt) or NWC = Inventory + Accounts Receivable - Accounts Payable) but some analysts also use current assets minus current liabilities.\n\n\n\nDenote by D and E the total debt and total equity of the firm respectively. Let \\(r_d\\) denote the firm’s cost of debt and \\(r_e\\) its cost of equity. Then the firm’s WACC is calculated as:\n\\[WACC = \\frac{D}{D+E} r_d (1 - T_c) + \\frac{E}{D+E} r_e\\]\nAbove, D is usually measured as the book value of long-term debt (but we should use market value of debt if possible), and E is the market value of common equity.\nFor the cost of debt \\(r_d\\), we can use interest expense divided by long-term debt but a better approach is to find out if the firm has issued bonds and use the yield on those bonds as the firm’s cost of debt.\nThe cost of equity \\(r_e\\) (i.e. the expected return on the stock), we usually use either CAPM or the Fama-French three factor model.\nExample 1:\nSuppose in the latest fiscal year, MSFT had EBIT of $100 (all values are in billions), depreciation of $10, capital expenditures of $20, a change in net working capital (NWC) of $5, total debt of $70 and 2 billion shares outstanding . Assume the tax rate is 30%. You believe the firm’s FCFFs will grow at 10% per year for the next 5 years, and at 4% per year thereafter. The firm has a cost of debt of 3% and a CAPM beta of 1.2. The current price of a share of MSFT is $200, and the current risk free rate is 0.001. Assume the market risk-premium is 6%. What is the intrinsic value of one share of MSFT as of the end of the latest fiscal year?\n\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#"
  },
  {
    "objectID": "teaching/FIN 421/lectures/Lecture 22 and 23 on DCF Valuation.html#free-cash-flow-to-the-firm-fcff",
    "href": "teaching/FIN 421/lectures/Lecture 22 and 23 on DCF Valuation.html#free-cash-flow-to-the-firm-fcff",
    "title": "L22_23: Discounted cash flow analysis",
    "section": "",
    "text": "There are several definitions for FCFF, but the textbook one is:\n\\[FCFF = EBIT(1 - T_c) + Depreciation - Capital Expenditures - ChangeNWC\\]\nand \\(T_c\\) is the firm’s marginal tax rate, and \\(ChangeNWC\\) is the change in the firm’s net working capital.\nIn practice, we often don’t have data on the firm’s marginal tax rate and have to use its average tax rate instead (tax expense divided by taxable income).\nWhen we calculate net working capital, we commonly ignore cash and interest-bearing debt (i.e. NWC = (Current assets - Cash) - (Current Liabilities - Current debt) or NWC = Inventory + Accounts Receivable - Accounts Payable) but some analysts also use current assets minus current liabilities."
  },
  {
    "objectID": "teaching/FIN 421/lectures/Lecture 22 and 23 on DCF Valuation.html#weighted-average-cost-of-capital-wacc",
    "href": "teaching/FIN 421/lectures/Lecture 22 and 23 on DCF Valuation.html#weighted-average-cost-of-capital-wacc",
    "title": "L22_23: Discounted cash flow analysis",
    "section": "",
    "text": "Denote by D and E the total debt and total equity of the firm respectively. Let \\(r_d\\) denote the firm’s cost of debt and \\(r_e\\) its cost of equity. Then the firm’s WACC is calculated as:\n\\[WACC = \\frac{D}{D+E} r_d (1 - T_c) + \\frac{E}{D+E} r_e\\]\nAbove, D is usually measured as the book value of long-term debt (but we should use market value of debt if possible), and E is the market value of common equity.\nFor the cost of debt \\(r_d\\), we can use interest expense divided by long-term debt but a better approach is to find out if the firm has issued bonds and use the yield on those bonds as the firm’s cost of debt.\nThe cost of equity \\(r_e\\) (i.e. the expected return on the stock), we usually use either CAPM or the Fama-French three factor model.\nExample 1:\nSuppose in the latest fiscal year, MSFT had EBIT of $100 (all values are in billions), depreciation of $10, capital expenditures of $20, a change in net working capital (NWC) of $5, total debt of $70 and 2 billion shares outstanding . Assume the tax rate is 30%. You believe the firm’s FCFFs will grow at 10% per year for the next 5 years, and at 4% per year thereafter. The firm has a cost of debt of 3% and a CAPM beta of 1.2. The current price of a share of MSFT is $200, and the current risk free rate is 0.001. Assume the market risk-premium is 6%. What is the intrinsic value of one share of MSFT as of the end of the latest fiscal year?\n\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#"
  },
  {
    "objectID": "teaching/FIN 421/lectures/Lecture 22 and 23 on DCF Valuation.html#free-cash-flow-to-equity-fcfe",
    "href": "teaching/FIN 421/lectures/Lecture 22 and 23 on DCF Valuation.html#free-cash-flow-to-equity-fcfe",
    "title": "L22_23: Discounted cash flow analysis",
    "section": "Free cash flow to equity (FCFE)",
    "text": "Free cash flow to equity (FCFE)\nThere are several definitions for FCFE, but the textbook one is:\n\\[FCFE = FCFF - Interest (1 - T_c) + Debt Issuance\\]\nwhere Debt Issuance is positive if long-term debt increased from last year, and negative if it decreased.\nExample 2:\nSuppose in the latest fiscal year, MSFT had EBIT of $100 (all values are in billions), depreciation of $10, capital expenditures of $20, a change in net working capital (NWC) of $5, total debt of $70 and 2 billion shares outstanding. In the latest fiscal year, the firm paid $10 in interest expense and in the year prior, it had total debt of $50. Assume the tax rate is 30%. You believe the firm’s FCFEs will grow at 10% per year for the next 5 years, and at 4% per year thereafter. The firm has a CAPM beta of 1.2 and the current risk free rate is 0.001. Assume the market risk-premium is 6%. What is the intrinsic value of one share of MSFT as of the end of the latest fiscal year?\n\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#"
  },
  {
    "objectID": "teaching/FIN 421/lectures/L04_Portfolio_Theory_1_Mechanics.html",
    "href": "teaching/FIN 421/lectures/L04_Portfolio_Theory_1_Mechanics.html",
    "title": "L04: Portfolio theory intro",
    "section": "",
    "text": "Every portfolio is described by two pieces of information:\n\nThe identity of the assets in that portfolio\nThe weight that each asset has in the portfolio\n\nWe learn how to describe the returns of a portfolio in terms of its weights and the returns of its assets:\n\nCalculate portfolio realized returns\nCalculate the correlation and covariance between the returns of two assets\nCalculate portfolio expected returns\nCalculate portfolio total risk"
  },
  {
    "objectID": "teaching/FIN 421/lectures/L04_Portfolio_Theory_1_Mechanics.html#example",
    "href": "teaching/FIN 421/lectures/L04_Portfolio_Theory_1_Mechanics.html#example",
    "title": "L04: Portfolio theory intro",
    "section": "Example",
    "text": "Example\nSuppose today Facebook (FB) is selling for $200 per share and Netflix (NFLX) is selling for $400 per share. You buy two shares of FB and one share of NFLX. Over the next month, FB has a return of 10% and NFLX has a return of 20%. The month after that, FB has a return of -10% and NFLX has a return of -20%. What will be the return of the portfolio over each month? What will the return of your portfolio be over this entire two-month period?\n\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#"
  },
  {
    "objectID": "teaching/FIN 421/lectures/L04_Portfolio_Theory_1_Mechanics.html#example-1",
    "href": "teaching/FIN 421/lectures/L04_Portfolio_Theory_1_Mechanics.html#example-1",
    "title": "L04: Portfolio theory intro",
    "section": "Example",
    "text": "Example\nSuppose over the past three months FB had returns of 10%, 10%, and -8%, and NFLX had returns of 20%, -10% and -1%. Calculate the sample covariance and correlation of the returns of FB and NFLX.\n\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#"
  },
  {
    "objectID": "teaching/FIN 421/lectures/L04_Portfolio_Theory_1_Mechanics.html#example-2",
    "href": "teaching/FIN 421/lectures/L04_Portfolio_Theory_1_Mechanics.html#example-2",
    "title": "L04: Portfolio theory intro",
    "section": "Example",
    "text": "Example\nSuppose over the past three months FB had returns of 10%, 10%, and -8%, and NFLX had returns of 20%, -10% and -1%. Today, you invest $1000 in FB and $4000 in NFLX. What is the expected return of your portfolio?\n\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#"
  },
  {
    "objectID": "teaching/FIN 421/lectures/L04_Portfolio_Theory_1_Mechanics.html#example-3",
    "href": "teaching/FIN 421/lectures/L04_Portfolio_Theory_1_Mechanics.html#example-3",
    "title": "L04: Portfolio theory intro",
    "section": "Example",
    "text": "Example\nSuppose over the past three months FB had returns of 10%, 10%, and -8%, and NFLX had returns of 20%, -10% and -1%. Today, you invest $1000 in FB and $4000 in NFLX. What is the total risk of your portfolio?\n\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#"
  },
  {
    "objectID": "teaching/FIN 421/lectures/L05_Portfolio_Theory_2_Optimal_Capital_Allocation.html",
    "href": "teaching/FIN 421/lectures/L05_Portfolio_Theory_2_Optimal_Capital_Allocation.html",
    "title": "L05: Optimal capital allocation",
    "section": "",
    "text": "The capital allocation line\n\nAll portfolios containing a risky asset and a risk-free asset lie on the same line in the mean-volatility space\n\nOptimal capital allocation\n\nFor a given level of risk aversion, what is the optimal proportion of your funds that you should invest in risky assets?"
  },
  {
    "objectID": "teaching/FIN 421/lectures/L05_Portfolio_Theory_2_Optimal_Capital_Allocation.html#example",
    "href": "teaching/FIN 421/lectures/L05_Portfolio_Theory_2_Optimal_Capital_Allocation.html#example",
    "title": "L05: Optimal capital allocation",
    "section": "Example",
    "text": "Example\nAssume you have estimated that the expected return on TSLA is 10% and the standard deviation of its future returns is 12%. The risk-free rate is 1%. In this example, we will create 21 different capital allocations containing TSLA and the risk-free asset: the first will have a weight of 0% in TSLA, the next will have a weight of 10% in TSLA and so on (in increments of 10%) until the 21st capital allocation, which has 200% in TSLA. Calculate the expected returns and standard deviations of the 21 complete portfolios obtained from these capital allocations. Plot these portfolios in mean-volatility space.\n\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#"
  },
  {
    "objectID": "teaching/FIN 421/lectures/L05_Portfolio_Theory_2_Optimal_Capital_Allocation.html#example-1",
    "href": "teaching/FIN 421/lectures/L05_Portfolio_Theory_2_Optimal_Capital_Allocation.html#example-1",
    "title": "L05: Optimal capital allocation",
    "section": "Example",
    "text": "Example\nAssume you have estimated that the expected return on TSLA is 5.6% and the standard deviation of its future returns is 18.6%. The risk-free rate is 0.1%. Your coefficient of risk aversion if 4. What is your optimal capital allocation?\n\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#"
  },
  {
    "objectID": "teaching/FIN 421/lectures/L05_Portfolio_Theory_2_Optimal_Capital_Allocation.html#discussion",
    "href": "teaching/FIN 421/lectures/L05_Portfolio_Theory_2_Optimal_Capital_Allocation.html#discussion",
    "title": "L05: Optimal capital allocation",
    "section": "Discussion",
    "text": "Discussion\nThe optimal capital allocation tells you the right proportion of your funds to invest in a given risky asset P. By “given” we mean you (or your client) have already decided what that asset (or portfolio) P is. We will soon talk more about optimal ways to find P as well. But for now, we assume we have already done that and we know what P is.\nHowever, it is important to point out that the optimal capital allocation formula\n\\[w_{oca} = \\frac{E[R_P] - R_f}{A \\sigma_{P}^2}\\]\nis only valid if \\(\\sigma_{P}\\) is the right measure of the risk we would be exposed to if we invested in P. If P is the only investment we make, then this is correct. But if we own other assets, and P becomes part of our larger, overall portfolio, then this is no longer correct. That’s because a large part of P’s total risk (\\(\\sigma_{P}\\)) will be diversified away as part of our overall portfolio.\nSo, in general, the optimal capital allocation formula above, should not be applied if P is an individual security (such as TSLA in our previous example). Instead it should be applied if P is a very well diversified portfolio of risky assets, so that its total risk is very close to its systematic risk."
  },
  {
    "objectID": "teaching/FIN 421/lectures/L06_Portfolio_Theory_3_Tangency_Portfolios.html",
    "href": "teaching/FIN 421/lectures/L06_Portfolio_Theory_3_Tangency_Portfolios.html",
    "title": "L06: Tangency portfolios",
    "section": "",
    "text": "Given a set of risky assets, what is the best portfolio we can construct with them? Assuming we are mean-variance optimizers, this amounts to finding the set of weights that gives us the portfolio with the maximum Sharpe ratio (i.e. the portfolio that gives us the best expected return per unit of risk). We start by learning how to do this for the special case when we only have two risky assets and then move on to the general case with an arbitrary (“N”) number of assets. These maximum Sharpe ratio portfolios are also called optimal risky portfolios or tangency portfolios.\n\nThe special case: Portfolio optimization with two risky assets\n\nThe investment opportunity set\nTangency portfolio\n\nThe general case: Portfolio optimization with an arbitrary number risky assets\n\nMatrix representation\nTangency portfolio\n\n\nBesides learning the mechanics of building optimal risky portfolios, another key lesson from this lecture is that the optimal risky portfolio does not depend on who the investor is (i.e. it does not depend on their risk aversion). What depends on their risk aversion is the proportion of their funds they should invest in this optimal risky portfolio. We touched on this in the previous lecture, and we will talk about it in much more detail in the following lecture."
  },
  {
    "objectID": "teaching/FIN 421/lectures/L06_Portfolio_Theory_3_Tangency_Portfolios.html#the-investment-opportunity-set",
    "href": "teaching/FIN 421/lectures/L06_Portfolio_Theory_3_Tangency_Portfolios.html#the-investment-opportunity-set",
    "title": "L06: Tangency portfolios",
    "section": "The investment opportunity set",
    "text": "The investment opportunity set\nThe investment opportunity set for a given collection of assets is the set of ALL the possible portfolios you can construct with those assets. This is an infinite set but we can get a good idea of what the investment opportunity set looks like by just building a small subset of portfolios and plotting them in mean-volatility space.\nIn the special case with only two risky assets A and B, using the notation above, for any choice of weights (\\(w_A\\), \\(w_B\\)) we obtain a different portfolio P. Recall that the expected return and variance of the resulting portfolio P are given by:\n\\[E[R_P] = w_A E[R_A] + w_B E[R_B]\\]\n\\[Var[R_P] =  w_{A}^2 Var[R_A] + w_{B}^2 Var[R_B] + 2 w_A w_B Cov[R_A, R_B]\\]\nwhere \\(w_B = 1 - w_A\\) and the portfolio standard deviation is \\(\\sigma_P = \\sqrt{Var[R_P]}\\)\n\nExample\nYou want to invest in Facebook (FB) and Netflix (NFLX) but are not sure how much to invest in each. You have estimated that the expected returns of FB and NFLX are 10% and 15% respectively, their standard deviations are 20% and 25% respectively and their covariance is 0.02. Construct 21 different portfolios of FB and NFLX by starting out with a weight of 0% in FB and progressively increasing it by 5% until you reach a portfolio with 100% in FB. For each portfolio, estimate its expected return and standard deviation of future returns and plot it in mean-volatility space.\n\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#"
  },
  {
    "objectID": "teaching/FIN 421/lectures/L06_Portfolio_Theory_3_Tangency_Portfolios.html#optimal-risky-portfolio-tangency-portfolio",
    "href": "teaching/FIN 421/lectures/L06_Portfolio_Theory_3_Tangency_Portfolios.html#optimal-risky-portfolio-tangency-portfolio",
    "title": "L06: Tangency portfolios",
    "section": "Optimal risky portfolio (tangency portfolio)",
    "text": "Optimal risky portfolio (tangency portfolio)\nThe combination between two risky assets A and B that gives us the maximum Sharpe ratio is obtained by setting\n\\[w_A = \\frac{(E[R_A] - R_f) \\sigma_{B}^2 - (E[R_B] - R_f)Cov[R_A, R_B]}{(E[R_A] - R_f) \\sigma_{B}^2 + (E[R_B] - R_f) \\sigma_{A}^2 - (E[R_A] - R_f + E[R_B] - R_f)Cov[R_A, R_B]}\\]\nand \\(w_B = 1 - w_A\\).\nThe reason why we also call this optimal risky portfolio the “tangency portfolio” is because its capital allocation line (CAL) intersects the investment opportunity set of A and B (see the curve we plotted in the previous example) at only one point (i.e. the CAL is tangent to the investment opportunity set).\n\nExample\nUsing the data from the previous example, find the optimal combination of FB and NFLX. Assume that the risk-free rate is 0.1%.\n\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#"
  },
  {
    "objectID": "teaching/FIN 421/lectures/L06_Portfolio_Theory_3_Tangency_Portfolios.html#matrix-notation",
    "href": "teaching/FIN 421/lectures/L06_Portfolio_Theory_3_Tangency_Portfolios.html#matrix-notation",
    "title": "L06: Tangency portfolios",
    "section": "Matrix notation",
    "text": "Matrix notation\nTo make the formulas more manageable, we assemble our inputs (expected returns, variances and covariances) into matrices.\nLet \\(\\mu\\) be the Nx1 vector of expected returns:\n\\[\\mu = \\begin{pmatrix} E[R_1] \\\\ E[R_2] \\\\ ... \\\\ E[R_N] \\end{pmatrix}\\]\nAnd \\(\\Sigma\\) the variance-covariance matrix of future returns. In row \\(i\\) and column \\(j\\) this matrix contains the covariance between the returns of assets \\(i\\) and \\(j\\). Note that when \\(i = j\\) (on the diagonal) this equals the variance of asset \\(i\\).\n\\[\\Sigma = \\begin{pmatrix} Var[R_1] & Cov[R_1, R_2] & ... & Cov[R_1, R_N] \\\\ Cov[R_2, R_1] & Var[R_2] & ... & Cov[R_2, R_N] \\\\ ... & ... & ... & ... \\\\ Cov[R_N, R_1] & Cov[R_N, R_2] & ... & Var[R_N] \\end{pmatrix}\\]"
  },
  {
    "objectID": "teaching/FIN 421/lectures/L06_Portfolio_Theory_3_Tangency_Portfolios.html#tangency-portfolio-with-n-risky-assets",
    "href": "teaching/FIN 421/lectures/L06_Portfolio_Theory_3_Tangency_Portfolios.html#tangency-portfolio-with-n-risky-assets",
    "title": "L06: Tangency portfolios",
    "section": "Tangency portfolio with N risky assets",
    "text": "Tangency portfolio with N risky assets\nLet \\(\\mu^e\\) be the vector of risk premia on our N assets:\n\\[\\mu^e = \\begin{pmatrix} E[R_1] - R_f \\\\ E[R_2] - R_f \\\\ ... \\\\ E[R_N] - R_f \\end{pmatrix}\\]\nand \\(\\mathbf{1^{T}}\\) be the transpose of a vector of N ones: \\(\\mathbf{1^{T}} = (1, 1, ..., 1)\\).\nThen the vector \\(W_{tan}\\) of weights that gives us the tangency portfolio (i.e. portfolio with the maximum Sharpe ratio) is given by:\n\\[W_{tan} = \\frac{\\Sigma^{-1}\\mu^e }{ \\mathbf{1^{T}}\\Sigma^{-1}\\mu^e }\\]\nwhere \\(\\Sigma^{-1}\\) is the inverse of the variance-covariance matrix \\(\\Sigma\\)"
  },
  {
    "objectID": "teaching/FIN 421/lectures/L06_Portfolio_Theory_3_Tangency_Portfolios.html#real-world-application",
    "href": "teaching/FIN 421/lectures/L06_Portfolio_Theory_3_Tangency_Portfolios.html#real-world-application",
    "title": "L06: Tangency portfolios",
    "section": "Real-World Application",
    "text": "Real-World Application\nYou want to invest in the FANG stocks (Facebook, Apple, Netflix, and Google) but you are not sure how much to invest in each. Download monthly price data on the FANG stocks over the past 5 years (January 2018 to December 2022) and calculate monthly returns. Use these data to estimate the optimal weights you should use for the four stocks. Assume that the risk-free rate is 0.1%.\n\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#"
  },
  {
    "objectID": "teaching/FIN 421/lectures/L03_Foundations_2_Future_Returns.html",
    "href": "teaching/FIN 421/lectures/L03_Foundations_2_Future_Returns.html",
    "title": "L03: Modeling future returns",
    "section": "",
    "text": "We start thinking about future returns. We model them as random variables and we are interested in estimating the expectation and volatility of the probability distribution associated with these random variables. We will use sample means to estimate the expectation (i.e. expected returns) and the sample standard deviation to estimate the volatility (i.e. total risk). We then introduce the concept of a risk premium and we show how investments should be assessed based on the risk - return tradeoff they offer.\n\nUsing sample means and standard deviations to estimate expected return and risk\nRisk-free rate, excess returns, and risk premia\nMean variance analysis and the Sharpe Ratio"
  },
  {
    "objectID": "teaching/FIN 421/lectures/L03_Foundations_2_Future_Returns.html#example",
    "href": "teaching/FIN 421/lectures/L03_Foundations_2_Future_Returns.html#example",
    "title": "L03: Modeling future returns",
    "section": "Example",
    "text": "Example\nSuppose for the last 4 months, TLSA had returns of 10%, 5%, -5%, and 14% respectively. Using this data alone, what is your estimate for the expected return and risk associated with TSLA’s next month’s return?\n\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#"
  },
  {
    "objectID": "teaching/FIN 421/lectures/L03_Foundations_2_Future_Returns.html#example-1",
    "href": "teaching/FIN 421/lectures/L03_Foundations_2_Future_Returns.html#example-1",
    "title": "L03: Modeling future returns",
    "section": "Example",
    "text": "Example\nSuppose returns on TSLA over the past 3 months were 10% (in the most recent month), 5%, and -3% respectively and that the yield on a 1-month Tbill were 0.1% (in the most recent month), 0.2%, and 0.3% respectively. What were the excess returns on TSLA over the past 3 months. What is your estimate for the risk premium on TSLA?\n\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#"
  },
  {
    "objectID": "teaching/FIN 421/lectures/L03_Foundations_2_Future_Returns.html#example-2",
    "href": "teaching/FIN 421/lectures/L03_Foundations_2_Future_Returns.html#example-2",
    "title": "L03: Modeling future returns",
    "section": "Example",
    "text": "Example\nAssume that you are only allowed to invest in TSLA or AAPL. You estimate that TSLA has an expected return of 10% and a standard deviation of 12% and AAPL has an expected return of 7% and a standard deviation of 6%. The yield on a 1-month Tbill is 1%. If you are a mean-variance optimizer, which stock should you invest in?\n\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#\n#"
  },
  {
    "objectID": "teaching/FIN 421/lectures/L01_Introduction_and_Setup.html",
    "href": "teaching/FIN 421/lectures/L01_Introduction_and_Setup.html",
    "title": "L01: Introduction",
    "section": "",
    "text": "Topics covered (in reverse order):\n\n\n\nAll asset prices are determined by supply and demand for them in the market\nIf investors are rational, these prices will equal the discounted value of their expected cash flows:\n\nPricing formula for stocks\n\\[P_t = \\frac{D_{t+1}}{1+d} + \\frac{D_{t+2}}{(1+d)^2} + \\frac{D_{t+3}}{(1+d)^3} + ...\\]\nPricing formula for bonds\n\\[P_t = \\frac{C_{t+1}}{1+YTM} + \\frac{C_{t+2}}{(1+YTM)^2} + ... + \\frac{C_T + P}{(1+YTM)^T} \\]\n\nIf investors are rational and markets are efficient, then all assets are priced correctly (current market price = intrinsic value of the asset). The only way to obtain a higher return is to incur a higher risk. You do this by investing more or less of your money in the market portfolio and the rest in the risk-free asset (e.g. Tbills). There is an optimal way of doing this, depending on your level of risk aversion. This process is called the capital allocation process and we will cover it in the second part of our course (more on this below).\nIf investors are not rational and markets are inefficient (they make systematic mistakes) we can take advantage of this by:\n\nBuying assets that are undervalued (current market price &lt; correct price)\nSelling (or shorting) assets that are overvalued (current market price &gt; correct price)\n\nTo do this, we need to figure out what is the “correct price” of an asset. This part of finance is generally referred to as valuation. We will focus on valuing equities (stocks) and we will cover this in the third part of our course. There are several approaches to valuing a company. The most commonly used ones are:\n\nDividend discount models (DDM): uses simplified versions of the above stock pricing formula. For this, we need to come up with estimates for:\n\nThe future dividends associated with the assets: mostly covered in corporate finance (pro-forma statements etc)\nThe appropriate discount rate for those assets: mostly covered in investments class (portfolio theory, CAPM etc)\n\n\nDiscounted cash flow analysis (DCF): similar to DDM, except instead of dividends, we use free cash flows\nMultiples valuation: find comparable firms, estimate multiples like P/E ratios for them, and assume they apply to the firm you are valuing. Then use these ratios to back out the price for your firm.\n\n\n\n\n\n\nBefore we get to valuation (pricing individual assets), we need to understand what happens when you invest in more than one asset (i.e. when you own a portfolio of assets). This is very important because\n\nIt shows us how diversification works, which in turn\nShows us the difference between systematic (diversifiable) risk and indiosyncratic (non-diversifiable) risk, which in turn\nShows us how we should be calculating the appropriate discount rate for an asset (we will see that this is the same as the expected return of the asset)\n\nUnderstanding how to analize portfolios is also crucial for developing optimal asset allocation decisions:\n\nAs mentioned above, if you believe investors are rational and markets are efficient, then the optimal capital allocation decision tells you how much to invest in the risk free asset and how much to invest in the market portfolio and your investment process ends here.\nIf you believe investors are not rational and markets are inefficient, it may be optimal for you to invest in something other than the market portfolio (e.g. any mispriced assets that you may have found). In this case, we can use portfolio theory to find the optimal weights you should be putting in each of these risky assets. This is called the optimal risky portfolio. Once this is done, we can use our optimal asset allocation process to find the optimal percent of our funds to invest in this optimal risky portfolio, so as to obtain a complete portfolio that matches our level of risk aversion.\n\n\n\n\n\n\nBefore we get to portfolio theory or valuation, we need to have a firm grasp on some of they key quantities we will use to describe asset prices and returns. We will cover this in the first part of the course:\n\nReturns\n\nCalculating holding period returns (HPR)\nCompounding returns over multiple periods\nAnnualizing returns (APR vs EAR)\nAdjusting for inflation (real vs nominal rates of return)\nAverage returns:\n\nArithmetic average\nGeometric average\n\nVolatility of returns:\n\nVariance\nStandard deviation\n\n\nExcess returns and risk premia\nRisk aversion\nSharpe ratios"
  },
  {
    "objectID": "teaching/FIN 421/lectures/L01_Introduction_and_Setup.html#pricing-of-individual-assets",
    "href": "teaching/FIN 421/lectures/L01_Introduction_and_Setup.html#pricing-of-individual-assets",
    "title": "L01: Introduction",
    "section": "",
    "text": "All asset prices are determined by supply and demand for them in the market\nIf investors are rational, these prices will equal the discounted value of their expected cash flows:\n\nPricing formula for stocks\n\\[P_t = \\frac{D_{t+1}}{1+d} + \\frac{D_{t+2}}{(1+d)^2} + \\frac{D_{t+3}}{(1+d)^3} + ...\\]\nPricing formula for bonds\n\\[P_t = \\frac{C_{t+1}}{1+YTM} + \\frac{C_{t+2}}{(1+YTM)^2} + ... + \\frac{C_T + P}{(1+YTM)^T} \\]\n\nIf investors are rational and markets are efficient, then all assets are priced correctly (current market price = intrinsic value of the asset). The only way to obtain a higher return is to incur a higher risk. You do this by investing more or less of your money in the market portfolio and the rest in the risk-free asset (e.g. Tbills). There is an optimal way of doing this, depending on your level of risk aversion. This process is called the capital allocation process and we will cover it in the second part of our course (more on this below).\nIf investors are not rational and markets are inefficient (they make systematic mistakes) we can take advantage of this by:\n\nBuying assets that are undervalued (current market price &lt; correct price)\nSelling (or shorting) assets that are overvalued (current market price &gt; correct price)\n\nTo do this, we need to figure out what is the “correct price” of an asset. This part of finance is generally referred to as valuation. We will focus on valuing equities (stocks) and we will cover this in the third part of our course. There are several approaches to valuing a company. The most commonly used ones are:\n\nDividend discount models (DDM): uses simplified versions of the above stock pricing formula. For this, we need to come up with estimates for:\n\nThe future dividends associated with the assets: mostly covered in corporate finance (pro-forma statements etc)\nThe appropriate discount rate for those assets: mostly covered in investments class (portfolio theory, CAPM etc)\n\n\nDiscounted cash flow analysis (DCF): similar to DDM, except instead of dividends, we use free cash flows\nMultiples valuation: find comparable firms, estimate multiples like P/E ratios for them, and assume they apply to the firm you are valuing. Then use these ratios to back out the price for your firm."
  },
  {
    "objectID": "teaching/FIN 421/lectures/L01_Introduction_and_Setup.html#portfolio-theory",
    "href": "teaching/FIN 421/lectures/L01_Introduction_and_Setup.html#portfolio-theory",
    "title": "L01: Introduction",
    "section": "",
    "text": "Before we get to valuation (pricing individual assets), we need to understand what happens when you invest in more than one asset (i.e. when you own a portfolio of assets). This is very important because\n\nIt shows us how diversification works, which in turn\nShows us the difference between systematic (diversifiable) risk and indiosyncratic (non-diversifiable) risk, which in turn\nShows us how we should be calculating the appropriate discount rate for an asset (we will see that this is the same as the expected return of the asset)\n\nUnderstanding how to analize portfolios is also crucial for developing optimal asset allocation decisions:\n\nAs mentioned above, if you believe investors are rational and markets are efficient, then the optimal capital allocation decision tells you how much to invest in the risk free asset and how much to invest in the market portfolio and your investment process ends here.\nIf you believe investors are not rational and markets are inefficient, it may be optimal for you to invest in something other than the market portfolio (e.g. any mispriced assets that you may have found). In this case, we can use portfolio theory to find the optimal weights you should be putting in each of these risky assets. This is called the optimal risky portfolio. Once this is done, we can use our optimal asset allocation process to find the optimal percent of our funds to invest in this optimal risky portfolio, so as to obtain a complete portfolio that matches our level of risk aversion."
  },
  {
    "objectID": "teaching/FIN 421/lectures/L01_Introduction_and_Setup.html#foundations-of-investments",
    "href": "teaching/FIN 421/lectures/L01_Introduction_and_Setup.html#foundations-of-investments",
    "title": "L01: Introduction",
    "section": "",
    "text": "Before we get to portfolio theory or valuation, we need to have a firm grasp on some of they key quantities we will use to describe asset prices and returns. We will cover this in the first part of the course:\n\nReturns\n\nCalculating holding period returns (HPR)\nCompounding returns over multiple periods\nAnnualizing returns (APR vs EAR)\nAdjusting for inflation (real vs nominal rates of return)\nAverage returns:\n\nArithmetic average\nGeometric average\n\nVolatility of returns:\n\nVariance\nStandard deviation\n\n\nExcess returns and risk premia\nRisk aversion\nSharpe ratios"
  },
  {
    "objectID": "teaching/FIN 421/lectures/Lecture 16.html",
    "href": "teaching/FIN 421/lectures/Lecture 16.html",
    "title": "L16: Bond risk",
    "section": "",
    "text": "Anything that may affect the realized return on your bond investment is considered a source of risk.\nTwo major sources of risk for bonds:\n\nDefault risk:\n\nRisk that you will not get paid the coupons and/or par value of the bond\n\nInterest rate risk:\n\nPrice risk:\n\nRisk that an interest rate increase will decrease the price you will be able to sell the bond at.\n\nReinvestment rate risk:\n\nRisk that an interest rate decrease will decrease the future value of your reinvested coupons\n\n\n\nOffsetting forces: - When interest rates increase, bond prices decrease, but coupons can be reinvested at a higher rate (i.e. price risk and reinvestment risk offset each other to some degree)\n\nThese two forces perfectly offset if the bond has a duration equal to the horizon of the investor\n\nThe process of setting the duration of your bond portfolio equal to your horizon is called immunization"
  },
  {
    "objectID": "teaching/FIN 421/lectures/Lecture 16.html#example-1-duration-is-higher-for-bonds-with-longer-maturity",
    "href": "teaching/FIN 421/lectures/Lecture 16.html#example-1-duration-is-higher-for-bonds-with-longer-maturity",
    "title": "L16: Bond risk",
    "section": "Example 1: Duration is higher for bonds with longer maturity",
    "text": "Example 1: Duration is higher for bonds with longer maturity\nBonds A, B and C all have coupon rate and YTM of 6% (semiannual, par=1000). They were all issued today and mature in 1 year (bond A), 10 years (bond B) and 30 years (bond C). Calculate their duration.\n\n# \n#\n#\n#\n#\n#\n#\n#"
  },
  {
    "objectID": "teaching/FIN 421/lectures/Lecture 16.html#example-2-duration-is-higher-for-bonds-with-lower-ytm",
    "href": "teaching/FIN 421/lectures/Lecture 16.html#example-2-duration-is-higher-for-bonds-with-lower-ytm",
    "title": "L16: Bond risk",
    "section": "Example 2: Duration is higher for bonds with lower YTM",
    "text": "Example 2: Duration is higher for bonds with lower YTM\nBonds A, B and C all have a coupon rate of 4% (semiannual, par=1000) and mature in 10 years. They were all issued today and have a YTM of 2% (bond A), 4% (bond B) and 6% (bond C). Calculate their duration.\n\n# \n#\n#\n#\n#\n#\n#\n#"
  },
  {
    "objectID": "teaching/FIN 421/lectures/Lecture 16.html#example-3-duration-is-higher-for-bonds-with-lower-coupon-rates",
    "href": "teaching/FIN 421/lectures/Lecture 16.html#example-3-duration-is-higher-for-bonds-with-lower-coupon-rates",
    "title": "L16: Bond risk",
    "section": "Example 3: Duration is higher for bonds with lower coupon rates",
    "text": "Example 3: Duration is higher for bonds with lower coupon rates\nBonds A, B and C all have a YTM of 6% (semiannual, par=1000) and mature in 10 years. They were all issued today and have a coupon rate of 3% (bond A), 6% (bond B) and 9% (bond C). Calculate their duration.\n\n# \n#\n#\n#\n#\n#\n#\n#"
  },
  {
    "objectID": "teaching/FIN 421/lectures/Lecture 16.html#using-duration",
    "href": "teaching/FIN 421/lectures/Lecture 16.html#using-duration",
    "title": "L16: Bond risk",
    "section": "Using duration",
    "text": "Using duration\nFor a bond with price \\(P_0\\), duration D, YTM \\(Y\\), and frequency F, the percentage change in price caused by a change \\(x\\) in YTM is approximated by:\n\\[\\frac{P_1 - P_0}{P_0} = - D \\cdot \\frac{x}{1 + Y/F}\\]"
  },
  {
    "objectID": "teaching/FIN 421/lectures/Lecture 16.html#using-modified-duration",
    "href": "teaching/FIN 421/lectures/Lecture 16.html#using-modified-duration",
    "title": "L16: Bond risk",
    "section": "Using modified duration",
    "text": "Using modified duration\nFor a bond with price \\(P_0\\), modified duration D*, YTM \\(Y\\), and frequency F, the percentage change in price caused by a change \\(x\\) in YTM is approximated by:\n\\[\\frac{P_1 - P_0}{P_0} = - D^{*} \\cdot x\\]\nExamples 1,2,3:\nGo back to each of the bonds in Examples 1, 2, and 3, above, and for each of them:\n\nUse their duration to estimate the price impact (in %) of an increase in YTM of 0.5%.\nUse their modified duration to estimate the price impact (in %) of a decrease in YTM of 0.75%.\n\n\n# \n#\n#\n#\n#\n#\n#\n#\n# \n#\n#\n#\n#\n#\n#\n#"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "FIN 525: Empirical Methods in Finance\n\nThe objective of this course is to familiarize the students with the various databases and statistical methods needed to undertake practitioner-type research in finance (manage large datasets, perform empirical analysis of financial data, interpret statistical results, and present the analysis). The students are assumed to have a basic knowledge of finance and statistics/econometrics. The course is empirically oriented and is geared towards students who are interested in a career in financial industry or finance research in general. Students will be introduced to the Python programming language, which will be used to conduct all our data analysis.\nLectures are accessible through the sidebar menu.\n\n\nFIN 421: Investments\n\nThis course introduces students to the fundamental principles of investments management and provide them with the theoretical framework and the analytical tools needed to make sound investment decisions. Broadly speaking, the major topics covered will include: the risk-return tradeoff, optimal asset allocation, security selection (valuation), and derivative securities. The computational aspects for each topic will be showcased using Microsoft Excel.\nLectures are accessible through the sidebar menu."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Publications",
    "section": "",
    "text": "Publications\n\n\nPolicy Uncertainty and Corporate Investment (2016)\nwith Huseyin Gulen\n\nReview of Financial Studies, Vol. 29 (3), 2016, 523-564\nLead Article, Editor’s Choice Article\nMentions: Economic Report of the President of the United States (2023), The Economist (paywall)\nAbstract:\nUsing the policy uncertainty index of Baker, Bloom, and Davis (2013), we document a strong negative relationship between firm-level capital investment and the aggregate level of uncertainty associated with future policy and regulatory outcomes. More importantly, we find evidence that the relation between policy uncertainty and capital investment is not uniform in the cross section, being significantly stronger for firms with a higher degree of investment irreversibility and for firms which are more dependent on government spending. Our results lend empirical support to the notion that policy uncertainty can depress corporate investment by inducing precautionary delays due to investment irreversibility.\n\n\n\n\nDoes Policy Uncertainty Affect Mergers and Acquisitions? (2018)\nwith Alice Bonaime and Huseyin Gulen\n\nJournal of Financial Economics, Vol. 129 (3), 2018, 531-558\nAbstract:\nPolitical and regulatory uncertainty is strongly negatively associated with merger and acquisition activity at the macro and firm levels. The strongest effects are for uncertainty regarding taxes, government spending, monetary and fiscal policies, and regulation. Consistent with a real options channel, the effect is exacerbated for less reversible deals and for firms whose product demand or stock returns exhibit greater sensitivity to policy uncertainty, but attenuated for deals that cannot be delayed due to competition and for deals that hedge firm-level risk. Contractual mechanisms (deal premiums, termination fees, MAC clauses) unanimously point to policy uncertainty increasing the target’s negotiating power.\n\n\n\n\nExports, Investment, and Policy Uncertainty (2019)\nwith Andrew Greenland and John Lopresti\n\nCanadian Journal of Economics, Vol. 52 (3) 2019, 1248-1288\nAbstract:\nBuilding on a literature that underscores the value of delaying investment in the face of uncertainty, we study how policy uncertainty in fifteen large economies affects trade flows into these countries. We show that high levels of uncertainty in destination markets are associated with significantly lower exports to those markets. Furthermore, consistent with a model in which firms face irreversible costs to export, we show that this effect is entirely driven by a reduction in the entry of new products. Finally, we find that the effect of uncertainty on trade flows is significantly stronger when trade costs are less reversible.\n\n\n\n\nThe Use of Asset Growth in Empirical Asset Pricing Models\nwith Michael Cooper and Huseyin Gulen\n\nJournal of Financial Economics, Forthcoming\nAbstract:\nWe show that the performance of the new factor models of Hou, Xue, and Zhang (2015) and Fama and French (2015) depends crucially on how their investment factor is constructed. Both models use growth in total assets to measure investment. Their ability to price the cross-section of returns decreases significantly when the investment factor is constructed using traditional investment measures, or measures that also account for investment in intangibles. In contrast, we find that factors based on growth in inventory and accounts receivable contain the bulk of the pricing information in the asset growth factor. We show evidence that the superior performance of the asset growth factor seems to be attributable to its ability to capture aggregate shocks to equity financing costs.\n\n\n\n\nWorking Papers\n\n\nCredit Cycles, Expectations, and Corporate Investment\nwith Huseyin Gulen and Stefano Rossi\n\nRevise and Resubmit at the Review of Financial Studies\nAbstract:\nWe provide a systematic empirical assessment of the Minsky (1977) hypothesis that business fluctuations are due to irrational swings in expectations. We build an aggregate index of irrational expectations using predictable firm level forecast errors and use it to provide three sets of results. First, we show that such an index of irrational expectations drives aggregate credit cycles. Next, we build an aggregate index of predictable credit cycles as the portion of credit cycles predicted by irrational expectations, and we show that such index drives cycles in firm-level debt issuance and investment. Finally, we show that after increases (re., decreases) in credit market sentiment firm-level financing and investment cycles are more pronounced for firms with ex ante more optimistic (re., pessimistic) expectations. Financial constraints do not have additional explanatory power for firm-level cycles in the cross section once irrational expectations are considered. We rationalize these results within a parsimonious dynamic Q-theory model with risky debt in which both corporate managers and credit investors hold diagnostic expectations.\n\n\n\n\nUsing Equity Market Reactions to Infer Exposure to Trade Liberalization\nwith Andrew Greenland, John Lopresti, and Peter Schott\n\nNBER Working Paper No. w27510\nAbstract:\nWe propose a method for identifying exposure to changes in trade policy based on asset prices that has several advantages over standard measures: it encompasses all avenues of exposure, it is natively firm-level, it yields estimates for both goods and service producers, and it can be used to study reductions in difficult-to-quantify non-tariff barriers in a way that controls naturally for broader macroeconomic shocks. Applying our method to two well-studied US trade liberalizations provides new insight into service sector responses to trade liberalizations as well as dramatically different responses among small versus large firms, even within narrow industries.\n\n\n\n\nPolicy Uncertainty, Corporate Risk-Taking, and CEO Incentives\nwith David Yin\n\nAbstract:\nWe show evidence that CEO risk-taking incentives represent a key mechanism through which policy uncertainty impacts the real economy. We document that high levels of policy uncertainty are associated with significantly lower future stock return volatility. This relation is stronger (more negative) for firms where the CEO has higher compensation delta or less transferable skills, and is weaker when the CEO has higher compensation vega. Furthermore, when policy uncertainty is high, firms are more likely to use financial hedging instruments and engage in more diversifying mergers, and CEOs sell more own-firm shares and exercise fewer options."
  }
]