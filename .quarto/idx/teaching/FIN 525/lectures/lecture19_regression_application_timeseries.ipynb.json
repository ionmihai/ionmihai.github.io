{"title":"L19: Robust timeseries regression","markdown":{"yaml":{"title":"L19: Robust timeseries regression"},"headingText":"Lecture overview","containsRefs":false,"markdown":"\n\n\n- Coverage\n    - We only cover \"static\" models (of the type $y_{t+k} = \\alpha + \\beta \\cdot X_t + \\epsilon_{t+k}$)\n    - We do not cover dynamic models (e.g. ARIMA models or VAR models which include lags of the dependent variable as explanatory variables)\n    - We do not cover conditional heteroskedasticity models (e.g. ARCH and GARCH models of the variance of the error term)\n        \n\n- Dealing with autocorrelated errors (failure of assumption A3)\n    - Newey-West correction\n\n\n- Dealing with non-stationary variables (failure of assumption A2)\n    - Test for stationarity\n    - Common ways to address non-stationarity\n        - First-differencing\n        - Detrending\n\n## Application\n\nThe showcase the tools in this lecture, we will develop a (somewhat crude) test of the Expectations Hypothesis of the term structure of interest rates. In a nutshell, this hypothesis claims long-term rates should equal compounded future expected short-term rates:\n\n$$ (1 + r_{t,t+N})^N = (1 + E_t(r_{t,t+1}))(1 + E_t(r_{t+1,t+2}))...(1 + E_t(r_{t+N-1,t+N})) $$\n\nAssuming rational expectations, future realized short-term rates should on average match current expectations of those rates. If this is the case, one way we can test the Expectations Hypothesis by testing if current long-term rates can predict future short-term rates.\n\nTo implement this test, we use the yield on 10-year Treasury bonds as our long-term rate, and the yield on the 3-month Treasury bill as our short-term rate. We then regress the 3-month rate from 5 years in the future on the current 10-year rate \n\n$$r^{3m}_{t+5} = \\alpha + \\beta \\cdot r^{10yr}_t + \\epsilon_{t+5} $$\n\n\nYes, the 5-year horizon is quite arbitrary (we should be testing all horizons up to 10 years at the same time), hence my calling it a \"somewhat crude\" test. The purpose of this application is to showcase the common statistical issues one often encounters in time-series regressions. See this paper https://core.ac.uk/download/pdf/6956537.pdf for more thorough tests of the hypothesis.\n\nWe start by downloading data on the two rates (monthly frequency, not seasonally adjusted) and running the regression mentioned above. The rest of the lecture describes two main issues with this regression (non-stationarity and autocorrelated errors) and describes common tools used to address these issues. \n\n# Preliminaries\n\n# Descriptive statistics\n\nWe start by just summarising the data and looking at correlations of the variables. With time-series regressions,we want to pay particular attention to how autocorrelated our variables are. \n\nAlways look at autocorrelations in your data before you run a time-series regression:\n\nThe above are just one-period (1 month) autocorrelations. We can look at all autocorrelations (all lags):\n\n# Linear regression\n\nWe start by running the simple regression of future short term rates on current long-term rates:\n\n\n$$r^{3m}_{t+5} = \\alpha + \\beta \\cdot r^{10yr}_t + \\epsilon_{t+5} $$\n\nThe low p-value for the ``r_10yr`` coefficient tells us that the long rate is a strong predictor of the future short rate, which is consistent with the Expectations Hypothesis.\n\n# Dealing with autocorrelated errors\n\nThe Durbin-Watson statistic in the regression above is close to 0, which suggests the error terms in our regression are positively autocorrelated. This violates assumption A3 discussed in the regression intro lecture. \n\nTo address this issue, we can apply the \"get_robustcov_results\" function to the results output from the OLS regression above, specifying that the covariance type (\"cov_type\") needs to be heteroskedasticity-and-autocorrelation consistent (HAC). We also need to specify the maximum number of lags we want to use when correcting our standar errors (the \"maxlag\" parameter). This implements the Newey-West (1987) covariance estimator that adjusts standard errors for autocorrelation in residuals up to the \"maxlag\" you specified. \n\nMost practitioners use $maxlag = N^{1/4}$ where N is the number of observations in our regression (though we will not go into detail about why this value is used).\n\nNote that this correction does not change the coefficients themselves, just their statistical significance. For example the constant term is no longer statistically significant. More importantly for our application: the long term rate is still a statistically significant predictor of the future short rate, which is consistent with the Expectations Hypothesis.\n\n# Dealing with non-stationarity\n\nA \"stationary\" time-series process is a process that has a constant mean and variance over time, and autocorrelations depend only on the lag, not on the time period itself (technically speaking, such processes are called \"covariance-stationary\" or \"weakly-stationary\" but these are usually shortened to \"stationary\").\n\nIf the variables in our time-series regression are non-stationary, the regression estimates are not reliable because non-stationarity can cause:\n- A \"spurious correlation\" between the two variables (variables may look highly correlated when in fact they are not economically related in any way)\n    - This is a failure of assumption A2\n    - This leads to a bias in the regression coefficients (affects conclusions on economic significance)\n- Heteroskedasticity and autocorrelation in the regression residuals \n    - This is a failure of assumption A3\n    - In thise case, t-statistics and p-values will be miscalculated (affects conclusions on statistical significance)\n\n\n\n## Testing for stationarity (unit root tests)\n\nBefore you run ANY time series regression, you need to first test if the variables in your regression are stationary (such tests are commonly referred to as \"unit root tests\"). There are several test you could use for this purpose, but one of the most common ones is the Augmented Dickey-Fuller test (ADF). You can implement this test using the \"adfuller\" function from the \"statsmodels.tsa.stattools\" package (\"st\" below).\n\nThe null hypothesis in the ADF test is that the series is non-stationary. So if the test returns a small p-value (e.g smaller that 1%), we can conclude that the series does not suffer from non-stationarity. \n\n\nThe p-value above is larger than 1\\% so we can not conclude that the short rate is non-stationary.\n\nThe p-value above is larger than 1\\% so we can not conclude that the long rate is non-stationary.\n\n## First-differencing\n\nThe most commonly used method to convert a non-stationary series into a stationary series it to first-difference it (i.e. current level minus the previous level of the series). Technically, this assumes that the non-stationary series has \"order of integration 1\" which is the case for most economic series of interest. You don't need to understand what that means for this class. I am only mentioning it so you understand that sometimes, first-differencing may NOT produce a stationary series. In such cases, a second-difference may help: take a first-difference of the first-difference. Alternatively, use the detrending methods in sections 3.3. and 3.4. below.   \n\nIt looks like the 1-month autocorrelation is still quite high. But, if we look at the autocorrelation plot (the gray lines are 95\\% confidence intervals), most of these autocorrelations are statistically insignificant. \n\nAgain, to formally test if we still have a non-stationary problem, we run a ADF test:\n\nThe p-value is virtually 0, so we can reject the null of non-stationarity for the first-differenced short rate.\n\nAgain, the p-value is virtually 0, so we can reject the null of non-stationarity for the first-differenced long rate too.\n\nThese results differ from the non-differenced regression in a very crucial way: the p-value of the ``r_10yr_change`` variable is not lower than 1\\% anymore, so we can not reject the null that the long rate has no predictive power over the short rate. This **contradicts** the prediction of the Expectations Hypothesis that long rates should have statistically significant predictive power over short rates.\n\n## Controlling for deterministic trends\n\nIf we believe that our variables are non-stationary because of a deterministic trend (like a linear trend or a quadratic trend), then we can adjust for this by simply including these trends in our regression. To do this, we first create the trend variables (we will restrict ourselves to a linear trend and a quadratic trend), and then we simply add them to our regression. \n\nNote that even controlling for trends renders the long-rate statistically insignificant. \n\n## Eliminating stochastic (changing) trends (OPTIONAL)\n\nIn some cases, non-stationarity could be caused by trends that change over time (e.g. a linear trend in the first part of the sample,  no trend in the middle, and a quadratic trend towards the end). In this case, the deterministic-trends approach from above may not accurately control for these trends and hence may not solve our non-stationarity problem.\n\nIn this circumstance, it is more appropriate to estimate these stochastic trends first (for each series). Then subtract these trends from the series and use these de-trended variables in our regression instead. \nThe **Hodrick-Prescott method** is very commonly used for this purpose and it outputs the detrended series directly (as well as the estimated trend). This method can be implemented using the ``.tsa.filters.hpfilter()`` function in the ``statsmodel`` package, as below. \n\nOnce again, detrending the data using stochastic trends also renders the long rate insignificant. \n","srcMarkdownNoYaml":"\n\n# Lecture overview\n\n- Coverage\n    - We only cover \"static\" models (of the type $y_{t+k} = \\alpha + \\beta \\cdot X_t + \\epsilon_{t+k}$)\n    - We do not cover dynamic models (e.g. ARIMA models or VAR models which include lags of the dependent variable as explanatory variables)\n    - We do not cover conditional heteroskedasticity models (e.g. ARCH and GARCH models of the variance of the error term)\n        \n\n- Dealing with autocorrelated errors (failure of assumption A3)\n    - Newey-West correction\n\n\n- Dealing with non-stationary variables (failure of assumption A2)\n    - Test for stationarity\n    - Common ways to address non-stationarity\n        - First-differencing\n        - Detrending\n\n## Application\n\nThe showcase the tools in this lecture, we will develop a (somewhat crude) test of the Expectations Hypothesis of the term structure of interest rates. In a nutshell, this hypothesis claims long-term rates should equal compounded future expected short-term rates:\n\n$$ (1 + r_{t,t+N})^N = (1 + E_t(r_{t,t+1}))(1 + E_t(r_{t+1,t+2}))...(1 + E_t(r_{t+N-1,t+N})) $$\n\nAssuming rational expectations, future realized short-term rates should on average match current expectations of those rates. If this is the case, one way we can test the Expectations Hypothesis by testing if current long-term rates can predict future short-term rates.\n\nTo implement this test, we use the yield on 10-year Treasury bonds as our long-term rate, and the yield on the 3-month Treasury bill as our short-term rate. We then regress the 3-month rate from 5 years in the future on the current 10-year rate \n\n$$r^{3m}_{t+5} = \\alpha + \\beta \\cdot r^{10yr}_t + \\epsilon_{t+5} $$\n\n\nYes, the 5-year horizon is quite arbitrary (we should be testing all horizons up to 10 years at the same time), hence my calling it a \"somewhat crude\" test. The purpose of this application is to showcase the common statistical issues one often encounters in time-series regressions. See this paper https://core.ac.uk/download/pdf/6956537.pdf for more thorough tests of the hypothesis.\n\nWe start by downloading data on the two rates (monthly frequency, not seasonally adjusted) and running the regression mentioned above. The rest of the lecture describes two main issues with this regression (non-stationarity and autocorrelated errors) and describes common tools used to address these issues. \n\n# Preliminaries\n\n# Descriptive statistics\n\nWe start by just summarising the data and looking at correlations of the variables. With time-series regressions,we want to pay particular attention to how autocorrelated our variables are. \n\nAlways look at autocorrelations in your data before you run a time-series regression:\n\nThe above are just one-period (1 month) autocorrelations. We can look at all autocorrelations (all lags):\n\n# Linear regression\n\nWe start by running the simple regression of future short term rates on current long-term rates:\n\n\n$$r^{3m}_{t+5} = \\alpha + \\beta \\cdot r^{10yr}_t + \\epsilon_{t+5} $$\n\nThe low p-value for the ``r_10yr`` coefficient tells us that the long rate is a strong predictor of the future short rate, which is consistent with the Expectations Hypothesis.\n\n# Dealing with autocorrelated errors\n\nThe Durbin-Watson statistic in the regression above is close to 0, which suggests the error terms in our regression are positively autocorrelated. This violates assumption A3 discussed in the regression intro lecture. \n\nTo address this issue, we can apply the \"get_robustcov_results\" function to the results output from the OLS regression above, specifying that the covariance type (\"cov_type\") needs to be heteroskedasticity-and-autocorrelation consistent (HAC). We also need to specify the maximum number of lags we want to use when correcting our standar errors (the \"maxlag\" parameter). This implements the Newey-West (1987) covariance estimator that adjusts standard errors for autocorrelation in residuals up to the \"maxlag\" you specified. \n\nMost practitioners use $maxlag = N^{1/4}$ where N is the number of observations in our regression (though we will not go into detail about why this value is used).\n\nNote that this correction does not change the coefficients themselves, just their statistical significance. For example the constant term is no longer statistically significant. More importantly for our application: the long term rate is still a statistically significant predictor of the future short rate, which is consistent with the Expectations Hypothesis.\n\n# Dealing with non-stationarity\n\nA \"stationary\" time-series process is a process that has a constant mean and variance over time, and autocorrelations depend only on the lag, not on the time period itself (technically speaking, such processes are called \"covariance-stationary\" or \"weakly-stationary\" but these are usually shortened to \"stationary\").\n\nIf the variables in our time-series regression are non-stationary, the regression estimates are not reliable because non-stationarity can cause:\n- A \"spurious correlation\" between the two variables (variables may look highly correlated when in fact they are not economically related in any way)\n    - This is a failure of assumption A2\n    - This leads to a bias in the regression coefficients (affects conclusions on economic significance)\n- Heteroskedasticity and autocorrelation in the regression residuals \n    - This is a failure of assumption A3\n    - In thise case, t-statistics and p-values will be miscalculated (affects conclusions on statistical significance)\n\n\n\n## Testing for stationarity (unit root tests)\n\nBefore you run ANY time series regression, you need to first test if the variables in your regression are stationary (such tests are commonly referred to as \"unit root tests\"). There are several test you could use for this purpose, but one of the most common ones is the Augmented Dickey-Fuller test (ADF). You can implement this test using the \"adfuller\" function from the \"statsmodels.tsa.stattools\" package (\"st\" below).\n\nThe null hypothesis in the ADF test is that the series is non-stationary. So if the test returns a small p-value (e.g smaller that 1%), we can conclude that the series does not suffer from non-stationarity. \n\n\nThe p-value above is larger than 1\\% so we can not conclude that the short rate is non-stationary.\n\nThe p-value above is larger than 1\\% so we can not conclude that the long rate is non-stationary.\n\n## First-differencing\n\nThe most commonly used method to convert a non-stationary series into a stationary series it to first-difference it (i.e. current level minus the previous level of the series). Technically, this assumes that the non-stationary series has \"order of integration 1\" which is the case for most economic series of interest. You don't need to understand what that means for this class. I am only mentioning it so you understand that sometimes, first-differencing may NOT produce a stationary series. In such cases, a second-difference may help: take a first-difference of the first-difference. Alternatively, use the detrending methods in sections 3.3. and 3.4. below.   \n\nIt looks like the 1-month autocorrelation is still quite high. But, if we look at the autocorrelation plot (the gray lines are 95\\% confidence intervals), most of these autocorrelations are statistically insignificant. \n\nAgain, to formally test if we still have a non-stationary problem, we run a ADF test:\n\nThe p-value is virtually 0, so we can reject the null of non-stationarity for the first-differenced short rate.\n\nAgain, the p-value is virtually 0, so we can reject the null of non-stationarity for the first-differenced long rate too.\n\nThese results differ from the non-differenced regression in a very crucial way: the p-value of the ``r_10yr_change`` variable is not lower than 1\\% anymore, so we can not reject the null that the long rate has no predictive power over the short rate. This **contradicts** the prediction of the Expectations Hypothesis that long rates should have statistically significant predictive power over short rates.\n\n## Controlling for deterministic trends\n\nIf we believe that our variables are non-stationary because of a deterministic trend (like a linear trend or a quadratic trend), then we can adjust for this by simply including these trends in our regression. To do this, we first create the trend variables (we will restrict ourselves to a linear trend and a quadratic trend), and then we simply add them to our regression. \n\nNote that even controlling for trends renders the long-rate statistically insignificant. \n\n## Eliminating stochastic (changing) trends (OPTIONAL)\n\nIn some cases, non-stationarity could be caused by trends that change over time (e.g. a linear trend in the first part of the sample,  no trend in the middle, and a quadratic trend towards the end). In this case, the deterministic-trends approach from above may not accurately control for these trends and hence may not solve our non-stationarity problem.\n\nIn this circumstance, it is more appropriate to estimate these stochastic trends first (for each series). Then subtract these trends from the series and use these de-trended variables in our regression instead. \nThe **Hodrick-Prescott method** is very commonly used for this purpose and it outputs the detrended series directly (as well as the estimated trend). This method can be implemented using the ``.tsa.filters.hpfilter()`` function in the ``statsmodel`` package, as below. \n\nOnce again, detrending the data using stochastic trends also renders the long rate insignificant. \n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../../styles.css"],"toc":true,"output-file":"lecture19_regression_application_timeseries.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.361","theme":"zephyr","page-layout":"full","grid":{"sidebar-width":"400px"},"title":"L19: Robust timeseries regression"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}