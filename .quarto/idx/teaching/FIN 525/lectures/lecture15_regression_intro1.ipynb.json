{"title":"L15: Linear regression intro","markdown":{"yaml":{"title":"L15: Linear regression intro"},"headingText":"Preliminaries","containsRefs":false,"markdown":"\n\n\n\nLoad Fama-French factor data:\n\nDownload monthly prices (keep only Adjusted Close prices):\n\nCalculate monthly returns, drop missing and rename \"Adj Close\" to \"TSLA\":\n\nWe need to merge ``firm_ret`` with ``ff3f`` but note that their dates look different. Check their format first:\n\nConvert index of ``firm_ret`` to monthly period, to match the date format un ``ff3f``:\n\nMerge the two datasets:\n\n# Linear regression basics\n\nA linear regression is a statistical model, which means it is a set of assumptions about the relation between two or more variables. In particular, the standard linear regression assumptions are (we restrict ourselves to two variables X and Y for now):\n\n## A1. Linearity \n\nThe relation between the variables is assumed to be linear in parameters:\n\n$$Y_t = \\alpha + \\beta \\cdot X_t + \\epsilon_t $$\n\nNote that \"linear in parameters\" means the function that describes the relation between X and Y (the equation above) is linear with respect to $\\alpha$ and $\\beta$ (e.g. $Y = \\alpha \\cdot X^{\\beta} + \\epsilon$ is not linear in parameters). It does not mean that the relation needs to be linear with respect to X (e.g. $Y = \\alpha + \\beta \\cdot X^2 + \\epsilon$ is still linear in parameters).\n\nBefore we cover the remaining assumptions, a bit of terminology:\n\n\n- Y is commonly referred to as the \"dependent\", or \"explained\" or \"endogenous\" variable\n\n\n- X is commonly referred to as the \"independent\", or \"explanatory\", or \"exogenous\" variable (though, remember, that X can stand for more than one variables)\n\n\n- $\\epsilon$ is commonly referred to as the \"residual\" of the regression, or the \"error\" term, or the \"disturbance\" term\n\n\n- $\\alpha$ (alpha) and $\\beta$ (beta) are the \"coefficients\" or \"parameters\" of the regression. **The outcome of \"running a regression\" is to calculate estimates for these alpha and beta coefficients.** \n\n\n- the t subscript is meant to represent the fact that we observe multiple realizations of the X and Y variables, and the linear relation is assumed to hold for each set of realizations (different t's can represent different points in time, or different firms, different countries, etc). Going forward in this section, we will assume that t stands for time, to make the interpretation clearer.\n\n## A2. Mean independence\n\nThis assumption states that the independent variable(s) X convey no information about the disturbance terms ($\\epsilon$'s). Technically, we write this assumption as:\n\n$$E[\\epsilon_t | X] = 0$$\n\nThis is also called the \"strict exogeneity\" assumption. When this condition is not satisfied, we say that our regression model has an **endogeneity problem**.\n\n\n## A3. Homoskedastic and uncorrelated disturbances\n\nThis assumption states that all disturbance terms have the same variance (i.e. they are \"homoskedastic\"): \n\n$$ Var[\\epsilon_t | X] = \\sigma^2                 \\text{  , for all observations $t$} $$\n\nWhen this assumption is not satisfied, we say we have a **heteroskedasticity problem**.\n\nThe standard linear regression model also assumes that any two disturbance terms are uncorrelated with each other:\n\n$$ Cov [\\epsilon_{t_1}, \\epsilon_{t_2}] = 0 \\text{ , for all $t_1 \\neq t_2$}$$ \n\n\n\n## A4. Full rank\n\nThis assumption states that there are no exact linear relationships between the explanatory variables X (when there are two or more such variables). When this assumption is not satisfied, we say we have a **multicollinearity problem**. \n\nIn the next few lectures, we will cover strategies that we can use when some of the above assumptions are not satisfied.\n\n# Regression fitting: ordinary least squares (OLS) \n\nBy far the most common method for estimating linear regression coefficients is by minimizing the sum of the squares of the error terms (hence \"least squares\"). \n\nThe package we will use for linear regression fitting is called \"**statsmodels**\". Install this package by typing the following in a terminal (or Anaconda Prompt):\n\n**pip install statsmodels**\n\nIn fact, for the most part, we will only use the \"api\" subpackage of \"statsmodels\" as below. Here is the official documentation for the package if you want to learn more about it:\n\nhttps://www.statsmodels.org/stable/index.html\n\nAs mentioned above, we will estimate our regression coefficients using OLS (ordinary least squares). This can be done with the ``OLS`` function of ``statsmodels``:\n\nSyntax:\n```python\nclass statsmodels.regression.linear_model.OLS(endog, exog=None, missing='none', hasconst=None, **kwargs)\n```\n\nWhen we use this function, we can replace ``statsmodels.regression.lineal_model`` with ``sm`` (as imported above).\nThe ``endog`` parameter is where we specify where the data for our dependent variable is, and the ``exog`` parameter is where we specify where our independent variables are. We usually set ``missing=True`` to tell Python that we want to get rid of all the rows in our regression data that have any missing values.\n\n## Example 1: estimating a stock's alpha and beta using the market model\n\nThe market model (aka the \"single-factor model\" or the \"single-index model\") is a linear regression model that relates the excess return on a stock to the excess returns on the market portfolio: \n\n$$R_{i,t} - R_{f,t} = \\alpha_i + \\beta_i (R_{m,t} - R_{f,t}) + \\epsilon_{i,t}$$\n\nwhere:\n- $R_{i,t}$ is the return of firm $i$ at time $t$\n- $R_{m,t}$ is the return of the market at time $t$ (we generally use the S&P500 index as the market portfolio)\n- $R_{f,t}$ is the risk-free rate at time $t$ (most commonly the yield on the 1-month Tbill)\n\nBelow, we estimate this model for TSLA, using the data we gathered at the top of these lecture notes:\n\nTo \"run\" (i.e. \"fit\" or \"estimate\") the regression, we use the ``.fit()`` function which can be applied after the ``sm.OLS()`` function. We store results in \"res\":\n\nThe above shows that ``res`` is a \"RegressionResultsWrapper\". We have not seen this kind of an object before. Check all the attributes of the results (``res``) object:\n\nParticularly important results are stored in ``summary()``, ``params``, ``pvalues``, ``tvalues``, ``rsquared``. We'll cover all of these below. As the name suggests, the ``summary()`` attribute contains a summary of the regression results:\n\n## Example 2: estimating a stock's alpha and beta(s) using the Fama-French three-factor model\n\nThe Fama-French three factor model is a linear regression model that relates the excess return on a stock to the excess returns on the market portfolio and the returns on the SMB (small minus big) and HML (high minus low book-to-market) factors: \n\n$$R_{i,t} - R_{f,t} = \\alpha_i + \\beta_{i,m} (R_{m,t} - R_{f,t}) + \\beta_{i,smb}  R_{smb} + \\beta_{i,hml} R_{hml} + \\epsilon_{i,t}$$\n\n**Challenge**:\n\nEstimate this regression for TSLA using the data we gathered at the top of these lecture notes.\n\n# Interpreting regression results\n\n## Coefficients\n\nThe \"const\" row contains information about the firm's $\\alpha$ and the \"Mkt-RF\" row contains information about the firm's $\\beta$. The $\\alpha$ and $\\beta$ coefficient estimates themselves are in the \"coef\" column ($\\alpha = 0.0416$, and $\\beta = 1.83$ in the single-factor model).\n\nThe \"const\" coefficient tells us what we should expect the return on TSLA to be on a day with no systematic shocks (i.e. a day with market return of 0).\n\nThe 'Mkt-RF' coefficient tells us how we should expect the return on TSLA to react to a given shock to the market portfolio (e.g. the 1.83 coefficient tells us that, on average, if the market goes up by 1%, TSLA goes up by 1.83%, and when the market goes down by 1%, TSLA goes down by 1.83%)\n\nThe results object (``res``) stores the regression coefficients in its ``params`` attribute:\n\nNote that ``res.params`` is a Pandas Series, so we can access its individual elements using the index labels:\n\n**Challenge**: \n\nPrint out (separately) the alpha and each of the betas from the three-factor model\n\n## Statistical significance\n\n**The p-values** are in the \"P > |t|\" column. P-values lower than 0.05 allow us to conclude that the corresponding coefficient is statistically different from 0 at the 95% confidence level (i.e. reject the null hypothesis that the coefficient is 0). At the 99% confidence level, we would need the p-value to be smaller than 1% (1 minus the confidence level) to reject the null hypothesis that alpha = 0.\n\n**The t-statistics** for the two coefficients are in the \"t\" column. Loosely speaking a t-statistic that larger than 2 or smaller than -2 allows us to conclude that the corresponding coefficient is statistically different from 0 at the 95% confidence level (i.e. reject the null hypothesis that the coefficient is 0). In terms of statistical significance, the t-statistic does not provide any new information over the p-value.\n\nThe last two columns give us the 95% confidence interval for each coefficient.\n\nFor the market model, TSLA's alpha has a p-value of 0.031 so we can conclude that it's alpha is statistically significantly different from 0 at the 95% confidence level (but not at the 99% confidence level).\n\nThe fact that the alpha is positive and statistically different from 0 (at 95% level) means that, based on the single-factor model, TSLA seems to be **undervalued**. A negative alpha would mean the stock in **overvalued**.\n\nIf we can not reject the null hypothesis that alpha is 0, the conclusion is NOT that alpha = 0 and therefore the stock is correctly valued (since we can never \"accept\" a null hypothesis, we can only fail to reject). The conclusion is that we do not have enough evidence to claim that the stock is either undervalued or overvalued (which is not the same thing as saying that we have enough evidence to claim that the stock is correctly valued).\n\nThe results object (``res``) stores the regression p-values in its ``pvalues`` attribute:\n\nThe p-values can be accessed individually:\n\nT-statistics are stored in the ``tvalues`` attribute:\n\nT-statistics of individual coefficients:\n\n**Challenge**: \n\nIs TSLA mispriced (undervalued OR overvalued) at the 5% significance level with respect to the Fama-French 3-factor model?\n\n**Challenge**: \n\nDoes TSLA have a significant exposure (at 5% level) to either of the 3 factors in the Fama-French model?\n\n## The R-squared coefficient \n\nThe R-squared coefficient (top-right of the table, also referred to as the \"coefficient of determination\") estimates the percentage of the total variance in the dependent variable (Y) that can be explained by the variance in the explanatory variable(s) (X). \n\nIn the context of our market-model example, the R-squared tells us the percentage of the firm's total variance that is systematic in nature (i.e. non-diversifiable). The percentage of total variance that is idiosyncratic (diversifiable) equals 1 minus the R-squared.\n\nThe R-squared is stored in the ``rsquared`` attribute of the regression results object:\n\nUsing the market model, our estimates of the percentage or total TSLA variance that is systematic vs idiosyncratic are:\n\n**Challenge**: \n\nWhat percentage of TSLA total variance can be diversified away under the Fama-French 3-factor model?\n\n## Diagnostics (bottom of the table):\n\nThe regression table reports (at the bottom) a few statistics that help us understand if some of the assumption of the linear regression model are not satisfied.\n\n- Durbin-Watson: tests for residual autocorrelation. Takes values in [0,4]. Below 2 means positive autocorr. Above 2 means negative aoutocorr. A value of 2 is perfect (means no autocorrelation).\n\n\n- Omnibus: tests normality of residuals. Prob(Omnibus) close to 0 means reject normality\n\n\n- JB: another normality test (null is skew=0, kurt=3). Prob(JB) close to 0 means rejection of normality\n\n\n- Cond. No: tests for multicollinearity. Over 100 is worrisome, but we still have to look at correlations between variables to determine if any of them need to be dropped.\n","srcMarkdownNoYaml":"\n\n# Preliminaries\n\n\nLoad Fama-French factor data:\n\nDownload monthly prices (keep only Adjusted Close prices):\n\nCalculate monthly returns, drop missing and rename \"Adj Close\" to \"TSLA\":\n\nWe need to merge ``firm_ret`` with ``ff3f`` but note that their dates look different. Check their format first:\n\nConvert index of ``firm_ret`` to monthly period, to match the date format un ``ff3f``:\n\nMerge the two datasets:\n\n# Linear regression basics\n\nA linear regression is a statistical model, which means it is a set of assumptions about the relation between two or more variables. In particular, the standard linear regression assumptions are (we restrict ourselves to two variables X and Y for now):\n\n## A1. Linearity \n\nThe relation between the variables is assumed to be linear in parameters:\n\n$$Y_t = \\alpha + \\beta \\cdot X_t + \\epsilon_t $$\n\nNote that \"linear in parameters\" means the function that describes the relation between X and Y (the equation above) is linear with respect to $\\alpha$ and $\\beta$ (e.g. $Y = \\alpha \\cdot X^{\\beta} + \\epsilon$ is not linear in parameters). It does not mean that the relation needs to be linear with respect to X (e.g. $Y = \\alpha + \\beta \\cdot X^2 + \\epsilon$ is still linear in parameters).\n\nBefore we cover the remaining assumptions, a bit of terminology:\n\n\n- Y is commonly referred to as the \"dependent\", or \"explained\" or \"endogenous\" variable\n\n\n- X is commonly referred to as the \"independent\", or \"explanatory\", or \"exogenous\" variable (though, remember, that X can stand for more than one variables)\n\n\n- $\\epsilon$ is commonly referred to as the \"residual\" of the regression, or the \"error\" term, or the \"disturbance\" term\n\n\n- $\\alpha$ (alpha) and $\\beta$ (beta) are the \"coefficients\" or \"parameters\" of the regression. **The outcome of \"running a regression\" is to calculate estimates for these alpha and beta coefficients.** \n\n\n- the t subscript is meant to represent the fact that we observe multiple realizations of the X and Y variables, and the linear relation is assumed to hold for each set of realizations (different t's can represent different points in time, or different firms, different countries, etc). Going forward in this section, we will assume that t stands for time, to make the interpretation clearer.\n\n## A2. Mean independence\n\nThis assumption states that the independent variable(s) X convey no information about the disturbance terms ($\\epsilon$'s). Technically, we write this assumption as:\n\n$$E[\\epsilon_t | X] = 0$$\n\nThis is also called the \"strict exogeneity\" assumption. When this condition is not satisfied, we say that our regression model has an **endogeneity problem**.\n\n\n## A3. Homoskedastic and uncorrelated disturbances\n\nThis assumption states that all disturbance terms have the same variance (i.e. they are \"homoskedastic\"): \n\n$$ Var[\\epsilon_t | X] = \\sigma^2                 \\text{  , for all observations $t$} $$\n\nWhen this assumption is not satisfied, we say we have a **heteroskedasticity problem**.\n\nThe standard linear regression model also assumes that any two disturbance terms are uncorrelated with each other:\n\n$$ Cov [\\epsilon_{t_1}, \\epsilon_{t_2}] = 0 \\text{ , for all $t_1 \\neq t_2$}$$ \n\n\n\n## A4. Full rank\n\nThis assumption states that there are no exact linear relationships between the explanatory variables X (when there are two or more such variables). When this assumption is not satisfied, we say we have a **multicollinearity problem**. \n\nIn the next few lectures, we will cover strategies that we can use when some of the above assumptions are not satisfied.\n\n# Regression fitting: ordinary least squares (OLS) \n\nBy far the most common method for estimating linear regression coefficients is by minimizing the sum of the squares of the error terms (hence \"least squares\"). \n\nThe package we will use for linear regression fitting is called \"**statsmodels**\". Install this package by typing the following in a terminal (or Anaconda Prompt):\n\n**pip install statsmodels**\n\nIn fact, for the most part, we will only use the \"api\" subpackage of \"statsmodels\" as below. Here is the official documentation for the package if you want to learn more about it:\n\nhttps://www.statsmodels.org/stable/index.html\n\nAs mentioned above, we will estimate our regression coefficients using OLS (ordinary least squares). This can be done with the ``OLS`` function of ``statsmodels``:\n\nSyntax:\n```python\nclass statsmodels.regression.linear_model.OLS(endog, exog=None, missing='none', hasconst=None, **kwargs)\n```\n\nWhen we use this function, we can replace ``statsmodels.regression.lineal_model`` with ``sm`` (as imported above).\nThe ``endog`` parameter is where we specify where the data for our dependent variable is, and the ``exog`` parameter is where we specify where our independent variables are. We usually set ``missing=True`` to tell Python that we want to get rid of all the rows in our regression data that have any missing values.\n\n## Example 1: estimating a stock's alpha and beta using the market model\n\nThe market model (aka the \"single-factor model\" or the \"single-index model\") is a linear regression model that relates the excess return on a stock to the excess returns on the market portfolio: \n\n$$R_{i,t} - R_{f,t} = \\alpha_i + \\beta_i (R_{m,t} - R_{f,t}) + \\epsilon_{i,t}$$\n\nwhere:\n- $R_{i,t}$ is the return of firm $i$ at time $t$\n- $R_{m,t}$ is the return of the market at time $t$ (we generally use the S&P500 index as the market portfolio)\n- $R_{f,t}$ is the risk-free rate at time $t$ (most commonly the yield on the 1-month Tbill)\n\nBelow, we estimate this model for TSLA, using the data we gathered at the top of these lecture notes:\n\nTo \"run\" (i.e. \"fit\" or \"estimate\") the regression, we use the ``.fit()`` function which can be applied after the ``sm.OLS()`` function. We store results in \"res\":\n\nThe above shows that ``res`` is a \"RegressionResultsWrapper\". We have not seen this kind of an object before. Check all the attributes of the results (``res``) object:\n\nParticularly important results are stored in ``summary()``, ``params``, ``pvalues``, ``tvalues``, ``rsquared``. We'll cover all of these below. As the name suggests, the ``summary()`` attribute contains a summary of the regression results:\n\n## Example 2: estimating a stock's alpha and beta(s) using the Fama-French three-factor model\n\nThe Fama-French three factor model is a linear regression model that relates the excess return on a stock to the excess returns on the market portfolio and the returns on the SMB (small minus big) and HML (high minus low book-to-market) factors: \n\n$$R_{i,t} - R_{f,t} = \\alpha_i + \\beta_{i,m} (R_{m,t} - R_{f,t}) + \\beta_{i,smb}  R_{smb} + \\beta_{i,hml} R_{hml} + \\epsilon_{i,t}$$\n\n**Challenge**:\n\nEstimate this regression for TSLA using the data we gathered at the top of these lecture notes.\n\n# Interpreting regression results\n\n## Coefficients\n\nThe \"const\" row contains information about the firm's $\\alpha$ and the \"Mkt-RF\" row contains information about the firm's $\\beta$. The $\\alpha$ and $\\beta$ coefficient estimates themselves are in the \"coef\" column ($\\alpha = 0.0416$, and $\\beta = 1.83$ in the single-factor model).\n\nThe \"const\" coefficient tells us what we should expect the return on TSLA to be on a day with no systematic shocks (i.e. a day with market return of 0).\n\nThe 'Mkt-RF' coefficient tells us how we should expect the return on TSLA to react to a given shock to the market portfolio (e.g. the 1.83 coefficient tells us that, on average, if the market goes up by 1%, TSLA goes up by 1.83%, and when the market goes down by 1%, TSLA goes down by 1.83%)\n\nThe results object (``res``) stores the regression coefficients in its ``params`` attribute:\n\nNote that ``res.params`` is a Pandas Series, so we can access its individual elements using the index labels:\n\n**Challenge**: \n\nPrint out (separately) the alpha and each of the betas from the three-factor model\n\n## Statistical significance\n\n**The p-values** are in the \"P > |t|\" column. P-values lower than 0.05 allow us to conclude that the corresponding coefficient is statistically different from 0 at the 95% confidence level (i.e. reject the null hypothesis that the coefficient is 0). At the 99% confidence level, we would need the p-value to be smaller than 1% (1 minus the confidence level) to reject the null hypothesis that alpha = 0.\n\n**The t-statistics** for the two coefficients are in the \"t\" column. Loosely speaking a t-statistic that larger than 2 or smaller than -2 allows us to conclude that the corresponding coefficient is statistically different from 0 at the 95% confidence level (i.e. reject the null hypothesis that the coefficient is 0). In terms of statistical significance, the t-statistic does not provide any new information over the p-value.\n\nThe last two columns give us the 95% confidence interval for each coefficient.\n\nFor the market model, TSLA's alpha has a p-value of 0.031 so we can conclude that it's alpha is statistically significantly different from 0 at the 95% confidence level (but not at the 99% confidence level).\n\nThe fact that the alpha is positive and statistically different from 0 (at 95% level) means that, based on the single-factor model, TSLA seems to be **undervalued**. A negative alpha would mean the stock in **overvalued**.\n\nIf we can not reject the null hypothesis that alpha is 0, the conclusion is NOT that alpha = 0 and therefore the stock is correctly valued (since we can never \"accept\" a null hypothesis, we can only fail to reject). The conclusion is that we do not have enough evidence to claim that the stock is either undervalued or overvalued (which is not the same thing as saying that we have enough evidence to claim that the stock is correctly valued).\n\nThe results object (``res``) stores the regression p-values in its ``pvalues`` attribute:\n\nThe p-values can be accessed individually:\n\nT-statistics are stored in the ``tvalues`` attribute:\n\nT-statistics of individual coefficients:\n\n**Challenge**: \n\nIs TSLA mispriced (undervalued OR overvalued) at the 5% significance level with respect to the Fama-French 3-factor model?\n\n**Challenge**: \n\nDoes TSLA have a significant exposure (at 5% level) to either of the 3 factors in the Fama-French model?\n\n## The R-squared coefficient \n\nThe R-squared coefficient (top-right of the table, also referred to as the \"coefficient of determination\") estimates the percentage of the total variance in the dependent variable (Y) that can be explained by the variance in the explanatory variable(s) (X). \n\nIn the context of our market-model example, the R-squared tells us the percentage of the firm's total variance that is systematic in nature (i.e. non-diversifiable). The percentage of total variance that is idiosyncratic (diversifiable) equals 1 minus the R-squared.\n\nThe R-squared is stored in the ``rsquared`` attribute of the regression results object:\n\nUsing the market model, our estimates of the percentage or total TSLA variance that is systematic vs idiosyncratic are:\n\n**Challenge**: \n\nWhat percentage of TSLA total variance can be diversified away under the Fama-French 3-factor model?\n\n## Diagnostics (bottom of the table):\n\nThe regression table reports (at the bottom) a few statistics that help us understand if some of the assumption of the linear regression model are not satisfied.\n\n- Durbin-Watson: tests for residual autocorrelation. Takes values in [0,4]. Below 2 means positive autocorr. Above 2 means negative aoutocorr. A value of 2 is perfect (means no autocorrelation).\n\n\n- Omnibus: tests normality of residuals. Prob(Omnibus) close to 0 means reject normality\n\n\n- JB: another normality test (null is skew=0, kurt=3). Prob(JB) close to 0 means rejection of normality\n\n\n- Cond. No: tests for multicollinearity. Over 100 is worrisome, but we still have to look at correlations between variables to determine if any of them need to be dropped.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../../styles.css"],"toc":true,"output-file":"lecture15_regression_intro1.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.361","theme":"zephyr","page-layout":"full","grid":{"sidebar-width":"400px"},"title":"L15: Linear regression intro"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}