{"title":"L07: Pandas I/O","markdown":{"yaml":{"title":"L07: Pandas I/O"},"headingText":"Preliminaries","containsRefs":false,"markdown":"\n\n\nIf you have not done so already (you were asked to do this in ``lecture01``):\n\n\n1. Open a Terminal, type the following command and hit enter:\n    - pip install yfinance pandas-datareader\n    \n    \n2. Open a Terminal, type the following command and hit enter:\n    - conda install -y openpyxl xlrd\n\n# Relative vs absolute paths\n\nIn this lecture, we will have to tell Python exactly where on our drive it should store, some files we create. We can do this by specifying the full path to these locations, for example:\n\n'C:/Users/ionmi/Dropbox/TEACHING'\n\nThe above is an **absolute path**: it contains the full path to the TEACHING folder on my drive. \n\nAn alternative way to specify a path (which we will use very often in this course) is to specify that path **relative** to the current working directory. To do this, we use a combination of one or more dots ('.') and/or slashes ('/') which have the following meaning:\n\n'.' means the current working directory (in our case, this is the directory where these lecture notes are stored on the drive).\n\n'..' means the parent of the current working directory.\n\n'../..' means the parent of the parent of the current working directory.\n\n'../../..' means the parent of the parent of the parent of the current working directory.\n\netc.\n\nTo see this in practice, we can use the ``Path`` function in the ``pathlib`` package, which allows us to see the absolute path of a given relative path:\n\n# Reading and writing .pkl (pickle) files\n\nPython has a proprietary data format called \"pickle\". These types of files have the extension \".pkl\". Saving and loading data from pickle files is significantly faster than from/to \".csv\", so we will be using it quite a bit throughout the course.\n\n## .to_pickle()\n\nTo store this data in a .pkl file, we use the \"``.to_pickle``\" function, applied right after the name of the dataframe which contains the data we want to store.\n\n\nSyntax:\n\n```python\nDataFrame.to_pickle(path, compression='infer', protocol=5, storage_options=None)\n```\n\nNote that the first argument (``path``) is mandatory (it has no default value). This argument is where you specify the **name** of the .pkl file you want to create (``mydata.pkl`` below) and the **location** (directory) where this file should be stored (``.`` below) all in a single string, separated by ``/``.  \n\nNote that we can also compress the file:\n\n## .read_pickle()\n\nTo read data from an existing .pkl file, we use the \"``.read_pickle``\" function, specifying as an argument the path to the file we want to read (including its name):\n\nSyntax:\n```python\npandas.read_pickle(filepath_or_buffer, compression='infer', storage_options=None)\n```\n\nFor example, if we want to read the contents of the .pkl file we just created above, and store those contents into a new variable ``df2``, we would use:\n\nAnd we can read compressed .pkl files too:\n\nNote a very important difference in how we use the two functions above. The syntax for ``.to_pickle()`` starts with ``DataFrame.to_pickle`` which tells us that the function must be applied to and existing DataFrame. On the other hand, the syntax for ``.read_pickle()`` starts with ``pandas.read_pickle``, which we converted to ``pd.read_pickle``, because we imported pandas as pd in the first cell code in this notebook (at the top). \n\nThis pattern is the same for all the read-write functions we discuss in this lecture: the write functions (``.to_pickle()``, ``.to_csv()``, ``.to_excel()``) are written after the name of the dataframe we want to write to a file, while the read functions (``.read_pickle()``, ``.read_csv()``, ``.read_excel()``) follow the name of the pandas package (which we renamed to ``pd`` above).\n\n# Reading and writing .txt and .csv files \n\nThe most common way to read and write dataframes from/to .csv and .txt files is with the Pandas functions \"``.to_csv()``\" (for writing) and \"``.read_csv()``\" (for reading).\n\n## .to_csv()\n\nHere is the abbreviated version of the syntax for ``.to_csv()`` excluding parameters that are not used as often:\n\n```python\nDataFrame.to_csv(path_or_buf=None, sep=',', columns=None, header=True, index=True, index_label=None)\n```\n\nNote that the default separator is a comma (``sep=','``) which means we can omit that parameter when we write .csv files. The ``columns`` parameter allows you to specify which columns of the dataframe you want to write in the .csv file.\n\nTo write tab-delimited .txt files, we use ``sep='\\t'`` and change the file extension to ``.txt``:\n\nTo write space-delimited .txt files, we use ``sep=' '`` (though I always recommend using tabs for .txt files):\n\n## .read_csv()\n\nHere is the abbreviated version of the syntax for ``.read_csv()`` excluding parameters that are not used as often:\n\n```python\npandas.read_csv(filepath_or_buffer, sep=',',  header='infer', names=None, index_col=None, usecols=None, nrows=None, \n                skiprows = None)\n```\n\nNote that the default separator is a comma (``sep=','``) which means we can omit that parameter when we read .csv files:\n\nNote however, that we did not specify that the first column is just an index for the table, so that first column was just included as data in the table itself. Note also that ``.read_csv()`` guessed that the column names are on the first row, because the default value of the ``header`` parameter is ``infer``. To be save, I always recommend being explicit about where the row names and column names are (remember, Python starts counting from 0:\n\nTo read tab-delimited .txt files, we use ``sep='\\t'``:\n\nTo read space-delimited .txt files, we use ``sep=' '``:\n\nThe ``.to_csv()`` and ``.read_csv()`` functions have a lot more useful parameters. In the practice problems for this lecture, you will be asked to investigate some of them on your own by reading the official documentation:\n\n- ``.to_csv()``: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html\n- ``.read_csv()``: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\n\n# Reading and writing .xlsx files\n\nWe'll use ``.to_excel()`` to write Excel files and ``.read_excel()`` to read Excel files. The biggest difference from the .csv functions is that, with the excel ones, you can specify a particular sheet in the the Excel file that you want to read/write.\n\n## .to_excel()\n\nHere is an abbreviate version of the syntax for ``.to_excel()``:\n\n```python\nDataFrame.to_excel(excel_writer, sheet_name='Sheet1', columns=None)\n```\n\n## .read_excel()\n\nHere is an abbreviated version of the syntax for ``.read_excel()``:\n\n```python\npandas.read_excel(io, sheet_name=0, header=0, names=None, index_col=None, usecols=None, skiprows=None, nrows=None)\n```\n\nThe ``.to_excel()`` and ``.read_excel()`` functions have a lot more useful parameters. In the practice problems for this lecture, you will be asked to investigate some of them on your own by reading the official documentation:\n\n- ``.to_excel()``: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_excel.html\n- ``.read_excel()``: https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html\n\n# Some important data-acquisition packages\n\n## The \"requests\" package\n\nThe ``requests`` package allows us to retrieve data from websites. If you want a more detailed discussion of the full functionality of this package, see the documentation at https://docs.python-requests.org/en/latest/. \n\nHere, we'll just see how we can use the package to download data from files hosted on websites. For this, we need to URL to the file we want to download. In the example below, I use data on economic policy uncertainty in the US from this website:\n\nhttps://www.policyuncertainty.com/us_monthly.html\n\nIf you right-click on the \"Download Data\" link and select \"Copy Link Address\", you should see the link below when you paste it in your code:\n\nWe use the ``.get()`` function to retrieve the (binary) data from the URL above:\n\nWe can check if the request was successful using the ``status_code`` attribute. 200 means the request was successful, 404 means there was an error.\n\nTo write the data we retrieved into an Excel file on our computer, we use the Python built-in ``open()`` function, specifying the path to the file we want to write the data to (``'./policy_uncertainty.xlsx`` below), specifying that we are writing binary data in it (``wb`` below):\n\nNow we can write the data into that file using the Python built-in ``write()`` function and then closing that file with the ``close()`` function. Note that the actual data from the URL above is found under the ``content`` attribute of the request ``r`` that we created above.\n\nWe can check if this process worked by either manually opening the ``policy_uncertainty.xlsx`` in our working directory, or by using ``pd.read_excel()`` to just read the data into a dataframe and take a look at it:\n\n## The \"pandas_datareader\" package\n\nThe ``pandas_datareader`` package allows us to download data from many different sources on the internet. Here is a list of all these sources:\n\nhttps://pandas-datareader.readthedocs.io/en/latest/readers/index.html\n\nThe general syntax to download data from a particular source is as follows: \n\nSyntax:\n\n```python\npandas_datareader.DataReader(name,data_source=None,start=None,end=None)\n```\n\nThe two sources I will cover here are the St. Louis Federal Reserve Economic Data (FRED) (``data_source = 'fred'``) which contains a lot of useful **macroeconomic data**, and the Fama-French Data (Ken Frenchâ€™s Data Library) (``data_source = 'famafrench'``)  which contains returns on many portfolios commonly used in **asset pricing** (e.g. the market portfolio, SMB, HML, etc).\n\nFor both of these sources, we use the ``name`` parameter to specify **what** exactly we want to download from these data sources. \n\nFor example, to download data on the the CPI from FRED, we need to use ``name = 'CPIAUCSL'`` which is the internal name that FRED uses for the CPI data:\n\nhttps://fred.stlouisfed.org/series/CPIAUCSL\n\nTo download data on the Fama-French three risk factors (market, SMB, and HML) we use ``name = 'F_Research_Data_Factors'\"`` which is the name of the text file containing these factors on Ken French's website:\n\nhttps://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html\n\nNote that for the 'famafrench' data source, the 'DataReader' function return a dictionary of dataframes, not a single pandas dataframe. That's because the 'F-F_Research_Data_Factors' contains multiple tables. The monthly returns on the Fama-French risk factors are in the first entry in that dictionary (the 0 key), so we can retrieve it like this:\n\nThere is no easy way to know under what ``name`` you can find the data you need. You have to look at the FRED and Fama-French websites first, to see what names those websites use for the data you need and then type those names into your code, like we did above.\n\n## The \"yfinance\" package\n\nThe yfinance package allows us to retrieve stock price data from Yahoo Finance. The full documentation for the package can be found here: https://pypi.org/project/yfinance/ (especially, look under \"Fetching data for multiple tickers\" on the main page).\n\nSyntax:\n```python\nyfinance.download(tickers, start = None, end = None, interval = '1d')\n```\n\nFor example, to retrieve monthly stock prices for Microsoft and Apple, we need to supply their tickers in a single string (separated by a space) as the first parameter to the ``download`` function and change the ``interval`` parameter to ``1mo`` (otherwise it will give us daily data):\n\nWe will always drop missing values (with ``.dropna()``) and use Adj Close prices (prices adjusted for dividends and splits):\n\nNote that, if we download data for a single stock, this will return a pandas Series, not a DataFrame:\n\nAs mentioned before, we will usually turn Series into dataframes before continuing to work with them further:\n\n## The \"wrds\" package\n\nThe ``wrds`` package allows us to download data directly from the WRDS database. Unfortunately, this functionality is not available for class accounts like the ones I created for this course. I only mention this package for PhD students, who should be able to use this package with their own individual WRDS credentials. The documentation for this package is found here:\n\nhttps://pypi.org/project/wrds/\n","srcMarkdownNoYaml":"\n\n# Preliminaries\n\nIf you have not done so already (you were asked to do this in ``lecture01``):\n\n\n1. Open a Terminal, type the following command and hit enter:\n    - pip install yfinance pandas-datareader\n    \n    \n2. Open a Terminal, type the following command and hit enter:\n    - conda install -y openpyxl xlrd\n\n# Relative vs absolute paths\n\nIn this lecture, we will have to tell Python exactly where on our drive it should store, some files we create. We can do this by specifying the full path to these locations, for example:\n\n'C:/Users/ionmi/Dropbox/TEACHING'\n\nThe above is an **absolute path**: it contains the full path to the TEACHING folder on my drive. \n\nAn alternative way to specify a path (which we will use very often in this course) is to specify that path **relative** to the current working directory. To do this, we use a combination of one or more dots ('.') and/or slashes ('/') which have the following meaning:\n\n'.' means the current working directory (in our case, this is the directory where these lecture notes are stored on the drive).\n\n'..' means the parent of the current working directory.\n\n'../..' means the parent of the parent of the current working directory.\n\n'../../..' means the parent of the parent of the parent of the current working directory.\n\netc.\n\nTo see this in practice, we can use the ``Path`` function in the ``pathlib`` package, which allows us to see the absolute path of a given relative path:\n\n# Reading and writing .pkl (pickle) files\n\nPython has a proprietary data format called \"pickle\". These types of files have the extension \".pkl\". Saving and loading data from pickle files is significantly faster than from/to \".csv\", so we will be using it quite a bit throughout the course.\n\n## .to_pickle()\n\nTo store this data in a .pkl file, we use the \"``.to_pickle``\" function, applied right after the name of the dataframe which contains the data we want to store.\n\n\nSyntax:\n\n```python\nDataFrame.to_pickle(path, compression='infer', protocol=5, storage_options=None)\n```\n\nNote that the first argument (``path``) is mandatory (it has no default value). This argument is where you specify the **name** of the .pkl file you want to create (``mydata.pkl`` below) and the **location** (directory) where this file should be stored (``.`` below) all in a single string, separated by ``/``.  \n\nNote that we can also compress the file:\n\n## .read_pickle()\n\nTo read data from an existing .pkl file, we use the \"``.read_pickle``\" function, specifying as an argument the path to the file we want to read (including its name):\n\nSyntax:\n```python\npandas.read_pickle(filepath_or_buffer, compression='infer', storage_options=None)\n```\n\nFor example, if we want to read the contents of the .pkl file we just created above, and store those contents into a new variable ``df2``, we would use:\n\nAnd we can read compressed .pkl files too:\n\nNote a very important difference in how we use the two functions above. The syntax for ``.to_pickle()`` starts with ``DataFrame.to_pickle`` which tells us that the function must be applied to and existing DataFrame. On the other hand, the syntax for ``.read_pickle()`` starts with ``pandas.read_pickle``, which we converted to ``pd.read_pickle``, because we imported pandas as pd in the first cell code in this notebook (at the top). \n\nThis pattern is the same for all the read-write functions we discuss in this lecture: the write functions (``.to_pickle()``, ``.to_csv()``, ``.to_excel()``) are written after the name of the dataframe we want to write to a file, while the read functions (``.read_pickle()``, ``.read_csv()``, ``.read_excel()``) follow the name of the pandas package (which we renamed to ``pd`` above).\n\n# Reading and writing .txt and .csv files \n\nThe most common way to read and write dataframes from/to .csv and .txt files is with the Pandas functions \"``.to_csv()``\" (for writing) and \"``.read_csv()``\" (for reading).\n\n## .to_csv()\n\nHere is the abbreviated version of the syntax for ``.to_csv()`` excluding parameters that are not used as often:\n\n```python\nDataFrame.to_csv(path_or_buf=None, sep=',', columns=None, header=True, index=True, index_label=None)\n```\n\nNote that the default separator is a comma (``sep=','``) which means we can omit that parameter when we write .csv files. The ``columns`` parameter allows you to specify which columns of the dataframe you want to write in the .csv file.\n\nTo write tab-delimited .txt files, we use ``sep='\\t'`` and change the file extension to ``.txt``:\n\nTo write space-delimited .txt files, we use ``sep=' '`` (though I always recommend using tabs for .txt files):\n\n## .read_csv()\n\nHere is the abbreviated version of the syntax for ``.read_csv()`` excluding parameters that are not used as often:\n\n```python\npandas.read_csv(filepath_or_buffer, sep=',',  header='infer', names=None, index_col=None, usecols=None, nrows=None, \n                skiprows = None)\n```\n\nNote that the default separator is a comma (``sep=','``) which means we can omit that parameter when we read .csv files:\n\nNote however, that we did not specify that the first column is just an index for the table, so that first column was just included as data in the table itself. Note also that ``.read_csv()`` guessed that the column names are on the first row, because the default value of the ``header`` parameter is ``infer``. To be save, I always recommend being explicit about where the row names and column names are (remember, Python starts counting from 0:\n\nTo read tab-delimited .txt files, we use ``sep='\\t'``:\n\nTo read space-delimited .txt files, we use ``sep=' '``:\n\nThe ``.to_csv()`` and ``.read_csv()`` functions have a lot more useful parameters. In the practice problems for this lecture, you will be asked to investigate some of them on your own by reading the official documentation:\n\n- ``.to_csv()``: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html\n- ``.read_csv()``: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\n\n# Reading and writing .xlsx files\n\nWe'll use ``.to_excel()`` to write Excel files and ``.read_excel()`` to read Excel files. The biggest difference from the .csv functions is that, with the excel ones, you can specify a particular sheet in the the Excel file that you want to read/write.\n\n## .to_excel()\n\nHere is an abbreviate version of the syntax for ``.to_excel()``:\n\n```python\nDataFrame.to_excel(excel_writer, sheet_name='Sheet1', columns=None)\n```\n\n## .read_excel()\n\nHere is an abbreviated version of the syntax for ``.read_excel()``:\n\n```python\npandas.read_excel(io, sheet_name=0, header=0, names=None, index_col=None, usecols=None, skiprows=None, nrows=None)\n```\n\nThe ``.to_excel()`` and ``.read_excel()`` functions have a lot more useful parameters. In the practice problems for this lecture, you will be asked to investigate some of them on your own by reading the official documentation:\n\n- ``.to_excel()``: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_excel.html\n- ``.read_excel()``: https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html\n\n# Some important data-acquisition packages\n\n## The \"requests\" package\n\nThe ``requests`` package allows us to retrieve data from websites. If you want a more detailed discussion of the full functionality of this package, see the documentation at https://docs.python-requests.org/en/latest/. \n\nHere, we'll just see how we can use the package to download data from files hosted on websites. For this, we need to URL to the file we want to download. In the example below, I use data on economic policy uncertainty in the US from this website:\n\nhttps://www.policyuncertainty.com/us_monthly.html\n\nIf you right-click on the \"Download Data\" link and select \"Copy Link Address\", you should see the link below when you paste it in your code:\n\nWe use the ``.get()`` function to retrieve the (binary) data from the URL above:\n\nWe can check if the request was successful using the ``status_code`` attribute. 200 means the request was successful, 404 means there was an error.\n\nTo write the data we retrieved into an Excel file on our computer, we use the Python built-in ``open()`` function, specifying the path to the file we want to write the data to (``'./policy_uncertainty.xlsx`` below), specifying that we are writing binary data in it (``wb`` below):\n\nNow we can write the data into that file using the Python built-in ``write()`` function and then closing that file with the ``close()`` function. Note that the actual data from the URL above is found under the ``content`` attribute of the request ``r`` that we created above.\n\nWe can check if this process worked by either manually opening the ``policy_uncertainty.xlsx`` in our working directory, or by using ``pd.read_excel()`` to just read the data into a dataframe and take a look at it:\n\n## The \"pandas_datareader\" package\n\nThe ``pandas_datareader`` package allows us to download data from many different sources on the internet. Here is a list of all these sources:\n\nhttps://pandas-datareader.readthedocs.io/en/latest/readers/index.html\n\nThe general syntax to download data from a particular source is as follows: \n\nSyntax:\n\n```python\npandas_datareader.DataReader(name,data_source=None,start=None,end=None)\n```\n\nThe two sources I will cover here are the St. Louis Federal Reserve Economic Data (FRED) (``data_source = 'fred'``) which contains a lot of useful **macroeconomic data**, and the Fama-French Data (Ken Frenchâ€™s Data Library) (``data_source = 'famafrench'``)  which contains returns on many portfolios commonly used in **asset pricing** (e.g. the market portfolio, SMB, HML, etc).\n\nFor both of these sources, we use the ``name`` parameter to specify **what** exactly we want to download from these data sources. \n\nFor example, to download data on the the CPI from FRED, we need to use ``name = 'CPIAUCSL'`` which is the internal name that FRED uses for the CPI data:\n\nhttps://fred.stlouisfed.org/series/CPIAUCSL\n\nTo download data on the Fama-French three risk factors (market, SMB, and HML) we use ``name = 'F_Research_Data_Factors'\"`` which is the name of the text file containing these factors on Ken French's website:\n\nhttps://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html\n\nNote that for the 'famafrench' data source, the 'DataReader' function return a dictionary of dataframes, not a single pandas dataframe. That's because the 'F-F_Research_Data_Factors' contains multiple tables. The monthly returns on the Fama-French risk factors are in the first entry in that dictionary (the 0 key), so we can retrieve it like this:\n\nThere is no easy way to know under what ``name`` you can find the data you need. You have to look at the FRED and Fama-French websites first, to see what names those websites use for the data you need and then type those names into your code, like we did above.\n\n## The \"yfinance\" package\n\nThe yfinance package allows us to retrieve stock price data from Yahoo Finance. The full documentation for the package can be found here: https://pypi.org/project/yfinance/ (especially, look under \"Fetching data for multiple tickers\" on the main page).\n\nSyntax:\n```python\nyfinance.download(tickers, start = None, end = None, interval = '1d')\n```\n\nFor example, to retrieve monthly stock prices for Microsoft and Apple, we need to supply their tickers in a single string (separated by a space) as the first parameter to the ``download`` function and change the ``interval`` parameter to ``1mo`` (otherwise it will give us daily data):\n\nWe will always drop missing values (with ``.dropna()``) and use Adj Close prices (prices adjusted for dividends and splits):\n\nNote that, if we download data for a single stock, this will return a pandas Series, not a DataFrame:\n\nAs mentioned before, we will usually turn Series into dataframes before continuing to work with them further:\n\n## The \"wrds\" package\n\nThe ``wrds`` package allows us to download data directly from the WRDS database. Unfortunately, this functionality is not available for class accounts like the ones I created for this course. I only mention this package for PhD students, who should be able to use this package with their own individual WRDS credentials. The documentation for this package is found here:\n\nhttps://pypi.org/project/wrds/\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../../styles.css"],"toc":true,"output-file":"lecture07_data_input_output.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.361","theme":"zephyr","page-layout":"full","grid":{"sidebar-width":"400px"},"title":"L07: Pandas I/O"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}